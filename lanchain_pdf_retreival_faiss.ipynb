{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2792379-d507-497a-9791-b81a8988719d",
   "metadata": {},
   "source": [
    "!pip install pypdf2\n",
    "!pip install faiss-cpu\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4174b0-cd90-4d5c-9e8d-9d2fb569e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "05154a4e-943f-433a-a802-331c5049f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc472a44-1bf6-4559-a4c3-28eb2c28e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader(\"Accenture-Tech-Vision-2024.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c13ce16a-4f59-4d82-b7ba-529a0c8a3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing_extensions import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "866e980a-ae83-424b-a708-0fed65b94bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=''\n",
    "\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text +=content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13a0c698-bfe4-45de-ace8-96849c9673ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Technology Vision  2024\\nHuman  \\nby design\\nHow AI unleashes the next \\nlevel of human potentialTechnology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  2\\nWelcome to our Technology Vision for \\n2024. This year’s Vision is grounded in two \\nrealities. First, technology is driving a wave \\nof reinvention that is impacting every part \\nof every business. Second, this emerging \\ntechnology is becoming more \"human″ in its \\nnature, creating unprecedented capabilities \\nthat in essence give people superpowers. \\nCollectively these two realities stand to \\nreshape the way we work and live.\\nConsider the possibilities. Where once we \\nadapted to technology – such as changing \\nour habits for a new app or computer \\ninterface – technology is beginning to adapt \\nto us. Gen AI applications create realistic \\nscripts and images as if created by people. \\nNew spatial computing mediums have begun to close the physical-digital divide to  \\nenable simultaneous activities in multiple \\nspaces. Body-sensing technology like  \\nbrain-computer interfaces and ambient \\ncomputing are beginning to read and \\nunderstand us like never before.  \\nThis year’s Technology Vision comes at a \\ntime of expansive innovation in technology \\nthat is creating massive opportunities \\nfor leaders – from new ways to drive \\nproductivity to entirely new ways of doing \\nbusiness and tackling grand challenges. \\nWe identify actions to take today and also \\nchart the steps to a future where technology \\ntransitions from a passive proxy to an active \\ncollaborator that engages with us through \\nmore natural interaction. This move to more human-like technology \\nraises questions about the impact on \\npeople. In this year’s Vision, we explore this \\nissue from all dimensions, centering on the \\nimportance of shaping technology that is \\nhuman by design. Technology amplifies \\nhuman creativity and productivity so we \\ncan create a positive impact for the most \\nimportant part of any enterprise. People. \\nStep boldly into this future with us,  \\nand together we can shape our use  \\nof technology. We believe it\\'s Human  \\nby design. \\nJulie Sweet \\nChair and CEOPaul Daugherty  \\nChief Technology  \\nand Innovation Officer Foreword\\nHow AI unleashes the next  \\nlevel of human potentialHuman by designTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  3\\nA match made in AI   \\nReshaping our relationship  \\nwith knowledge   \\n \\nPeople are asking generative AI  \\nchatbots for information – transforming \\nthe business of search today, and the \\nfutures of software and data-driven \\nenterprises tomorrow.Meet my agent  \\nEcosystems for AI  \\n \\n \\nAI is taking action, and soon whole \\necosystems of AI agents could command \\nmajor aspects of business. Appropriate \\nhuman guidance and oversight is critical.The space we need  \\nCreating value in new realities  \\n \\n \\nThe spatial computing technology \\nlandscape is rapidly growing, but to \\nsuccessfully capitalize on this new medium, \\nenterprises will need to find its killer apps.Our bodies electronic   \\nA new human interface  \\n \\n \\nA suite of technologies – from eye-tracking \\nto machine learning to BCI – are starting \\nto understand people more deeply, and in \\nmore human-centric ways.  \\n ContentsIntroduction:  \\nHuman by design \\nHow AI unleashes the next  \\nlevel of human potential\\nIn the coming years businesses will have an \\nincreasingly powerful array of technologies \\nat their disposal – and across the board, the \\ntechnology is becoming more human.  \\n \\nPage 4-12\\nPage 13-27 Page 28-43 Page 44-59 Page 60-77Technology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  4\\nHuman  \\nby design\\nThe relationship between humans and \\ntechnology is at an inflection point.  \\n \\nHave you ever seen dystopian pictures of \\nhumans in 1,000 years? Hunched backs, \\nsallow skin, big and sensitive eyes – the \\nhallmarks of people who spend too much \\ntime indoors, detached from the physical \\nworld. These images reflect how the artists \\nsee our relationship with technology today. \\nThey’re visceral and striking, and based on \\ntrue fears. People worry about screentime \\nand the cognitive impact of technology, \\nand increasingly we hear concerns about \\ntechnology controlling our lives or about \\nlosing control of technology – despite using \\nit more than ever. \\n \\nBut the future doesn’t need to be what \\nthese artists imagine. Not if we recast the \\nrelationship between people and technology \\nand design technology to amplify, rather \\nthan change, the things that make us human. How AI unleashes the next  \\nlevel of human potential\\nIt’s time to make technology \\nhuman by design.Technology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  5\\nThis is a moment for reinvention. In the coming \\nyears, businesses will have an increasingly \\npowerful array of technologies at their disposal \\nthat will open new pathways to unleash greater \\nhuman potential, productivity, and creativity. \\nAutonomous agents that can act on our \\nbehalf; intelligent interfaces that transform \\nthe way we interact with information and \\nsoftware; spatial technologies that blend the \\ndigital world into our physical one, or instantly \\ntransport us from our desk to a factory to \\na mountain top; and even technologies like \\nbrain-computer interfaces that once sounded \\nlike science fiction are starting to find relevant, \\napproachable, enterprise use cases. Early \\nadopters and leading businesses have kick-\\nstarted a race toward a new era of value and \\ncapability. And their strategies are underpinned \\nby one common thread – the technology is \\nbecoming more human.It sounds counterintuitive. After all, hasn’t \\ntechnology always been human? Humans \\ninvent technology, we build it, we scale it. We \\nuse it to overcome limitations and do more. In \\nfact, creating tools that extend our physical \\nand cognitive abilities is so unique to humanity \\nthat some argue it defines us as a species.1\\nYet, by nature, the tools we build are often \\ndistinctly unhuman. They don’t look or act \\n“human,” which has always been the point of \\ncreating them. As humans we had aspirations, \\nbut limitations: we wanted to plant a field, \\nbut couldn’t till the ground; we wanted to \\nreach the stars, but we were earthbound; \\nwe wanted to solve problems, but couldn’t \\nalways crunch the numbers. Tools filled the \\ngaps by doing and being what we couldn’t, \\nand in the process radically transformed our \\nlives. Automobiles expanded our freedom \\nof mobility. Cranes let us build skyscrapers \\nand bridges. Machines helped us create, \\ndistribute, and listen to music.Hasn’t technology always  \\nbeen human? \\nTechnology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  6\\nTechnology’s unhuman nature can also be \\nits drawback, though. Extended use of hand \\ntools can lead to arthritis. Years of looking \\nat screens can accelerate vision problems. \\nWe have amazing navigational tools, but they \\nstill distract us from driving. The discordance \\ncan even go beyond our physical bodies \\nand permeate the environments we live in: \\nhomes or offices are often designed to get \\nthe best bandwidth, combustion engines may \\nbe a need for some but generate pollution \\nfor others. Granted, there have been efforts \\nto create tools that are more ergonomic or \\neasier to use. But even so, time and again \\nwe see and make decisions about our lives \\nbased on what is best for a machine  rather \\nthan optimizing human potential. This is \\nwhy artists imagining the future of human \\nevolution envision a world where we are \\nat conflict with the technology we use. \\nTechnology amplifies our abilities and lets us \\ndo more, but its unnaturalness is just as likely \\nto leave its mark.\\nNow, for the first time in history we see strong \\nevidence that we are reversing course – not \\nby moving away from technology, but rather \\nby embracing a generation of technology \\nthat is more human. Technology that is more \\nintuitive, both in design and its very nature, \\ndemonstrates more human-like intelligence, \\nand is easy to integrate across every aspect  \\nof our lives.Our world is becoming a fusion of atoms and \\nbits, and if we want to help people better live \\nin it, we need to design technology in ways \\nthat amplify these human-like traits. It’s not \\nan entirely new trend: the invention of the \\ngraphical user interface (GUI) created images \\nthat were friendly and more intuitive than \\nlines of code; the smartphone miniaturized \\ncompute to reflect the mobility intrinsic to \\nhumans’ lives; one of AI’s most impactful \\nuses was translating across languages. But \\nnow this slow trickle is about to become a \\ntorrential river of deliberate design.\\nConsider the impact generative AI and \\ntransformer models are having on the world \\naround us. What began as chatbots like \\nChatGPT and Bard has become a driving \\nforce in making technology more intuitive, \\nintelligent, and accessible to all. One example \\nis Adobe Photoshop’s Generative Fill and \\nGenerative Expand features, powered by \\nAdobe Firefly.2 These innovations let anyone \\nadd, expand, or remove content from images  \\nnon-destructively, using simple text prompts. \\nUsers can now experiment with their ideas, \\nideate around different concepts, and \\nproduce dozens of variations faster than \\never before. Where AI once focused on \\nautomation and routine tasks, it’s now shifting \\nto augmentation, changing how people \\napproach work, and is rapidly democratizing \\nthe technologies and specialized knowledge \\nwork that were once reserved for the highly \\ntrained or deep-pocketed.  \\nFor more on the evolution of AI, see A new era of generative AI for everyone from Accenture Research.95% of executives agree that making technology  \\nmore human will massively expand the  \\nopportunities of every industry. \\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  05001,0001,5002,0002,500\\n2022Q11,625\\n1,508\\n1,333 1,3211,3721,933\\n2022Q2 2022Q3 2022Q4\\nNumber of \\ncompanies Total \\nmentionsChatGPT released \\nin November 2022\\n2023Q1 2023Q2 2023Q3Number of mentionsNumber ofcompanies\\n2,24740,000\\n35,000\\n30,000\\n25,000\\n20,000\\n15,000\\n10,000\\n5,000\\n0Introduction 7\\nAnd generative AI has the potential to impact \\nmuch more than just the task at hand. It’s also \\nstarting to profoundly reshape organizations \\nand markets. Google Cloud, for instance, \\nrecently announced a generative AI search tool \\nmeant to help doctors and nurses rapidly find \\npatient information that may be stored across \\nmultiple systems and in different formats – a \\nmajor challenge that has plagued healthcare \\nsystems for years.3  FrameDiff, a generative \\nAI computational tool created by MIT CSAIL \\nresearchers, is crafting synthetic protein structures that go beyond what exists in nature \\nto open new possibilities in drug development.4 \\nAnd for software companies, tools like GitHub’s \\nCopilot (a generative AI assistant that helps \\nwrite code) are demonstrating potential to \\nmake software engineers more satisfied with \\ntheir jobs.5  In fact, in many cases generative AI \\ntools are so intuitive to use and employees are \\nadopting them so rapidly, they are permeating \\nworkplaces from the bottom up – faster than \\norganizations can create formal programs. \\nTo learn more about how our interaction experience is changing, go to Accenture Life Trends 2024 .Generative AI has the potential \\nto impact much more than just  \\nthe task at hand. Can AI have  \\nyour attention?The number of mentions of AI \\nin earnings call transcripts has \\nincreased by 6x since the release \\nof ChatGPT in November 2022.\\nSource: Accenture Research NLP analysis on earnings call transcripts (S&P Global Transcripts) across \\n10,452 companies and over 70K transcripts; Jan 2022 – Sep 2023Number of companies mentioning AI, along with total number  \\nof mentions in earnings call transcripts, 2022Q1 – 2023Q3\\n#TechVision2024Technology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  8\\nOf course, the advent of more human \\ntechnology is happening across many more \\ndimensions than AI alone. And in the process, \\nit\\'s starting to solve many of the pain points \\nthat exist between us and technology, paving \\nthe way for greater human potential.\\nTo solve challenges innate to digital work, \\nlike video fatigue, Microsoft made major \\nupdates to Microsoft Mesh, their platform for \\ncreating immersive spaces that blend digital \\nand physical.6 The company is trying to use \\nimmersion to solve pain points today, as well \\nas drive new collaborative ways of working. \\nRecognizing the importance social media \\nplays in many people’s lives, but also the \\nfriction it can create, social media newcomers \\nDiscord and Mastodon built social networks \\nnot driven by a centralized recommendation \\nalgorithm, but one more reflective of the \\ntypes of communities and relationships \\nwe build in our personal lives. And Boston \\nDynamics has long been at the forefront of \\nmaking robotics more human, promising \\na smoother integration between robotics \\nand the world around us. For instance, their \\nbipedal robot Atlas has been trained on \\ndiverse tasks, allowing it to mimic human \\nmovement and physical intuition.7 What’s \\nmore is these robots no longer just mirror \\nhumans physically, but socially too. Humans \\nwill usually interact with robots through \\ncomplex lines of code and puzzling machine \\nlogic, leaving an impasse between people \\nwho don’t speak that language and the robots \\nnext to them. However, researchers found \\na way to put ChatGPT onboard a Boston \\nDynamics robot, allowing people to use natural \\nlanguage to command the robot or ask it about \\nits previous tasks and receive a clear response \\nin plain English.8\\nThis is why it’s so important for businesses \\nto build and use technology that’s human \\nby design. When technology is more human \\nit’s more accessible and makes people more \\nproductive and connected. And how often do \\nwe want to do more ? Manufacture more custom \\nproducts. Expand to more markets. Work with \\nmore partners.\\nWe are about to see a massive expansion of \\nevery industry. Think of it like this: in the 1700s, \\nthe industrial revolution made creating physical \\nthings easier, and in turn refaced the way our \\nworld works and how we live in it. Now, as \\ntechnology becomes more human, it becomes \\neasier to work with – and will spark an infusion \\nof technology through every dimension of the \\nbusiness. We are already seeing its impact \\non our ability to create. Recent innovations \\nhave led to an explosion of digital art, music, \\nand product designs. And technology that is \\nhuman by design is also introducing brand-new \\npossibilities – digital helpers like AI agents, or \\ndigital spaces where we can build and create \\neven in ways that break the laws of physics. \\nBy building fundamentally intuitive bridges \\nbetween people and the most advanced \\ntechnologies of our age, productivity and \\nvalue creation are poised to grow exponentially \\nacross every industry. It’s an entire universe of \\nnew ideas and new actions for businesses and \\nconsumers alike.  When technology is more human, \\nit’s more accessible.Technology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  9\\nTechnology that is human by design will also \\nreach new people and knowledge that has \\nnever been digitized before. While this will \\ncreate more of what we have, it will also enable \\nthe creation of things and ideas to which \\nenterprises have never had access. Think of all \\nthe people historically alienated by technology \\nwho will be able to contribute to the digital \\nrevolution. As technology becomes more \\nintuitive, we can tap into these people as new \\ncustomers and new employees.  In doing so, their wealth of knowledge will \\nbecome enterprise-actionable for the first \\ntime. And when every person can be part of \\nthe digital transformation, on-going efforts \\nto modernize things like data, products, \\nworkforce, and more will only accelerate. \\nCompanies leading the shift to more human \\ntechnologies will be injected with innovation \\nopportunities as they both buoy and are \\nbuoyed by a flood of new people with the tools \\nto affect change in the digital world.Yet the world we will shape from this expansion \\nof economic growth and empowerment of \\nentire populations is still undecided – and \\nenterprises have a responsibility to shape it \\ninto a world we want to live in. Leaders will be \\nfaced with familiar questions: Which products \\nand services are ripe for scaling? What new \\ndata is at your disposal? What transformative \\nactions can you take? But they will also be at \\nthe center of answering questions they may \\nhave never expected: What kind of oversight \\ndoes AI need? Who will be included in the \\ndigital transformation? What responsibilities do \\nwe have to the people in our ecosystem?   \\n \\nHuman by design is not just a description of \\nfeatures, it’s a mandate for what comes next. \\nAs enterprises look to reinvent their digital \\ncore, human technology will become central \\nto the success of their efforts. Every business \\nis beginning to see the potential emerging \\ntechnologies have to reinvent the pillars of \\ntheir digital efforts. Digital experiences, data \\nand analytics, products, all stand to change \\nas technologies like generative AI, spatial \\ncomputing, and others mature and scale.\\nIn this moment of reinvention, enterprises \\nhave the chance to build a strategy that \\nmaximizes human potential, and erases the \\nfriction between people and technology. The \\nfuture will be powered by artificial intelligence \\nbut must be designed for human intelligence. \\nAnd as a new generation of technology gives \\nenterprises the power to do more, every choice \\nthey make matters that much more too. The \\nworld is watching. Will you be a role model or a \\ncautionary tale?\\nTechnology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  10\\n \\n \\n \\nThink about the things that make us human: \\nthe way we think, act, feel, and understand \\none another. Now technology is starting to \\nreflect that range of human experience. It’s a \\ntransformation that will reset our relationship \\nwith technology and completely change how \\nwe use it and what we do with it.\\nLast year\\'s Technology Vision explored \\nhow the convergence of atoms and bits is \\nbuilding the foundations of our new reality. We \\ndescribed a world where the dissolving barrier \\nbetween our digital and physical realities was \\nopening up brand new innovations in nearly \\nall dimensions of technology, from artificial \\nintelligence, to identity and science tech – and \\nimportantly how each of these pieces would \\nbecome a critical part of the enterprise core \\nmoving forward.\\nIn this year’s Technology Vision we investigate \\nwhere the impact of that foundation matters \\nmost: people.\\nThe advent of more human technology is \\nboth a highly concentrated example and the \\ndirect result of the broader trend towards a \\nworld where atoms meet bits. The four trends \\nthis year outline to enterprise leaders the key \\ndimensions where technology is becoming \\nhuman by design, and how organizations will \\nneed to prepare.\\nFirst: I think, therefore I am. The way we collect, \\nstore, and access information has always been \\na deeply rooted part of the human experience.   \\nIn A match made in AI we explore how \\ntechnology is starting to imitate how we \\nprocess information. These are not just \\nsuperficial changes to the way we interact \\nwith technology, but rooted in memory \\nstructures designed and organized in a \\nsimilar fashion to people’s brains. The earliest \\nchanges are starting in search and will come \\nto disrupt the way we approach knowledge \\nand knowledge management.   \\n \\nAutonomy and the ability to act is even more \\ninnate to the human experience – before \\npeople could write or build, we were hunting \\nand gathering, making decisions, and \\nengaging the world around us. Now, in  \\nMeet my agent we are tracking the evolution \\nfrom AI that can perform singular tasks to  \\nAI agents that, with appropriate oversight, \\ncan work with one another and act as proxies \\nfor people and enterprises alike. Today we \\nmight think of it as automated assistants for \\nindividual interactions, but tomorrow the \\nagent ecosystem has the potential  \\nto underpin the entire business-to-business \\nlandscape.\\nIn The space we need , we’re watching the \\nemergence of a new spatial computing \\nmedium, and the applications taking \\nadvantage of its capabilities to pierce the \\nphysical-digital divide. The metaverse \\nstruggled under the weight of ever-expanding \\ndefinitions and expectations, but the value in \\nthe technology behind it has never been in \\ndoubt. In the end we are physical beings, and \\nthe digital world has always been a strange \\nenvironment. Now spatial computing is letting \\nthe digital world reflect what it means to be \\nhuman and in a physical space.And lastly, it’s always been a challenge to \\nunderstand people. While technology can \\ntrack and observe what people do, it often \\nlacks the specificity of what was intended. \\nOur bodies electronic looks at an emerging \\nsuite of technology that is starting to sidestep \\nthe unnatural technology interactions of the \\npast to read and understand people more \\nclosely than has ever been possible.\\nTo think, act, feel, and understand – these are \\nhuman qualities. But by surrounding ourselves \\nwith tools that mirror us, we make it easier to \\nconnect to the world on a deeper level, and \\nwe empower people to take a larger role \\nin shaping it. Individuals, companies, and \\ngovernments – all empowered to do more.  \\nMake it human:  \\nThe 2024 trendsTechnology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  11\\nPositive engineering:  \\nOur technology crossroads \\nHuman by design technologies can deliver \\nprofound benefits to people and enterprises \\nalike, but the path forward isn’t so simple. The \\nworld is arriving at what might be technology’s \\nbiggest inflection point in history, and \\nenterprises – and the decisions their leaders \\nmake – are at the heart of shaping how we \\nmove forward.  \\n \\nAs we experience more growth and innovation, \\nit won’t all be for the better. There will be \\nmore (and new) opportunities for fraud, \\nmisinformation, and breaches of security. If \\nwe engineer tools with human capabilities \\nbut without human intelligence – or even \\nhuman conscience – we can create in a way \\nthat deteriorates both the bottom line and the \\ngreater good. More human technology means more ethical \\nquestions, and many of these questions require \\nanswers before we can proceed. We made \\nagents that could talk and act indistinguishably \\nfrom humans, expanding human capabilities \\nin impressive ways. But as quickly as ChatGPT \\nwas released, we also started seeing fear-\\nladen headlines. Will machines devolve human \\ncreativity? Will they take our jobs? Will they \\ntry to destroy us? This isn’t merely luddite \\nspeculation, many leading AI researchers \\nhave (controversially) raised concerns and \\nhalted research over the potential dangers \\nof AI.9,10 When the metaverse was introduced, \\nit challenged us to question the impact it \\nwould have on people. Would the allure of the \\nmetaverse cause us to cocoon in our homes, \\npotentially impacting our mental health?11 Now \\nbrain scanning can be used to decode what \\npeople are thinking.12 Will that potential be \\nused for or against us?  \\n More human  \\ntechnology means  \\nmore ethical questions,  \\nand many of these  \\nquestions require  \\nanswers before  \\nwe can proceed. None of these questions have a clear resolution, \\nbut enterprises will  be on the front lines \\nof answering them. While some may seem \\nimplausible, borderline unfathomable, they \\nare quietly creeping out of science fiction \\nbooks and into boardroom conversations. In \\nthe era of human tech, every product and \\nevery service that enterprises bring to market \\nholds the potential to transform lives, empower \\ncommunities, and ignite change – for better \\nor for worse. And, invariably, enterprises will \\nface the delicate balancing act of needing to \\nact fast versus needing to act carefully, as well \\nas the expectation that competitors or other \\ncountries may not share the same concerns or \\nimpose the same guardrails.   Technology Vision  2024  |  Human by design  Introduction\\n#TechVision2024Technology Vision  2024  |  Human by design  12\\nThe choices enterprise leaders make, the \\nvalues they uphold, and the priorities they \\nset will reverberate far beyond profit margins \\nand shareholder returns – which makes it \\nmore important than ever that enterprises \\ninnovate with purpose. As we strive to \\nmake technology human by design, we \\nneed to think of security as an enabler– \\nan essential way to build trust between \\npeople and technology – rather than as a \\nlimitation or requirement. And we need to \\nbuild technology without overshadowing or \\nupending what it means to be human. It’s \\na concept we call “positive engineering.” \\nOver the last few years, ethical questions \\nhave entered the technology domain from \\na number of different directions. Inclusivity, \\naccessibility, sustainability, job security, \\nprotection of creative intellectual property, \\nand so much more. Each of them roots back \\nto one single question: how do we balance \\nwhat we can achieve with technology with \\nwhat we want as people? \\nAs some humans enter the digital world for \\nthe first time, and others dive deeper and \\ndeeper in, companies must prioritize their  \\nwell-being, privacy, and security. Companies \\nthat strive for technological inclusivity will \\nbridge both societal gaps and the voids that \\nexist between the organization, its employees, \\nand its customers. As technologies become \\nmore human and expand opportunities for \\nenterprise growth, they must also create new \\npaths for humans to thrive.\\nThis is a transformative moment for \\ntechnology and people alike, and the world is \\nready for you to help shape it.  \\n93% of executives agree that with rapid  \\ntechnological advancements, it is more  \\nimportant than ever for organizations to  \\ninnovate with purpose. A match \\nmade in AI\\nReshaping our relationship \\nwith knowledgeTechnology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  14\\n#TechVision2024\\n1996\\n20221665\\n1998\\n20231768\\n20011873\\n2008\\n20251967\\n2010\\n20272012 2019\\n2028 2029 20311975 1990\\nAsk Jeeves  \\nis founded.8\\nOpenAI releases \\nChatGPT.15The first scientific  \\njournal is printed.1\\nGoogle goes  \\nonline with its  \\nPageRank algorithm.9 \\nBing Chat is unveiled by \\nMicrosoft.16Encyclopedia  \\nBritannica publishes \\nits first edition.2\\nWikipedia launches.10The Dewey  \\nDecimal System is  \\ndeveloped for Amherst \\nCollege Library.3\\nStack Overflow  \\nbegins crowdsourcing \\nprogramming questions \\nand answers.11\\nA leading airline\\nwill announce  \\nthat customers are  \\njust as satisfied with  \\nchatbot agents as  \\nhuman agents.The “Ask NYPL”  \\n(New York Public Library) \\nhotline opens.4\\nMicrosoft  \\nintroduces SharePoint  \\nfor enterprises.12 \\nData poisoning  \\n(adding malicious  \\ndata to ML models)  \\nwill be a top  \\ncybersecurity threat  \\nto enterprises.ORBIT launches as a  \\ndatabase search service \\nfor research abstracts.5\\nGoogle announces  \\nits knowledge graph, a \\nsignificant step toward \\nsemantic search.13Researchers propose \\nK-BERT, a knowledge \\ngraph-enabled LLM.14\\nMajor corporations  \\nwill have proprietary  \\nchatbots to assist  \\nwith knowledge  \\nmanagement, research, \\nand task completion.AI advisors will  \\nreceive more  \\nsearch traffic  \\nthan traditional  \\nsearch engines.A smartphone  \\nwill launch that  \\nreplaces the app-based \\ninterface with an  \\nagent-based one.Ohio State University  \\nimplements the first  \\nmajor digital catalog.6Three McGill  \\nUniversity students \\nbuild Archie,  \\nthe first search  \\nengine.71967Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  15\\n#TechVision2024\\nThe big picture\\nOur relationship with data is changing –  \\nand with it, how we think, work, and interact with  \\ntechnology. The entire basis of the digital enterprise  \\nis getting disrupted. \\nTechnology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  16\\n#TechVision2024\\nThis is no exaggeration. Think back about 15 \\nyears, to when the July cover story of The \\nAtlantic  asked, “ Is Google Making Us Stupid? ” \\nWriter Nicolas Carr claimed that internet \\nsearch was changing how his mind worked, \\ntransforming how he concentrated and how \\ndeeply he read.17  Today, we can answer his \\nquestion with an unequivocal “ No.” Using \\nGoogle hasn’t diminished our intelligence.   \\n \\nOne point, though, was true – how we access \\ninformation does shape our behaviors. Search \\nchanged nearly everything it touched. It \\nbecame a primary way for people and \\nbusinesses to interact with data. It wildly \\nexpanded the knowledge people can access \\nand reduced research time from hours to \\nminutes. And it had the same transformative \\neffect on enterprises. It created new inroads \\ninto customers’ lives via ads and search engine \\noptimization (SEO), created new methods of \\nserving and discovering products, and became \\nintrinsic to the employee experience. We’re so \\nused to it that most people don’t even realize \\nhow much search has permeated their lives. \\nBut almost 70% of all website traffic begins \\nwith search.18 The web made every piece of \\ninformation part of a vast library. And for over \\n20 years, search has been our librarian. \\n Then at the end of 2022, leaders in search \\nwent into high alert.19 For the first time in \\nyears, people’s relationship with data was in \\nflux. The “librarian” model was giving way to a \\nnew “advisor” model. Rather than run a search \\nto curate results, people were starting to ask \\ngenerative AI chatbots for answers.  \\n \\nIt started in November 2022 when OpenAI \\nlaunched ChatGPT, which became the fastest-\\ngrowing app of all time.20 Large language \\nmodels (LLMs) had been around for years, \\nbut ChatGPT’s ability to answer questions in \\na direct and conversational manner made \\nthe difference. Microsoft, OpenAI’s partner, \\nquickly recognized what was happening and \\nreleased Bing Chat in February 2023.21 They \\npositioned it not just as a search tool but \\nas a “copilot for the web.” The next month, \\nSalesforce announced Einstein GPT – a \\ngenerative AI CRM technology that leverages \\nall the customer data a company stores in \\nSalesforce applications to generate content like \\npersonalized marketing materials or knowledge \\narticles.22 By June, the electronic health record \\nsoftware company Epic was integrating GPT-4 \\ninto its products to allow clinicians to speedily \\ngenerate summaries of patient charts.23 \\n We’re so used to it that most people don ’t  \\neven realize how much search has  \\npermeated their lives. Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  17\\n#TechVision2024\\nHow people access and interact with data is \\nchanging. Fast. And this change is a code red – \\nnot just for traditional search companies,  \\nbut for every company. It’s not just web search \\nengines but search in the very broadest sense \\nthat is being impacted by generative AI – \\neverything from searching for an email or  \\na file to looking up customer details in a  \\nCRM database.  \\n \\nData is one of the most important factors \\nshaping today\\'s digital businesses. And these \\nchatbots – that can synthesize vast amounts \\nof information to provide answers and advice, \\nuse different data modalities, remember prior \\nconversations, and even suggest what to ask \\nnext – are disrupting that undercurrent. Many \\nlong-standing enterprise functions like digital \\nmarketing, advertising, and product discovery \\nall stand to change. And beyond the open \\nweb, companies are also chock full of valuable \\ninformation that wants to be found and used \\nby customers, employees, partners, and \\ninvestors. But whether it’s because we don’t \\nrecall the right search terms, we can’t write the \\nquery, the data is siloed, or the documents are \\ntoo dense, a lot of that information is hard to \\naccess or distill. For the data-driven business \\nof today, that’s serious untapped value that \\ngenerative AI could bring in.  \\n \\nHowever, the true disruption here isn’t just in \\nhow we access data – it could also transform \\nthe entire software market. What if the \\ninterface to every app and digital platform \\nbecame a generative AI chatbot? What if \\nthat became the way we read, write, and \\ninteract with data, as a core competency of all \\nplatforms? The evolution happening in search is inevitably changing software, the role it \\nplays in the enterprise, and the digital world  \\nat large.\\nCompanies have the chance today to \\nreimagine how information works throughout \\ntheir organization, and in doing so, invent the \\nnext generation of data-driven business. \\n \\nCustomer experiences and interactions \\nwill look different as companies position \\nthemselves not just as a search result, but \\nas a trusted advisor. Internal workflows will \\ntransform when employees are powered with \\nthe information and answers they need, when \\nthey need them. As businesses deliver novel \\nproducts and services, their entire value \\npropositions may shift.  \\n \\nIt\\'s a huge opportunity. Yet more than that, \\nit’s absolutely necessary. As an example, \\nconsider how a car manufacturer today can \\nall but guarantee their new makes and models \\nappear in the search results for “New Cars \\n2024.” Could they say the same if a potential \\ncustomer was asking an AI what new car they \\nshould buy? The attention mechanism that \\nmade transformer models so powerful is now \\nsuggesting what people  should pay attention \\nto as well. If enterprises fail to act, this \\ntransformation in our relationship with data \\ncould have consequences ranging from losing \\nout on seamless internal information sharing, \\nto losing direct access to customers, all the \\nway to losing control of their brand. Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  18\\n#TechVision2024\\nEvery enterprise has an information strategy. \\nIt’s defined through the software they use, \\nhow they market information externally, and \\nhow they use it internally. Search, broadly \\nspeaking, has underpinned these strategies \\nfor decades. But every day, consciously or not, \\nmore people shift from search to asking, and \\nmore companies look to meet them where \\nthey are, using generative AI chatbots as the \\nUI for enterprise platforms and a new access \\npoint for customers. Already there are multiple \\noptions to implement LLMs and generative \\nAI chatbots in the enterprise, but it’s far from \\na simple technology rollout. To truly reap \\nthe benefits of generative AI and build the \\ndata-and-AI powered enterprise of the future, \\nbusinesses need to radically rethink their core \\ntechnology strategy. How they gather and \\nstructure data, their broader architectures, and \\nhow they deploy technology tools and the \\nfeatures they include need to be rethought. \\nAnd new practices like training, debiasing, and \\nAI-oversight must be built in from the start.   \\n \\nIt\\'s a lot, but enterprises can’t afford to be the \\nlast to move. People want  the simplicity of \\nasking questions and getting answers. Who \\nelse remembers Ask Jeeves from the 90s? The \\ndesire to ask was there, but the backend to \\nsupport it wasn’t. Now with generative AI, a \\ndigital butler is finally in the cards. The way we \\ninteract with data, and how we live, work, and \\nthink, is all changing. Enterprises need to be \\njust as pliable – or a new generation of data-\\ndriven business will rise without them.Now, with generative AI, a digital  \\nbutler is finally in the cards. Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  19\\n#TechVision2024\\nThe technology:  \\nUnlocking your  \\ndata-driven enterprise\\nFPO\\nData powers digital business; there’s no \\nquestion about that. Across every industry  \\nand in more ways than we can count, \\nenterprises have worked to build data-driven \\npractices. And yet people still struggle with the \\nmost basic part: finding  the data they need.  \\nThis is why people are gravitating towards \\nasking – it offloads and automates the mental \\nburden of hunting for data.   \\n \\nAccording to a recent Gartner® survey, “47% \\nof digital workers struggle to find information \\nor data needed to effectively perform their \\njobs.”24 And employees are always looking \\nfor information. They look for documents in \\ntheir team’s file share, search the company \\nknowledge base for policy or benefits \\ninformation, and search their CRM to better \\nunderstand customers and sales. What’s \\nmore, other stakeholders are always trying \\nto find information about the enterprise, too: \\ncustomers browsing for products or searching help forums, shareholders looking for ESG \\nfilings, or partners trying to validate licensing. \\nFor all these people in need of information, \\nmany enterprises have failed to build sufficient \\nsearch capabilities.25  \\n \\nIt makes sense that the shift from search to \\nasking is so alluring. It’s shaking things up in \\na way enterprises desperately need. Putting \\nan LLM-advisor with the breadth of enterprise \\nknowledge at every employee’s fingertips \\ncould unlock the latent value of data and \\nfinally let enterprises tap into the promise of \\ndata-driven business. Of course, it’s not as \\nsimple as turning the knob to something new. \\nGenerative AI will be the interface that rests \\non top of enterprises’ vast data architectures, \\nbut if businesses want to capture its many \\nbenefits and transform how people access \\ntheir information, they need to finally get that \\nfoundation right. Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  20\\n#TechVision2024\\n95% of executives believe generative AI  \\n will compel their organization to modernize its  \\ntechnology architecture. \\nThe good news? New technologies and \\ntechniques can help enterprises shore up their \\ndata foundation and prepare for the future of \\ndata-driven business. In fact, some enterprises \\nhave already taken steps to modernize their \\ndata strategy. But the hard truth is that many \\nothers are still struggling, and their knowledge \\nmanagement systems are severely lacking. \\nWherever companies start from, LLM-advisors \\nwill demand a data foundation that’s more \\naccessible and contextual than ever.  \\n \\nThe knowledge graph is one of the most \\nimportant technologies here. It’s a graph-\\nstructured data model including entities \\nand the relationships between them, which \\nencodes greater context and meaning. Not only \\ncan a knowledge graph aggregate information \\nfrom more sources and support better \\npersonalization, but it can also enhance data \\naccess through semantic search.26Shoring up your  \\ndata foundation\\nTo demonstrate just how powerful a \\nknowledge graph and semantic search can \\nbe, consider a use case from Cisco Systems. \\nLike many large global enterprises, Cisco’s \\nsales team had tons of content to leverage. \\nBut they struggled to find relevant documents \\nthrough their index-driven search due to a \\nlack of metadata. So, they turned to Neo4j \\nto help create a metadata knowledge graph. \\nWhile they did not use LLMs, they relied on \\nnatural language processing to create an \\nontology and a machine tagging service to \\nassign document metadata, which was then \\nstored in a graph database. Now, finding \\ninformation takes half the time, and Cisco \\nsaves its salespeople over four million \\nhours per year with its boosted knowledge \\nmanagement capabilities.27 \\n In addition to knowledge graphs, other data \\nmanagement strategies will be important. Data \\nmesh and data fabric are two ways to help \\nmap and organize information that businesses \\nshould look into as they update their overall \\narchitecture. And vector databases are \\nessential to represent high-dimensional data \\nfor inferring relationships and similarity. What’s \\nmore, while building up an enterprise’s data \\nfoundation is critical to enabling an LLM-advisor \\ninterface, LLMs are also a tool to support the \\ncreation and maintenance of this foundation. \\nThe first step in creating a knowledge graph, \\nfor instance, is to determine its ontology, or \\nthe relevant entities and their relationships to \\neach other. LLMs can extract that data from \\nraw texts to automate this process.28 LLMs can \\nalso accept natural language text prompts to generate the associated schema and database \\nstructure based on the ontology, as well as to \\npopulate the graph database. So, as complex \\ndata foundations become ever more critical, \\nthey’re also becoming easier to maintain and \\nkeep up-to-date.29\\nGarbage in, garbage out has been enduring \\ntruth of machine intelligence. And in today’s \\nage of asking LLMs for answers, it’s more acute \\nthan ever. As enterprises start to bring LLM-\\nadvisors into their businesses, their success or \\nfailure here will make all the difference.  Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  21\\n#TechVision2024\\nOn their own, knowledge graphs, data mesh, \\nand data fabric would be a huge step up for \\nenterprise knowledge management systems. \\nBut there’s a lot of value to be gained in taking \\nthe next step and shifting from the librarian to \\nadvisor model. Imagine if instead of a search \\nbar, employees could ask questions in natural \\nlanguage and get clear answers – across \\nevery website and app in the enterprise. With \\nan accessible and contextual data foundation, \\nenterprises can start to build this – and there \\nare a few different options to explore. \\n \\nFirst, companies can train their own LLM \\nfrom scratch, though this approach is rare \\ngiven the significant resources required. \\nSome of the leaders here are AI powerhouses, \\nincluding Amazon, OpenAI, Google, Meta, \\nAI21, and Anthropic. Bloomberg also took this \\napproach, using its own massive knowledge \\nbase of financial data, along with a public \\ndataset, to train a 50-billion parameter LLM for \\nthe financial industry called BloombergGPT.30 \\nIt will be made available to customers on the \\nBloomberg Terminal.31 For large companies \\nwith vast resources, however, self-training \\nan LLM from scratch may be an appealing \\napproach to secure a competitive advantage.  \\n \\nA second option is to “fine-tune” an existing \\nLLM. Essentially, this means taking a more \\ngeneral LLM and adapting it to a domain by \\nfurther training it on a set of domain-specific \\ndocuments. OpenAI’s GPT-3.5, for instance, \\ncan be fine-tuned using a business’s own data, \\nto hone it into a more custom or efficient model for certain tasks.32 And major cloud \\nproviders like Amazon AWS, Microsoft Azure, \\nand Google Cloud all provide services to help \\ntheir customers fine-tune a private version of a \\nfoundation model with their own data.33,34,35 \\nThese models can then be integrated and \\ndeployed in company applications. While \\nthis takes considerably fewer resources than \\ntraining an LLM from scratch, it does not \\nensure that the model has the latest up-to-date \\ninformation. This option makes the most sense \\nfor domain-specific cases when real-time \\ninformation is not necessary, like for creative \\noutputs in design or marketing.  \\n \\nA slight variation on this is also gaining traction. \\nEnterprises are beginning to fine-tune smaller \\nlanguage models (SLMs) for specialized use \\ncases. SLMs like DeepMind’s Chinchilla and \\nStanford’s Alpaca have started to rival larger \\nmodels while requiring only a fraction of the \\ncomputing resources.36 These SLMs are not \\nonly more efficient, running at lower cost with \\nsmaller carbon footprints, but they can be \\ntrained more quickly and used on smaller,  \\nedge devices.  \\n \\n \\nExploring LLMs as your \\nnew data interfaceTechnology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  22\\n#TechVision2024\\nLastly, one of the most popular approaches to \\nbuilding an LLM-advisor has been to “ground” \\npre-trained LLMs by providing them with \\nmore relevant, use case-specific information, \\ntypically through retrieval augmented \\ngeneration (RAG). As suggested by the name, \\nthis combines an information retrieval system \\nwith a generative model, which can be either \\nself-trained or used out-of-the-box and  \\naccessed through an API.  \\n \\nAt a high level, RAG works something like this: \\nFirst, a user will type in their request. Next, that \\ninput is used to search for and retrieve relevant \\ndocuments – whether unstructured data like \\ntext from Word documents, chats, or PDFs, or structured data like CSVs or database tables – \\nas vector embeddings. Then, these documents, \\nalong with a prompt, are sent to the LLM. The \\nLLM is, of course, trained on a huge amount \\nof data initially, but only uses the specific \\ninformation it receives to generate its response \\nto the user.  \\n \\nGrounding an LLM through in-context learning \\nand RAG takes much less time and compute \\npower, and furthermore, requires far less \\nexpertise than training LLMs from scratch \\nor fine-tuning. In fact, this approach is built \\ninto Microsoft 365 Copilot, an AI assistant for \\nMicrosoft 365 applications and services.37 \\nAnd Salesforce’s Einstein GPT uses this approach to ground generative AI chatbot \\nresponses too, when connected to one of \\nOpenAI’s LLMs or any other external LLM.38 This \\noption works best for use cases that require \\nup-to-date information, though verifying for \\naccuracy may still be necessary.  \\n \\nThe field of generative AI and LLMs is moving \\nfast, so by the time you read this report, there \\nmay already be new best practices for building \\ngenerative AI advisors. But whatever way you \\nchoose to explore, one thing will stay constant: \\nyour data foundation needs to be solid and \\ncontextual, or your LLM-advisor will never live \\nup to its promise.\\n#TechVision2024Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  23\\nThe implications:\\nThe future of enterprise  \\nknowledge\\nNow that we’ve explored people’s  \\nchanging relationship with information,  \\nand how enterprise data practices can evolve \\nto meet the moment, we need to discuss \\nwhat it will let enterprises do. After all, it’s \\none thing to say LLM-advisors will launch a \\nnew generation of data-driven business. It’s \\nanother thing to build it. \\n \\nUnderstanding  \\nand mitigating risks \\n \\nFirst and most importantly, as businesses \\nbegin to explore the new possibilities LLM-\\nadvisors bring, they need to understand the \\nassociated risks. In March 2023, a lawyer \\nsubmitted a brief to a New York judge. \\n In it, he cited multiple prior court decisions, \\nindicating that his client’s case should not be \\ndismissed.39 But there was a problem. None \\nof those court decisions, or related quotations \\nand citations, could be found – ChatGPT had \\ncreated fictitious cases. According to the \\nlawyer, he was not aware that ChatGPT could \\nfabricate information, but the judge was not \\npleased. Not only did the judge fine that lawyer \\nand others involved, but he required them \\nto notify the real judges who were identified \\nas having written the fake cases.40 Most \\nembarrassing: the colossal mistake wound up \\non the front page of The New York Times .41 Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  24\\n#TechVision2024\\n \\nWhat this lawyer experienced was an almost \\nintrinsic characteristic of LLMs: “hallucinations.” \\nBecause LLMs are trained to deliver \\nprobabilistic answers with a high degree of \\ncertainty, there are times when these advisors \\nconfidently relay incorrect information. And as \\nLLM applications start to take a bigger share \\nof how we access and relay information, or \\ninteract with and integrate software, there can \\nbe serious consequences. Any way you slice it, \\nwhen you don’t know if what you’re reading is \\ntrue, that’s a major issue. \\n \\nAnd while hallucinations are perhaps LLMs’ \\nmost notorious risk, other issues come up \\nwhen we think about using these chatbots \\nin the enterprise. If using a public model, \\nproprietary data must be carefully protected \\nso that it cannot be leaked. And for private \\nmodels too, data cannot be shared with \\nemployees who should not have access. The \\ncost of computing is something that needs \\nto be managed. And underlying everything, \\nfew people have the relevant expertise to \\nimplement these solutions well.  \\n \\nAll that said, these challenges shouldn’t be \\ntaken as a deterrent, but rather as a call to \\nimplement the technology with appropriate \\ncontrols. And we’re not starting from zero. \\nThe governance that matters most for LLMs is \\nimportant for any AI implementation, especially \\nwhen it comes to data security, accuracy, and \\nethical issues.  \\n \\nThe data going into the LLM – whether through \\ntraining or the prompt – should be high \\nquality data. That means it should be fresh, \\nwell-labeled, and unbiased. Training data should be zero-party and proactively shared \\nby customers, or first-party and collected \\ndirectly by the company.42 And security \\nstandards should be implemented to protect \\nany personal or proprietary data. Finally, data \\npermissions must also be in place to ensure \\nthat the user is allowed to access any data \\nretrieved for in-context learning.  \\n \\nBeyond accuracy, the outputs of the \\ngenerative AI chatbot should also be \\nexplainable and align with the brand. There are \\nmultiple ways to help achieve this. Guardrails \\ncan be put in place so that the model does \\nnot respond with sensitive data or harmful \\nwords, and so that it declines questions \\noutside its scope. Moreover, responses can \\nconvey uncertainty and provide sources for \\nverification. One company that does this today \\nis Writer, a generative AI writing platform. It \\nleverages a knowledge graph to highlight the \\nAI-generated content that should be fact-\\nchecked, and it also suggests a replacement \\nwhich may be more accurate, along with its \\nsource, based on the relevant information in \\nthe knowledge graph.43 \\n \\nFinally, generative AI chatbots should be \\nsubject to continuous testing and human \\noversight. Companies should invest in ethical \\nAI and develop minimum standards to adhere \\nto. And they should gather regular feedback \\nand provide training for employees as well. \\nOf course, given the power of generative AI \\nchatbots, more associated risks are sure to \\narise – but these are some of the best ways to \\nstart mitigating those risks today.\\n Developer  \\ninterest in  \\ngenerative AI \\nsurged in 2023Following the launch of ChatGPT, \\nthere was a threefold surge in \\ngenerative AI-related posts on Stack \\nOverflow. This indicates a substantial \\nongoing trend among developers \\nwho are integrating and adopting \\ngenerative AI into their workflow.\\nSource: Accenture Research analysis on Stack Overflow Data, timeline: Sep 2022 - Aug 2023Total posts mentioning generative AI tools \\nby month: September 2022 - August 2023\\n2022-09 2022-10 2022-11 2022-12 2023-01 2023-02 2023-03 2023-04 2023-05 2023-06 2023-07 2023-08Release of \\nChatGPT ~3X\\n03006009001,2001,500\\n3423723975267088461,177\\n1,1081,199 1,1971,384\\n1,189A match made in AI 24\\n#TechVision2024Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  25\\n#TechVision2024\\nToyota is another company leveraging \\ngenerative AI to scour vehicle owner’s \\nmanuals and provide more direct answers to \\npeople’s car questions.45 Currently, they have \\na proof-of-concept that allows a driver to ask \\naloud a question like, “How do I disable the \\nVSC?” The Toyota AI will respond with clear \\ninstructions, as well as with the pages in the \\nmanual where the driver can find the answer. \\nIn addition, research shows that customer \\nservice workers can benefit from generative \\nAI chatbot assistance too, likely through the \\ndissemination of more tacit knowledge.46 The \\nfindings showed that the AI assistant not only \\nhelped workers become more productive, \\nresolving 14% more issues per hour on average, \\nbut it also improved customer sentiment and \\nemployee retention, while reducing requests \\nfor the manager.  \\n \\nCompanies are finding ways to add generative \\nAI chatbots to their products too. The social \\nplatform Discord, for instance, has launched \\nAI-generated conversation summaries so that \\nusers can quickly catch up on what they’ve \\nmissed.47 And its moderation tool AutoMod \\nnow uses OpenAI LLM technology to inform \\nmoderators when rules may have been broken, \\nwhile also considering the context of the \\nconversation. In addition, Snap has created \\none of the largest consumer chatbots, My AI, \\navailable through Snapchat, which has already \\nreceived more than 10 billion messages from \\nmore than 150 million people.48 Users can use \\nMy AI to learn more about their interests and \\nhobbies, as well as to help foster connections \\nwith their friends.  \\n \\nWhat the shift from search  \\nto asking will let us do \\nAt last, it’s time to start capitalizing on \\ntoday’s new age of LLM-advisors. Data \\nand software are so intrinsic to businesses \\ntoday, in how they operate and in what they \\nfundamentally offer, that enterprises have a \\nhuge range of opportunities to change and \\nimprove what they do now.   \\n \\nStarting with opportunities inside the \\nenterprise – equipped with generative \\nAI chatbots, employees and customers \\nwill have newfound access to not just \\nknowledge but answers in context, in a \\nway that they’ve never had before. This \\nnew interface connecting them with the \\ninformation they need will amplify internal \\nknowledge sharing, customer service, \\nproduct capabilities, and much more.  \\n \\nMorgan Stanley, for instance, has a vast \\ninternal knowledge library, including \\nhundreds of thousands of documents \\nranging from investment strategies to \\nmarket research and other insights.44 These \\ndocuments can be found across various \\ninternal sites, mostly in PDF form, so it \\ncan require significant time and energy \\nfor advisors to scan those documents and \\nfind the answers they’re looking for. Now, \\nhowever, with the help of GPT-4, Morgan \\nStanley has created a generative AI chatbot \\nthat can harness this wealth of internal \\nknowledge and help advisors get the insights \\nthey need, instantly.  \\n Technology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  26\\n#TechVision2024\\nThese are just a few examples of opportunities \\nbusinesses can capture when they combine \\ntheir droves of data with the power of \\ngenerative AI. And while the value we already \\nsee is vast – providing the answers employees \\nand customers want in a more accessible way, \\nsaving countless hours and frustrations, and \\nenabling better decision-making across the \\nbusiness – there’s still a lot more to discover. \\n \\nMoving to opportunities outside the \\nenterprise: how do generative AI chatbots \\nchange the way information about  the \\nenterprise is found, say, by customers who \\nare looking to potentially purchase the \\ncompany’s products or services?  \\n \\nIf people’s relationship with data is changing, \\nthen these questions are urgent. Already, \\npeople are replacing traditional search \\nengines with generative AI internet search-\\nbots like Microsoft Bing Chat, or augmenting \\nthe search experience with Google SGE, \\nwhich can provide AI-powered overviews and \\nresponses to people’s searches.49,50 They’re \\ngetting direct answers to their queries in \\nnatural language. And while sources are cited \\nand search results are also provided on the \\npage, the question is, will anyone actually visit \\nthose links?  \\n \\nIf they don’t, what does that mean for \\nwebsites – and what does that mean for \\nbusiness websites in particular? How can \\nbusinesses ensure that their customers get \\nthe right information that they are looking for \\nand need, or the most up-to-date information \\nwith the appropriate sources linked?   \\n This might sound less like an opportunity \\nand more like a problem, but these are open \\nquestions everyone will be tackling in the \\ncoming years. Enterprises still have time  \\nto get ahead and reimagine how they  \\nreach customers.  \\n \\nSome companies are looking at plugins to give \\nexplicit access to external data and improve \\nthe outputs of generative AI chatbots like \\nChatGPT and Bard. For instance, Edmunds.\\ncom and CarGurus.com – websites providing \\ncar inventory, pricing, and reviews – both \\nlaunched ChatGPT plugins to help prospective \\ncar-buyers.51 This way, customers can get \\nup-to-date information and explore cars in \\ntheir own terms and language, without being \\nconstrained by the limited search fields. Today, \\ncompanies with Bard plugins include Redfin, \\nInstacart, and Spotify.52 \\n \\nWhile plugins are one option for now, new \\ntrends are sure to gain steam in coming years, \\nand businesses will need to be willing and \\nready to try new things. Those that do may find \\nthemselves at the cutting edge of change. The \\nshift from search to asking is heralding a new \\nera of data-driven business, and its impact on \\nenterprises’ marketing and content approaches, \\nas well as how current and potential customers \\nfind them, might just decide the winners from \\nthe losers in this new age.Among the many other security implications \\nalready discussed in this trend, companies \\nshould also think about how LLM-advisors \\nmay change user data dynamics.  \\n \\nHistorically, search providers have held all \\nthe power, storing a treasure trove of data \\nabout both companies and their customers, \\nand often leaving people wary of how their \\ninformation was used and who even had \\naccess to it.   \\n \\nNow we have an opportunity to reinvent the \\nethos of search and restore trust between \\nbusinesses and their customers. Companies \\ncan now act as stewards of their own \\ninformation – storing, securing, analyzing, \\nand disseminating their data and institutional \\nknowledge directly to customers through \\ndigital advisors. This is a big responsibility: \\nyour company must ensure that your \\ndata remains secure while yielding high-\\nconfidence responses in your advisory \\nservices. It’s an even bigger opportunity: \\nwithout search providers mediating the \\nexchange of information, companies can \\nserve as a direct source of reliable insight \\nand win back their customers’ trust.  Security \\nimplicationsTechnology Vision  2024  |  Human by design  A match made in AI Technology Vision  2024  |  Human by design  27\\n#TechVision2024\\nConclusion: \\nA match \\nmade in AIGenerative AI is a game-changer for data \\nand software. Just as search did decades \\nago, LLMs are changing our relationship \\nwith information, and everything from \\nhow enterprises reach customers to how \\nthey empower employees and partners \\nstands to transform. Leading companies \\nare already diving in, imagining and \\nbuilding the next generation of data-\\ndriven business. And before long, it won’t \\njust be leaders – it’ll be the new way \\ndigital business works.  \\nIf you’re starting to think differently \\nabout information, then you’re on the \\nright track.Meet my \\nagent\\nEcosystems for AITechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  29\\nMeet my agent\\n2001\\n20231770\\n2011\\n20251892\\n20151961\\n2017\\n20261995 1996\\n2019\\n20282022 2023\\n2030 20321997 1999\\nThe Mechanical Turk,  \\nan “autonomous” chess \\nplayer, is built.1\\nThe National Academy  \\nof Sciences hosts a  \\ncolloquium on agent-\\nbased modeling.8Update allows ChatGPT \\nto receive speech and \\nimage inputs.14Apple releases Siri.9\\nAuto-GPT and BabyAGI \\nare launched.15A new code repository \\nwill launch for open-\\nsource code written  \\nby agents. Three-fourths of \\nknowledge workers will \\nuse copilots every day. The first truly lights-out \\ncar manufacturing plant \\nwill open. One half of home \\nmortgages will \\nbe approved and \\nserviced by agents.Authorities will \\ndismantle an \\ninsider trading \\nring that was using \\nintelligent agents \\nto collect protected \\ninformation.The first automatic \\ntelephone exchange is \\ninstalled.2\\nSchwab Intelligent \\nPortfolios, an \\nautonomous investment \\nadvisor, is launched.10GM successfully \\nintegrates the Unimate \\nrobotic arm in their \\nmanufacturing process.3\\nResearch shows that \\nhumans working with \\nsoftware agents reach \\nsolutions 55.6% faster.11BargainFinder becomes \\nthe first comparison-\\nshopping agent.4\\nDeepMind’s AlphaStar \\nbecomes a Grandmaster \\nin StarCraft II.12Amazon announces \\nProteus, its first fully \\nautonomous mobile \\nrobot.13NASDAQ uses an agent-\\nbased model to simulate \\nthe stock market.6eSnipe, a tool to \\nautomatically place  \\neBay bids, launches.7Microsoft releases  \\nClippy.5Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  30\\nMeet my agent\\nCan an AI agent launch your next product? The big pictureTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  31\\nMeet my agent\\nIt might sound futuristic, but it could happen \\nsooner than you think. Already, enterprises \\nare embedding AI across business operations. \\nGenerative AI has transformed industry leading \\ncreative tools at Adobe and propelled product \\nideation at Volkswagen.16,17 Siemens and Fanuc \\nhave reimagined manufacturing by embedding \\nAI across robotics and industrial processes.18,19  \\nAnd in the last few years, the advent of \\nfoundation models has radically expanded \\nthe deployment of AI to departments like \\nmarketing and sales to rapidly create new \\ncontent and expedite time to market.20  \\n \\nWith all this intelligence at their fingertips, \\nenterprises need to start asking these kinds of \\nquestions: “Can AI launch my next product?” \\n“Can it run my warehouse?” “Can it restructure \\nmy organization?” Otherwise, they’re at risk of \\nthinking too small. \\nWe are beginning to see AI break out of its \\nlimited scope of assistance  to engage more \\nand more of the world through action . Over the next decade, we will see the rise of entire agent \\necosystems – large networks of interconnected \\nAI that will push enterprises to think about their \\nintelligence and automation strategy in  \\na fundamentally different way. \\nA useful analogy for the progression of AI \\nagents is the advancement of self-driving \\ncars. For many years, drivers were entirely \\nresponsible for the operation of the vehicle \\n(no AI). But then semi-automated systems \\nlike cruise control or lane assist came into \\nplay (AI that assists). After that, automated \\ndriving became available to drivers in limited \\nconditions, and then fully self-driving cars \\nrequiring no driver at all (agents with increasing \\naction). And if you extrapolate this trend, we \\ncan imagine a future with self-driving cars that \\nall work together on the road (an ecosystem \\nof agents). For cars, these advances have not \\ncome as precise step changes but as progress \\non a continuum. The evolution of AI agents will \\nbe the same.Today, most AI strategies are narrowly focused \\non assisting in task and function. To the \\nextent that AI acts, it is as solitary actors \\ninstead of an ecosystem of interdependent \\nparts. We might use AI to participate in design, \\nfind manufacturing flaws, or pull insight \\nfrom consumer feedback – but it usually \\nrecommends rather than takes action, and is \\ngenerally siloed, not threaded across an  \\nentire operation.\\nBut now things are beginning to change.  \\nAs AI evolves into agents, automated systems \\nwill make decisions and take actions on their \\nown. Agents won’t just advise humans, they will \\nact on humans’ behalf. AI will keep generating \\ntext, images, and insights, but agents will \\ndecide for themselves what to do with it.\\nLook at DoNotPay, a company designed \\nto help consumers save money – from \\ncontesting parking tickets to identifying \\nunused subscriptions. Until recently, DoNotPay \\nidentified these issues and prompted customers to take action – but then the \\ncompany integrated GPT-4 and AutoGPT into \\nits software.21 The first user of these new \\nfeatures was DoNotPay’s CEO. He gave the \\nagent access to his financial accounts and \\nprompted it with a concise yet complex task: \\nfind me money . The agent identified $81 in \\nunnecessary subscriptions and an unusual \\n$37 in-flight Wi-Fi fee. Then, it offered to \\nautomatically send cancellations to the \\nsubscription providers, drafted a letter to \\ncontest the Wi-Fi charges, and checked in with \\nthe CEO for review. As icing on the cake, it \\neven drafted and sent emails that negotiated  \\na 20% reduction in the CEO’s cable and \\ninternet bill.96% of executives agree that leveraging AI agent  \\necosystems will be a significant opportunity for  \\ntheir organizations in the next three years. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  32\\nMeet my agent\\nBut even as this agent evolution begins, \\ncompanies already need to start thinking about \\nwhat’s next. Because if agents are starting to \\nact today, it’s not long until they start acting \\nwith each other . Tomorrow’s AI strategy will \\nrequire the orchestration of an entire concert \\nof actors: narrowly trained AI, generalized \\nagents, agents tuned for human collaboration, \\nand agents designed for machine optimization. \\nThese agents will build on each other’s efforts, \\nforming an ecosystem that will transform  \\nboth how and what companies are capable  \\nof producing.  \\nInstead of using AI to optimize an isolated \\nbusiness process, agents could command \\nentire chunks of the value chain. Today, AI \\ncan detect manufacturing flaws, but agents \\ncould enable true lights-out manufacturing. AI is already processing orders, yet agents \\ncould sell your product and then get it to the \\ncustomer’s door. Just as the moving assembly \\nline allowed Ford to reimagine what the \\nautomobile market could be, agent ecosystems \\nwill let companies reinvent what they offer \\nand how they offer it. With markedly greater \\nefficiency across multiple departments, will \\nyour prices become accessible to a new \\ndemographic? With greater insights and ideas \\nacross the enterprise, will you create a product \\nthat captures an entirely new market? With \\ncomprehensive access to your organization’s \\ninformation, agent ecosystems could generate \\nopportunities and solutions that neither siloed \\nAI nor siloed humans could conceive.\\nBut there’s a catch: there’s a lot of work to do \\nbefore AI agents can truly act on our behalf, or as our proxy. And still more work before they \\ncan act in concert with each other. The fact is, \\nagents are still getting stuck, misusing tools, \\nand generating inaccurate responses – and \\nthese are errors that can compound in a hurry. \\nWithout the appropriate checks and balances, \\nagents could wreak havoc on your business. \\nInnovative leaders will build the scaffolding \\nthat agents need to gradually earn their \\norganization’s trust and fulfill their explosive \\npotential – and they will turn to human \\nemployees as the first test pilots, deciding \\nwhen and where internal agents should be \\nallowed to fly solo. In other words, people   \\nwill create the support systems that turn \\nagents into reliable actors, and success  \\nhere will determine if agents work for or \\nagainst the organization. Humans and agents are co-dependent; if \\nyou want to reinvent your AI strategy to tap \\ninto agent ecosystems, you need to reinvent \\nyour people strategy, too. Already, 40% of all \\nworking hours across all industries could be \\nimpacted by large language models (LLMs) \\nlike GPT-4.22 And that number is likely to \\ngrow. While we’ve discussed pairing humans \\nand machines at the task-level before, we \\nhave never prepared for AI to operate our \\nbusinesses – until today. As agents are \\npromoted to be our colleagues and our proxies, \\nwe will need to reimagine the future of tech \\nand talent together. It’s not just about new \\nskills, it’s about ensuring that agents share our \\nvalues and goals. Agents will help build our \\nfuture, and it’s our job to make sure they  \\nbuild a world we want to live in. \\nToday, AI can detect  \\nmanufacturing flaws, but agents  \\ncould enable true lights-out  \\nmanufacturing. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  33\\nMeet my agent\\nThe technology:  \\nFrom assistance to  \\nactions to ecosystems\\nCompanies are kicking off their most  \\nimportant transformation of the next decade.\\nEvery step in this AI evolution will introduce \\ndiscrete technologies that, on their own,  \\nhold enormous innovative potential. But it is  \\ncritical that leaders also recognize these \\npieces as part of something greater: the  \\nagent ecosystem.  \\n \\nAI assistants are maturing into proxies that  \\ncan act on our behalf. As these agents emerge, \\nthe resulting business opportunities will \\ndepend on three core capabilities: access to \\nreal time data and services; reasoning through \\ncomplex chains of thought; and the creation \\nof tools – not for human use, but for the use of \\nthe agents themselves. Along with humans to \\nguide and oversee them, these advancements \\nwill allow agent ecosystems to complete \\ntasks in both the physical and digital worlds, generating immense value for every enterprise \\nthat takes part – and risking obsolescence for \\nthose who don’t.\\nStarting with access to real time data and \\nservices – when ChatGPT first launched, a \\ncommon mistake people made was thinking \\nthe application was actively looking up \\ninformation on the web. In reality, GPT-3.5 \\n(the LLM upon which ChatGPT was initially \\nlaunched) was trained on an extremely \\nwide corpus of knowledge and drew on the \\nrelationships between that data to provide \\nanswers. In fact, at that point, if you looked \\nclosely (or even asked it), it would tell you \\nthat the knowledge it held only went up to \\nSeptember 2021. But we live in the present.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  34\\nMeet my agent\\nrequire a series of complex instructions for \\nmachines. Imagine walking across the room \\nto get a glass of water. You don’t have to \\nthink too hard about it, but a machine needs \\nto understand what a glass is, where to find \\nit, how to get there, how to pick it up, how to \\nfill it, and where the sink is before water even \\nenters the glass.  \\nSome may argue that AI has been doing \\ncomplex tasks for some time now, such as \\nplaying chess or packing boxes. But these \\nare relatively specific tasks, in which any \\ndeviation from the pre-trained instructions \\noften results in failure. If we can’t find a cup \\nwhere we thought we left it, we adjust and go \\nlook for it. If a narrow AI is faced with a similar \\ndisruption, it’s usually programmed to abort \\nor restart the task. Even today’s LLMs face \\nsimilar limitations. With no internal memory, \\nthe ability to manage complex sequences has \\nbeen a problem. It may not abort the request, \\nbut it could do something like confidently \\nprovide a false answer.  \\n \\nBut AI research is starting to break down \\nbarriers to machine reasoning. Chain-of-\\nthought prompting is an approach developed \\nto help LLMs better understand steps in a \\ncomplex task.25 It started with researchers \\nrealizing they could provoke better outcomes \\nby breaking down prompts into explicit steps, \\nor even prompting the model to “think about \\nthis step by step.” Further research showed \\nthat by utilizing few-shot techniques and \\nproviding the model with several chained-\\nreasoning examples, the model would adapt \\nto follow a similar sequence in other tasks.26 For any tool to become a meaningful agent,  \\nit will need to combine the skills developed \\nfrom a carefully cultivated historical record  \\nwith the current information that comes  \\nfrom a rapidly expanding dataset of current \\nevents and knowledge.\\nIn March 2023, OpenAI announced the first \\nplugins for ChatGPT. “Plugins” allow LLMs \\nto look up information, use digital software, \\nexecute code, call APIs, and generate outputs \\nbeyond text by allowing the model to access \\nthe internet. Instead of relying solely on the \\nweights and tokens that make up the model’s \\nintelligence, ChatGPT can now search Expedia \\nto get travel information, access Instacart \\nfor ordering groceries, and engage Wolfram \\n(a computational intelligence platform) to \\nperform complex mathematical calculations.23 \\nAfter just a few months, ChatGPT had access to \\nhundreds of plugins.24 By the time you read this, \\nthose numbers may be higher.\\nThese plugins transform foundation models \\nfrom powerful engines working in isolation to \\nagents with the ability to navigate the current \\ndigital world.\\nWhile plugins have powerful innovative \\npotential on their own, they’ll also play a  \\ncritical role in the emergence of agent \\necosystems. Today, plugins give AI access to \\nour most relevant digital tools, but tomorrow \\nplugins could allow agents to engage powerful \\nAI models, enabling far more than AI has  \\never done alone.  \\n \\nThe second step in the agent evolution is the \\nability to reason and think logically – because \\neven the simplest everyday actions for people \\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  35\\nMeet my agent\\nThis style of prompting can initially require \\nhuman input, but research continues to reveal \\nthat models can be engineered to self-critique \\nand file information into their working memory– \\nopening the door to automating this type of \\nreasoning.27 AutoGPT and BabyAGI are two \\nopen-source applications that leverage LLMs \\nand automate chain-of-thought prompting. \\nThese applications will take broad queries or \\ninstructions, and then prompt themselves to \\nthink through the steps and ways to accomplish \\ntheir goals, articulating for themselves a \\ndetailed set of instructions that they will then \\nuse to accomplish the original ask.28,29\\nBetween chain-of-thought reasoning and \\nplugins, AI has the potential to take on complex \\ntasks by using both tighter logic and the \\nabundance of digital tools available on the web. \\nThey imbue AI with the potential to navigate \\nmore uncertainty and with more solutions, \\nopening up far greater opportunities for \\nbusinesses. But what happens if the required \\nsolution isn’t yet available?\\nWhen humans face a challenge that exceeds \\nour equipment, we acquire or build the tools \\nwe need. \\nWe run out to the hardware store, write a piece \\nof code, or otherwise find what we can use \\nto overcome the challenge. AI used to rely on \\nhumans exclusively to grow its capabilities. But \\nthe third dimension of agency we are seeing \\nemerge is the ability for AI to develop tools  \\nfor itself.Not only does Google’s research demonstrate \\nthe rapidly expanding capabilities of AI to \\nact, but it signals the beginning of multiagent \\ninteraction, as well as the opportunity that \\ncomes with it. LLMs today require immense \\ncomputing resources, making them expensive \\nto develop and run. LATM proposes a \\nspecialization strategy, where requests \\nthat have already been solutioned become \\nmore routine – and therefore executable by \\nlightweight models. The “tool maker” does the \\nheavy lifting, reducing the computational  \\ncosts required.\\nAnd LATM isn’t the only time Google has \\nexplored multi-agent interaction. In 2023, \\nGoogle ran an experiment where they put \\n25 distinct agents, each with their own \\nperspectives and backstories, into a virtual \\ntown.33 The agents were given the freedom \\nto interact with one another and the ability to \\nstore those interactions as memories, which \\nthey could reference later. What Google found \\nwere emergent social behaviors. When they \\nprompted one agent to throw a Valentine’s Day \\nparty, it invited other agents to the party, who \\nin turn asked each other to go as dates. Take Nvidia, which along with researchers from \\nseveral universities, explored the possibility \\nof developing an “embodied agent.” They \\nbuilt their agent – Voyager – in Minecraft, a \\npopular game about survival and exploration \\nthat takes place in a 3D world.30  To navigate \\nthis world, players acquire resources that \\nallow them to forge new tools, like a pickaxe \\nor lantern, which let them further traverse \\nand shape their environment. Voyager was \\ngiven the instruction to explore, and it was \\nequipped with a skill library it could add to over \\ntime. As Voyager met new barriers, it would \\nlearn which tools were needed to overcome \\nthe obstacle, then store that information in its \\nlibrary. When encountering further obstacles, \\nit would increasingly draw from its skill \\nlibrary – effectively, actions it taught itself – \\nto overcome them faster.31  The game has a \\nhierarchy of skills and tools, where players \\ncan only move up by mastering the lower-\\nlevel skills. Researchers were able to confirm \\nVoyager’s learning ability by watching it move \\nup this skill tree. Tool-building agents aren’t just confined to \\nsimulations, they have real world potential. \\nResearchers from Google, Stanford, and \\nPrinceton are working on generalizing this tool-\\nmaking ability.32  \\n \\nIn their paper “Large Language Models \\nas Tool Makers” (LATM), the research team \\ntook a novel approach to how AI can create \\nnew, reusable tools to solve problems. They \\ndeveloped a closed-loop system comprised \\nof two distinct AI models: the tool maker and \\nthe tool user. Instead of relying on a single \\nmodel to accomplish an entire request, LATM \\ntakes a collaborative approach. As the model \\nreceives requests, the “tool maker” creates \\nPython functions that accomplish the objective.  \\nBut rather than executing the function itself, \\nthe maker hands it off to the “tool user” – a \\nseparate, more lightweight AI model. Over time, \\nthe tool user can respond to requests that fit \\nits growing set of tools, and the tool maker \\nimproves tools over time by learning from \\nsimilar requests. \\nTool-building agents aren’t just \\nconfined to simulations – they have \\nreal world potential, too. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  36\\nMeet my agent\\nSocial behaviors in agents can enhance \\noutcomes for the entire ecosystem. In another \\nexample, researchers from the Allen Institute \\nfor AI simulated negotiations between a \\nbuyer agent and a seller agent, with a third-\\nparty critic agent that provided feedback to \\nimprove bargaining.34 The buyer and seller \\nmodels incorporated feedback from the critic \\nto improve future negotiation rounds. This \\ndiversity of perspectives could serve several \\npurposes: a system of checks and balances \\nto strengthen decision making; a productivity \\nmechanism; or divergent inspiration to create \\nnovel solutions.\\nFrom real-time information to reasoning, tool \\ncreation, and multiparty interaction, valuable \\nagent breakthroughs are happening fast.\\nBut this is why it is so critical to maintain focus \\non the evolution of the whole ecosystem – \\nbecause as independently valuable as each of \\nthese developments are, their combination  \\nwill spark a revolution in how we apply  \\nartificial intelligence.\\nAgents, for example, can already automate \\nentire tracts of scientific research by looking \\nfor information on the web, consulting scientific documents, and using scientific \\nequipment in a cloud lab.35 Google’s PaLM-E \\ncan take a command in natural language, \\nbreak it down into a series of subtasks, then \\ngenerate and execute commands to control \\nphysical robots.36 It’s not difficult to imagine \\nsuch an agent leading an entire manufacturing \\nplant. And MetaGPT can automate an entire \\nsoftware development stream by acting as a \\nproduct manager, architect, project manager, \\nand engineer all rolled into one, delegating \\ntasks to its array of GPTs. From one line of \\ntext, MetaGPT can generate user stories, \\ncompetitive analyses, requirements, data structures, APIs, documents, and beyond.37\\nThe agent ecosystem may seem overwhelming. \\nAfter all, beyond the three core capabilities of \\nautonomous agents, we’re also talking about \\nan incredibly complex orchestration challenge, \\nand a massive reinvention of your human \\nworkforce to make it all possible. It’s enough to \\nleave leaders wondering where to start.  \\n \\nThe good news is existing digital transformation \\nefforts will go a long way to giving enterprises \\na leg up. Data modernization and creating \\nlibraries of APIs will be key to integrating \\nenterprises’ systems into the AI ecosystem. It\\'s \\nimportant to remember, though, that these \\nmodels are not without their own drawbacks. \\nFaulty responses remain inherent to LLMs. And \\nmuch more research is needed on the risk and \\ncybersecurity implications of leveraging these \\nmodels. How enterprises balance the division \\nof work between human and machine will be a \\ndelicate process that must, above all, prioritize \\nhuman needs and benefits, not just what’s \\npossible with the technology.\\nBut make no mistake: the next decade will see \\nthe emergence of the agent ecosystem and \\nthe enterprises who embrace it will effectively \\noutpace their competition. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  37\\nMeet my agent\\nThe implications:  \\nAligning tech and  \\ntalent in the workforce \\nWhat happens when the agent ecosystem  \\ngets to work? Whether as our assistants or \\nas our proxies, the result will be explosive \\nproductivity, innovation, and the revamping  \\nof the human workforce.  \\n \\nAs assistants or copilots, agents could \\ndramatically multiply the output of individual \\nemployees. For the enterprise processes that \\nwill always depend on humans, agents will \\nact as collaborators. Diagnosing a medical \\ncondition? Agents could help, but they won’t \\nshare the diagnosis with the patient. Need \\nto inspire your team? Agents could write the \\nspeech, but they won’t deliver it. As copilots, \\nagents and humans will complement each \\nother, each playing to their own strengths. \\n \\nIn other scenarios, we will increasingly trust \\nagents to act on our behalf. As our proxies, they could tackle jobs currently performed \\nby humans, but with a giant advantage – a \\nsingle agent could wield all of your company’s \\nknowledge and information. Their knowledge \\nbase would far surpass that of your most senior \\nhuman employees, and agents could act on \\nthis knowledge everywhere, all at once. When \\nthey don’t have the information they need, \\nthey could create it. When they don’t have the \\nproper tools, they could build them. \\nHumans have limits on their knowledge and \\ntheir ability to take action. For agents, many  \\nof those limits won’t apply.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  38\\nMeet my agent\\nSo, what happens when agents work  \\ntogether? Imagine you need to boost sales  \\nfor a struggling product. Your Product \\nManagement Agent works with your Finance \\nAgent to set a growth target. Your Business \\nDevelopment Agent identifies new potential \\ncustomers, and your Marketing Agent creates \\naligned campaigns. Such a network of agents \\ncould act and iterate continuously, pivoting \\nafter a missed target and doubling down when \\nthey hit the mark. The agent ecosystem is an \\ninexhaustible source of productive innovation.\\nThe companies that promote trusted agents to \\npositions of power will discover new products, \\nservices, and capabilities. The more power we \\ngive agents over the value chain, the more \\nvalue they can create. When we arm agents \\nwith information and tools, many of their \\nabilities will transcend ours – meaning every \\ncompany and every person will be empowered \\nto do and create more than they ever could \\nbefore. No digital market will ever be the same. \\n \\nBusinesses will need to think about the human \\nand technological approaches they need to \\nsupport these agents. From a technology \\nside, a major consideration will be how these \\nentities identify themselves. Today, machines \\nmake up 43% of identities on enterprise networks.38 But they don’t act alone, and we \\nhave an existing security framework for how \\nthey connect. As agents take more actions on \\ntheir own, with behaviors that may mimic their \\nhuman counterparts, technologies like Web3, \\ndecentralized identity, or other emerging \\nsolutions will become critical to making \\nsure these agents can properly identify and \\nauthenticate themselves.  \\n \\nYet while the framework of technology is a core \\nconsideration, the impacts on human workers – \\ntheir new responsibilities, roles, and functions – \\ndemand even deeper attention. To be clear, \\nhumans aren’t going anywhere. Yes, your \\npeople will have extra capacity, but they are \\ngoing to need it. As agents take over enterprise \\nfunctions, it won’t be a purely machine \\noperation. Humans will make and enforce the \\nrules for agents. It’s time to rethink your talent \\nstrategy to prepare your people for this  \\nnew reality.  \\nToday, machines make  \\nup 43% of identities on  \\nenterprise networks. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  39\\nMeet my agent\\nRethinking human talent \\nWhat brave new world will agents inspire for \\nyour organization? The answer should come \\nfrom your humans, not from your agents.  \\n \\nIn the era of agent ecosystems, your most \\nvaluable employees will be those best \\nequipped to set the guidelines for agents. As \\nagents build their autonomy, humans must \\nmake and enforce the rules to ensure that \\ntheir proxies act for the betterment of the \\ncompany and the people within it. As humans \\nare empowered by these agents to do more \\nthan they ever could before, both must have \\nthe company’s North Star in mind. Whatever \\nchoices and decisions your employees make, \\nfor better or worse, are about to be amplified. \\nA company’s level of trust in their autonomous \\nagents will determine the value their agents \\ncan create. Your human talent is responsible \\nfor building that trust. Agent ecosystems will \\ntake actions without humans, but they won’t In the era of agent  \\necosystems, your most  \\nvaluable employees  \\nwill be those best  \\nequipped to set the  \\nguidelines for agents. always take the right actions. Before unleashing \\nagents, humans need to embed rules, \\nknowledge, and reasoning skills, and then \\nrigorously test agents to ensure their readiness. \\nAs agent ecosystems evolve, humans have \\ntwo primary responsibilities to engender \\ntrust in semi-autonomous systems: building \\nagent support systems and refining machine \\nreasoning.  \\n \\nEmployees at frontier organizations are already \\ndriving autonomous AI toward accurate \\nactions by curating their agent support \\nsystems. Existing LLMs are trained on massive \\namounts of information, which allows tools like \\nChatGPT to answer a range of questions with \\nmoderate accuracy. But if an agent controls \\nyour supply chain, for example, it first and \\nforemost requires expertise on your supply \\nchain – and extraneous information could lead \\nyour agent astray. As your employees embed \\nyour enterprise knowledge, proprietary data, and external tools into autonomous AI, these \\nsupport systems can dictate the information  \\nthe AI systems prioritize. \\nInvestment research company Morningstar has \\nsuccessfully focused its GPT-3.5-embedded \\nchatbot “Mo” on relevant proprietary \\ninformation by providing such a support \\nsystem.39 Prompt-tuned on more than 10,000 \\npieces of proprietary research, Mo serves as an \\nadvisor to Morningstar’s financial advisors and \\ncustomers – and it’s able to do this because \\nMorningstar’s human workforce set the stage in \\nthe background. Specifically, one Morningstar \\nteam created rules for what Mo can and can’t \\nanswer, and Morningstar’s lawyers ensured that \\nnone of Mo’s capabilities violated ethical or \\nregulatory bounds.40 Morningstar proactively \\ndeployed their humans to put bounds on Mo \\nbecause reactive trial-and-error isn’t an option \\nwhen you’re dispensing financial advice.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  40\\nMeet my agent\\nEnterprise knowledge can no longer exist solely \\ninside your employees if it’s to be any value to \\nagents. Now, your human talent must be able \\nto extract their knowledge and skills so that it \\ncan be transferred to and used by your digital \\nagents, too.\\nAs agents get access to the right information, \\nhumans must also teach agents how to reason \\nabout that information.\\nIn its simplest form, humans will test and \\ncorrect agent reasoning. This is already \\nhappening at some companies today. For \\nexample, Morgan Stanley fine-tuned GPT-4 \\non 100,000 internal documents, creating an \\nagent that answers questions for their financial \\nadvisors.41 Their employees regularly ask the \\nagent a series of “golden questions” to make \\nsure its “thinking” stays sharp. When the \\nsystem answers incorrectly, these knowledge \\nmanagers go back into the training documents \\nto figure out what needs to be fixed. \\nBut it isn’t enough to just think logically, agents \\nalso need to understand their limits. When \\ndoes an agent have enough information to act \\nalone, and when should it seek support before \\ntaking action? The specifics will vary agent to \\nagent, company to company, and industry to \\nindustry. But across the board, humans will \\ndecide how much independence to afford their \\nautonomous systems. Humans need to teach \\nagents how to determine what they know, and \\nmore importantly what they don’t know, so \\nthat agents can gather the information and \\ncertainty needed to keep working. Self-examining agents won’t hit the workplace \\nin the immediate future, but the seeds of a \\nreflective generation of agents are already \\nbeing sown. LLM-based planners are now \\ncapable of determining their level of certainty \\nfor an action and reaching out for human \\nsupport when confidence is low.42 When the \\nstakes aren’t high, a lower confidence level \\nmay suffice. An agent creating marketing \\nmaterial is unlikely to face life-or-death \\ndecisions. But when the stakes are  high, the \\nonly options are to act with certainty or not \\nat all. The human knowledge and reasoning \\nskills that an agent absorbs will determine \\nthe agent’s competitive edge in the broader \\necosystem. In other words, agents are only as \\nvaluable as the humans who teach them.\\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  41\\nMeet my agent\\nWhat sort of human talent will give you an \\nadvantage? In the agent ecosystem era, your \\ntalent must have an intricate understanding \\nof your company’s values and mission \\nand the ability to affect that vision to the \\nagent ecosystem. If employee, agent, and \\ncompany goals fall out of alignment, your \\nactors will move fast in different directions. \\nWhen this happens, the best-case scenario \\nis stalled growth, the worst-case scenario \\nis organizational destruction. When your \\nactors are aligned, however, their actions will \\ncompound to accelerate your company towards \\nits far-reaching goals. \\nWith that potential acceleration of action, \\nproductivity, and value, businesses will need \\nto decide what to do with it all. Will you create \\nnew products and enter new markets? Will you \\npay your employees more or embrace a four-\\nday work week? In fact, all of these choices \\ncan be beneficial. Think again of Ford and \\nthe introduction of the moving assembly line. \\nFord was able to increase wages and decrease \\nworking hours, which not only attracted \\nworkers but afforded them the time and wages \\nto drive the very cars that they were building. \\nThis new era could usher in the paradigm shift \\nfor work that many have been waiting for. Even \\nbetter, everyone could benefit.Agents are only as valuable as  \\nthe humans who teach them. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  42\\nMeet my agent\\nWhat companies can do now\\nWhat can you do now to set your human and \\nagent workforce up for success? Give agents a \\nchance to learn about your company, and give \\nyour company a chance to learn about agents. \\n \\nCompanies can start by weaving the connective \\nfabric between agents’ predecessors, LLMs, \\nand their support systems. There are many \\nmature generative AI models and a few digital \\ncopilots ready to be linked up to the humans, \\ndata, tools, and robots that are already critical \\nto your company. By fine-tuning LLMs on \\nyour company’s information, you are giving \\nfoundation models a head-start at developing \\nexpertise. The sooner you prepare your \\ninfrastructure and information to be acted upon \\nby agents, the sooner your future agents will be \\nready to fulfill their potential: acting as human \\nproxies within and outside of your organization. \\nFor now, this introduction will require rethinking \\nsome of your data management practices, such \\nas vectorizing databases, providing new APIs \\nfor accessing data, and expanding your tools to \\ninterface with corporate systems.It\\'s also time to introduce humans to their \\nfuture digital co-workers. Companies can lay \\nthe foundation for trust with future agents by \\nteaching their workforce to reason with existing \\nintelligent technologies. Challenge your \\nemployees to discover and transcend the limits \\nof existing autonomous systems. Help your \\npeople develop well-defined rules for when \\nthey can and cannot trust the autonomous \\nsystems at their disposal. In other words, train \\nand upskill your human workforce such that \\nthey are ready and excited to take the reins – \\nand know just how tightly to hold them – when \\nagent ecosystems hit mainstream.\\nFinally, let there be no ambiguity about your \\ncompany’s North Star. Every action your agents \\ntake will need to be traced back to your core \\nvalues and a mission, so it is never too early \\nto operationalize your values from the top to \\nthe bottom of your organization. When your \\nproxies start accelerating and amplifying the \\nwork of your human workers, they won’t stop \\nuntil they reach their goals. It’s time to get \\ncrystal-clear about what those goals are. \\nBy fine-tuning LLMs on your company’s  \\ninformation, you are giving foundation models  \\na head start at developing expertise. \\nFrom a security standpoint, agent \\necosystems will need to provide \\ntransparency into their processes and \\ndecisions. Consider the growing recognition \\nof the need for a software bill of materials –  \\na clear list of all the code components and \\ndependencies that make up a software \\napplication – so as to let companies and \\nagencies under the hood. Similarly, an agent \\nbill of materials could help explain and track \\nagent decision-making.  \\nWhat logic did the agent follow to make a \\ndecision? Which agent made the call? What \\ncode was written? What data was used and \\nwith whom was that data shared? The better \\nwe can trace and understand agent decision-\\nmaking processes, the more we can trust \\nagents to act on our behalf. When agents err \\nor overshare, these vulnerabilities can only \\nbe identified if the agent decision-making \\nprocess is decipherable. This won’t always \\nbe as onerous as it sounds: as humans \\nrefine agent thought processes and memory, \\nagents can become self-healing – relying \\non the vulnerabilities of past agents to drive \\na more accurate, autonomous, and secure \\nagent evolution.  Security \\nimplicationsTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  43\\nMeet my agent\\nConclusion: \\nMeet my agentAgent ecosystems have the potential \\nto multiply enterprise productivity and \\ninnovation to a level that humans can \\nhardly comprehend. But they will only \\nbe as valuable as the humans that guide \\nthem; human knowledge and reasoning \\nwill give one network of agents the edge \\nover another.  \\n \\nToday, artificial intelligence is a tool. In \\nthe future, AI agents will operate our \\ncompanies. It is our job to make sure \\nthey don’t run amok. Given the pace of \\nAI evolution, the time to start onboarding \\nyour agents is now. The space \\nwe need  \\nCreating value in  \\nnew realitiesTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  45\\nThe space we need\\n1957\\nMorton Heilig \\ninvents the \\nSensorama, a \\nmultisensory \\nimmersive movie \\nexperience.11973 1975 1992 2006\\n20112003 2008 1999\\nThe first tactile \\ntelephone is \\npatented.2Xerox PARC \\nreleases the \\ngraphical user \\ninterface (GUI).3Louis Rosenberg \\ncreates the first \\ninteractive AR \\nsystem.4Roblox \\nofficially \\nlaunches.7  \\nMicrosoft’s Kinect, \\nfor Xbox gesture \\nand voice control, \\nbecomes the \\nfastest-selling \\nconsumer device.9Simon Greenwold \\ncoins the \\nterm Spatial \\nComputing.6 BMW runs \\nthe first AR \\nadvertisement.8The first camera \\nphone is released.5\\n2012 2013 2016 2017 2020 2019 2018\\nOculus VR is \\nfounded.10Google Glass \\nsales begin.11Pokémon Go \\nreaches 228 million \\ndownloads in its \\nfirst quarter.12Apple announces \\nARKit for \\ndeveloping  \\nAR apps.13Nvidia releases \\nthe Omniverse \\nplatform.16Snapchat \\nlaunches \\nLandmarkers \\n- AR overlay \\ntechnology.15Not Impossible \\nLabs creates a \\nhaptic suit to \\nlet people feel \\nmusic.14 \\n2023 2026 2027 2028 2031 2030\\nApple \\nannounces the \\nApple Vision \\nPro spatial \\ncomputer.18A professional \\nsports league \\nwill launch \\nan immersive \\n3D replay and \\nhighlight platform.A major city \\nwill add spatial \\nentertainment, \\ndirections, and \\ninformation to \\npublic spaces.A state public \\nschool system \\nwill announce \\noffering physics \\ncourses taught \\nentirely in an \\nimmersive spatial \\nenvironment.A news site \\nspecializing \\nin spatially \\nimmersive content \\nwill become the \\nfastest growing \\nnew media \\ncompany. The gaming \\nmarket will be \\ndominated by \\nVR and spatially \\nimmersive \\ngames.2021\\nMicrosoft Mesh, \\nan immersive \\ncollaboration \\nplatform, is \\nreleased.17 Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  46\\nThe space we need\\nThe big picture\\nWhen the original Macintosh launched in 1984,  \\nit was met with skepticism. The mouse was called \\n\"useless\" and \"awkward.\"19 In 2001, when Apple \\nlaunched the iPod, it was criticized for entering a \\ncrowded market at too high a price point. The iPad? \\nIt was scoffed as a glorified iPod touch, an absurdity \\nbecause “how could anyone get serious work done \\nwithout a mouse?”20,21Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  47\\nThe space we need\\nApple got the last laugh. Today the Macintosh \\nis considered a revolutionary product, as \\nis the iPod, iPhone, and iPad. They each \\nshaped the world of computing in unique \\nand transformative ways. Still, in 2023 when \\nApple launched the Vision Pro, its mixed reality \\nglasses, it seemed the critics hadn’t learned \\ntheir lesson. \\nThis was Apple’s entry into spatial computing – \\nan already growing market – and should \\nsignal to every enterprise leader that a new \\ntechnology medium has arrived. Yet few are \\nrecognizing the moment for what it is. \\nSpatial computing is about to change the \\ncourse of technology innovation and the ways \\npeople work and live. Whereas desktop and \\nmobile used screens as portals to the digital \\nworld, spatial will finally combine our disparate \\nrealities, fusing digital and physical together. \\nApps built for this medium will let people \\nimmerse themselves in digital worlds with a \\nphysical sense of space, or layer content on \\ntop of their physical surroundings. \\nIt\\'s a huge moment. Distinct technology eras \\nare shaped and defined by the computing \\nmediums we use. Desktop introduced Spatial computing is  \\nabout to change the  \\ncourse of technology  \\ninnovation and the  \\nways people work  \\nand live. consumers to the information world. Then \\nmobile untethered the digital world, letting us \\ntake computers everywhere. And throughout \\nthe explosive technology innovation of the \\npast decades, desktop and mobile were the \\nfoundation of it all. The fact is, computing \\nmediums don’t change very often, and it’s a  \\nbig deal when they do. \\nSo, why doesn’t it feel like we’re at the \\nbeginning of a new technology era? Why are \\nwe inundated instead with talk of a “metaverse \\nslump”? The metaverse is one of the best-\\nknown applications of spatial computing.  \\nBut just look at the price of digital real estate, \\nbooming in 2021 and 2022, down 80-90% \\nin 2023.22 While some early endeavors are \\nsucceeding, why are so many others  \\nfalling flat?\\nThis is why we need to remember the Macintosh. \\nNew mediums don’t come very often, and when \\nthey do, the uptake is slow. But the payoff for \\ndiving in early is nearly immeasurable.\\nSome companies are holding off, content to \\nsay metaverse hype outpaced technology \\nmaturity. But others are racing ahead, building \\nthe technology capabilities themselves. Meta has been rapidly developing its Reality Labs \\nVR and AR products, and introduced Codex \\nAvatars, which use AI and smartphone cameras \\nto create photorealistic avatars.23,24 Epic’s \\nRealityScan App lets people scan 3D objects \\nin the physical world with just their phone and \\nturn them into 3D virtual assets.25 Underlying it \\nall, advancing technologies like generative AI \\ncontinue to make it faster and cheaper to build \\nspatial environments and experiences. And, \\nperhaps quietly, these technologies are already \\nbeing proven out in industrial applications. \\nDigital twins for manufacturing, the growth \\nof VR/AR in training and remote operation, \\nand the establishment of collaborative design \\nenvironments are all already having practical – \\nand valuable – impacts on industry. \\nAcross the board, forward-thinking companies \\nacting today recognize a core truth: expecting \\nimmediate, mass adoption of a new medium is \\nunrealistic, but wait too long and you’ll spend \\nthe next five or ten years trying to catch up.\\nBut while the supporting technology is \\nradically improving, it’s only the first hurdle.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  48\\nThe space we need\\nEnterprises that fail to see the significance of a \\nnew computing medium will struggle to get the \\napplications right, too. Think back to the move \\nfrom desktop to mobile. Google Maps debuted \\nin 2005 as a desktop app, and it changed how \\npeople navigated the world.26 But people still \\nprinted out their routes to take them on-the-go. \\nThen the smartphone arrived. Google met the \\nmoment by unveiling a mobile Google Maps, \\nwhich drew on real-time user data to refine \\nits accuracy at staggering speed. Now, nearly \\nwherever you and your phone go, you can get \\nfrom Point A to Point B. More than one billion \\npeople today use mobile Google Maps.     \\n \\nThis success happened because Google \\ndidn’t just put Google Maps on the phone – it \\nchanged what the product was to meet the \\nnew medium’s advantages. And that’s exactly \\nhow enterprises need to approach spatial \\ncomputing. Existing concepts of what an app \\nis no longer apply. If enterprises want to build \\nenriching experiences that truly improve on \\nwhat we had before, their designs must match \\nthe new medium. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  49\\nThe space we need\\nIt sounds simple, but it isn’t. Spatial computing, \\nwith its ability to blend physical and digital, is \\nstill mostly uncharted territory. Think about your \\nfirst website or mobile application. Did your \\ncompany get it right immediately? Or did it take \\ntime to learn from mistakes? Just as before, \\nit will take time for enterprises to build the \\nskills, infrastructure, and experience necessary \\nto deliver new experiences to customers. If \\nenterprises delay, waiting for spatial computing \\nto hit some imagined saturation, they are \\ncommitting to being too late.  \\n \\nSpatial is quickly becoming a key part of the \\nenterprise fabric. Already, early adopters are finding ways to unlock its unique advantages, \\nand those that follow can rapidly benefit from \\nthese learnings. Successful spatial computing \\ndeployments in industrial settings have shown \\nit can be used to better convey massive \\namounts of complex information by tapping \\ninto multiple senses and communication \\navenues at once. Other experiments have \\nfound that when we see applications as \\n“spaces,” we can mold experiences to the \\nindividual’s environment and gestures, or \\ngive them freedom to self-direct. Mobile and \\ndesktop users, in contrast, could only click \\nor swipe where the design let them. And with \\nspatial computing able to augment our physical environments, it can lessen our need for bulky \\noffice equipment and to repeatedly update \\nphysical spaces.  \\n \\nA new computing medium is exceptionally \\nrare, and so a tipping point lies ahead. \\nSpatial computing could grow to be as \\ngroundbreaking as desktop and mobile, \\nushering in a new era of technology innovation. \\nBut to succeed, enterprises need to rethink \\ntheir position on it, starting today. They need \\nto get out of the slump and recognize this \\nmoment for what it is. The tools are more  \\nready every day – how you apply them is  \\nwhat matters now.\\n92% of executives agree their organization  \\nplans to create a competitive advantage  \\nleveraging spatial computing. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  50\\nThe space we need\\nThe technology:  \\nToday’s spatial  \\ntechnology landscape\\nThere’s no doubt that delving into the world of \\nspatial computing is a tall order for enterprises.  \\n3D assets are notoriously difficult to develop. \\nQuestions around interoperability and design \\nbest-practices are still being answered. And \\neven if a company invests in building a spatial \\napp, one of that app’s most important features \\nwill be to feel lived in and shared. The same \\nway walking into an empty office can be \\neerie, metaverse worlds without a population \\nfeel empty, with few users wanting to return. \\nDespite all this, and despite critics saying it’s \\ntoo early, spatial computing continues to grow.\\nThe truth is: the time to start is now.\\n The market is rapidly shifting from a space \\nwhere enterprises feel they can safely lag, to \\none where they will quickly need to catch \\nup. We are reaching a turning point with the \\ntechnology where the cost to create and  adopt \\nis coming down, and major advancements \\nare being made in how to build spatial apps, \\nmake them feel real, and ultimately live up \\nto their potential. And capitalizing on these \\ntechnologies will take new skills, investments, \\nand the institutional knowledge necessary to \\nexecute successfully. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  51\\nThe space we need\\nFirst, new standards, tools, and technologies \\nare making it easier – and cheaper – to build \\nspatial apps and experiences. It’s essential to \\nunderstand just how important this is.  \\n \\nThink about every website you’ve visited or \\nyour favorite apps on your phone. Even if \\ntheir purposes are different, something feels \\nundeniably familiar across even the most \\ndisparate experiences. Why? They all used the \\nsame foundation.\\nThat consistency came through languages \\nlike HTML and the TCP/IP protocol that gave \\nwebsites a common look and feel, as well \\nas uniform ways for us to access sites. For \\nmobile apps, the app stores provided design \\nguidelines, ensuring apps held to uniform \\nprinciples. This not only made the experience \\nstickier for users, but easier for enterprises \\nto design because they had a general understanding of what something should  \\nlook like. This improved the ecosystem, too, \\nsince contributors knew what types of apps  \\nor websites to expect to support.\\nFor a long time, spatial never had such a \\nfoundation. And when trying to build worlds \\nthat feel real, that lack of consistency is a \\nmajor roadblock. Builders of digital spaces \\nhave to consider dozens more factors beyond \\nwhat a web designer would weigh. Textures, \\nshapes, lighting, and physics are just a few \\ncritical components for a space’s look and feel. \\nAnd because 3D modelling came into spatial \\ncomputing out of disparate tracts – ranging \\nfrom game design to film animation  \\nto industrial-type applications like architectural \\ndrawing or CAD modelling – there was a  \\nwide array of languages, file types, vendors, \\nand design approaches. The ecosystem  \\nwas fractured. Enter Universal Scene Description (USD), or \\nwhat can best be described as a file format \\nfor 3D spaces. Developed by Pixar, USD is a \\nframework that lets creators map out aspects \\nof a scene, including specific assets and \\nbackgrounds, lighting, characters, and more. \\nSince USD is designed around bringing these \\nassets together in a scene, different software \\ncan be used across each one, enabling \\ncollaborative content building and non-\\ndestructive editing.27 USD might sound like \\nits main use is in entertainment applications, \\nbut it is quickly becoming central to the most \\nimpactful spatial applications, notably within \\nindustrial digital twins.\\nNvidia currently uses USD within its Omniverse \\nplatform for designing digital twins, as \\nwell as within Isaac Sim, its platform for \\ndeveloping and testing robotics in physically \\naccurate digital environments.28 And critically, companies that are driving forces in the \\nspatial revolution are adopting it as well. \\nApple included it as part of the VisionOS SDK, \\nand in 2023 Pixar, Nvidia, Apple, Autodesk, \\nand Adobe all helped launch the Alliance \\nfor OpenUSD in an effort to standardize the \\n3D ecosystem and help the world of spatial \\ncomputing flourish.29,30\\nWith some companies working on making \\nspatial worlds look and feel similar, others are \\nfocused on standardizing how we access those \\nexperiences. OpenXR is an open API standard \\nthat has now been adopted by most major \\ndevice manufacturers.31 It ensures applications \\ncan guarantee uniformity in head and hand \\nposition, controls, visual display, and more \\nacross most devices by using a single API \\n(rather than testing and designing for all of \\nthem independently). First, new standards, tools, and technologies are  \\nmaking it easier – and cheaper – to build spatial  \\napps and experiences. It’s essential to understand  \\njust how important this is. \\nBuilding spatial appsTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  52\\nThe space we need\\nCritically, enterprises need to understand they \\nwill not be operating spaces in isolation. Just as \\nno webpage or app exists on the internet alone, \\nthe next iteration of the web promises to bring \\nthese parallel experiences even closer together. \\nDigital identity and Web3 will play a major \\nrole underpinning these spaces, from how to \\nmove a pair of digital pants or our payment \\ninformation from one space to another, to \\nhow we identify the entities operating within \\nthose spaces. Today these technologies might \\nnot seem intrinsic to developing a successful \\nspace, but they will soon inform the long-term \\nviability and value of the space to the customer. \\nSome are already taking this into account. The \\nOpen Metaverse Alliance for Web3 (OMA3) is \\nbuilding a standard for how we move across \\nexperiences. Today, if you want to move from \\none metaverse world to another, you need \\nto quit one application and move to the next, \\nalmost like quitting and relaunching your \\nbrowser every time you want to go to a new \\nwebsite. In 2023, OMA3 launched a project \\ncalled the Inter-World Portaling System, an \\neffort to develop a protocol that would let \\ndevelopers move users from one space to \\nanother without breaking the immersion, like \\nhow an address bar sits at the top of whatever \\nwebsite you visit.32\\nBut as important as interoperability is, it still \\ndoesn’t help if developing 3D assets is very \\nexpensive – which, historically, it has been. One \\nestimate from 2020 had the average cost of \\n3D models anywhere from $40 to thousands \\nof dollars per asset. And 3D scenes will require \\na lot of assets.33 Yet this is one area where the \\ncost pressure is beginning to break. Generative AI is speeding up the creation of \\n3D digital content. Take Nvidia’s Neuralangelo, \\na generative neural network. It can rapidly \\nturn 2D video clips into 3D digital objects or \\nstructures.34 Those assets can be imported  \\ninto VR or AR spaces, digital twins, or video \\ngames. And Intel’s LDM3D AI model can churn \\nout 360-degree 3D images from simple  \\ntext prompts.35 \\nOther examples are cutting out the need \\nto create a 3D model in the first place. \\nQualcomm’s Snapdragon Spaces is leaning \\ninto realism by letting people blend digital \\ncontent with their physical environment. The \\nAR SDK is allowing developers to rapidly \\ncreate new applications that blend the \\ndigital and physical by using semantic scene \\nunderstanding, hand tracking, object detection, \\nand more.36 And Google’s TryOnDiffusion uses \\ngenerative AI to let online shoppers see how \\nclothes would realistically drape or fold over \\ntheir unique bodies.37  \\n \\nNot only is the financial cost starting to break, \\nso is the time investment as well. Some tools \\nare making it simple enough for anyone \\nto build spatial environments. Google’s \\nGeospatial Creator, powered by the Google \\nMaps Platform and ARCore (Google’s AR \\nSoftware Development Kit), lets you create an \\nimmersive experience in just minutes. You can \\nbuild 3D digital content to layer onto real-world \\nlocations and it integrates with design engines \\nlike Unity and Adobe Aero.38  The best part? \\nLittle to no coding skills are needed.\\nBut building a world isn’t the only  \\nhard part – you also need to fill it. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  53\\nThe space we need\\nPopulating spaces \\n“Where is everybody?” famously said physicist \\nEnrico Fermi when contemplating life in the \\nuniverse. Math told him our universe should \\nbe teeming with life. Yet looking at the stars, \\nspace seemed desolate – a feeling that often \\nexists in metaverse spaces we visit today. Well-\\ncrafted but empty spaces can turn users away, \\nwhich is why to be successful, enterprises need \\nto go beyond just making spaces look real, to \\nmaking them feel lived in.\\nOne obvious way to populate spaces is by \\nbringing in other users. The metaverse’s \\npromise has always been a shared vision, \\nwhere people could work or talk together. But \\nwith an estimated 400 million people engaging Enterprises need to go beyond just  \\nmaking spaces look real, to making  \\nthem feel lived in. \\nin metaverse experiences, these spaces \\ncould get crowded quickly. It’s one thing in \\nthe physical world to walk into a local retailer \\nthat has multiple locations. It’s another when \\neveryone in the world can access the same \\nvirtual store at once. \\nThis is why part of Meta’s training and guides \\nfor developers building 3D spaces includes \\nproxemics as a topic.39 Proxemics is the study \\nof how we use space, including how population \\ndensity or physical proximity impacts how we \\nact, communicate, and relate to each other. \\nMeta recognizes that spatial awareness is key \\nto making these experiences work. A user’s experience in a crowded space may \\nwell be an important factor in figuring out \\nwhen an enterprise needs another instance of a \\ndigital store or world to combat overcrowding.\\nAnother strategy is creating AI people. These AI \\ncharacters can populate spatial apps, making \\ninteractive and tailored experiences. It’s not \\na new concept: non-player characters (NPCs) \\nhave been in video games for almost as long \\nas they have existed. But historically, they’ve \\nbrought a different slew of challenges, like \\nfeeling hollow and impersonal. The software \\ncompany Inworld AI found that 52% of gamers \\ndisliked repetitive NPC dialogue.40 But making \\nhigher quality NPCs usually took more time and \\nmoney without quite enough pay-off. Inworld AI points to a new direction. It creates \\nAI characters with personalities, that can \\ncommunicate verbally as well as non-verbally.41  \\nThese characters are context-aware, so they \\ndon’t hallucinate or refer to content outside \\ntheir set world, mitigating misinformation \\nrisks.42 And they offer a sense of interpersonal \\nrealism that will ground the spatial apps  \\nof tomorrow.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  54\\nThe space we need\\nFinally, one emerging capability that \\ndifferentiates spatial computing from its digital \\ncounterparts is engaging our senses. New \\ntechnologies are letting engineers design \\nexperiences that address all types of senses, \\nlike touch, smell, and sound.\\nIn past VR attempts, adding haptics, or \\ntouch, could be bulky or underwhelming. But \\nUniversity of Chicago researchers recently \\nproposed using electrodes to better mimic \\ntouch.43 They built an electrode system with 11 \\ncontrollable tactile zones on a person’s fingers, \\nso they could “feel” digital content. Imagine  \\na meditation spatial app that took you to a  \\nvirtual beach where you could “feel” the  \\ngrains of sand.\\nScents can make digital spaces lifelike, too, \\nby evoking memories or triggering the all-\\nimportant fight-or-flight response. Scentient, \\na company trying to bring olfactory senses to \\nthe metaverse, believes scent can be the key \\nto overcoming the “uncanny valley” – when a \\nscene feels almost believable, but not quite \\nenough, turning users away. Scentient’s \\nleaders think scent brings realism and \\ndimension to a digital space, and they have \\nbeen experimenting with the technology for \\ntraining firefighters and emergency responders, \\nwhere smells, like the presence of natural gas, \\ncan be critical for evaluating an emergency.44 \\nAt first glance, the idea of scent may seem \\ngimmicky, but it can have real impact, and be a \\nkey part of a successful space.  \\n Of course, sound, or spatial audio, is also \\ncritical to realistic digital scene-building.  \\nThe New York Times  recently used web-based \\nspatial audio to immerse readers in the sonic \\nlandscape of the Cathedral of Notre Dame in \\nParis. On mobile or desktop, users could  \\n“walk” in the cathedral to hear how a choir \\nsounded depending on where they “stood”  \\nin the space.45,46\\nLastly, immersive spatial apps will need to \\nrespond to how we naturally move. Apple’s \\nVision Pro tracks eye movements to better \\nplace low-latency content in its display.47 And \\nMeta’s Direct Touch feature explores how \\ntracking hand motions in VR could replace \\nhand controllers for experiences where natural \\nmovement would be better.48\\nToday’s spatial computing technology  \\nlandscape is rapidly growing with exhilarating \\npotential. Design foundations and standards \\nare being set, and more tools appear every \\nday. Enterprises can mix-and-match these \\ntechnologies to find their own way to meet this \\nspatial moment. And once you can build spatial \\napps, the next challenge is what  to build.\\nSense of placeTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  55\\nThe space we need\\nThe implications:  \\nSpatial’s killer applications \\nSpatial computing is not coming to replace  \\ndesktop or mobile computing – but it \\nis becoming an important piece of the \\ncomputing fabric that makes up enterprise  \\nIT strategy.  \\n \\nJust as people aren’t going to write an essay \\non a mobile device, and tablets freed retail \\nassociates from stationary POS systems, spatial \\nwill have its own killer use cases that will leave \\nus wondering how we ever managed without it.  \\nWe’ve already seen the early stages. Digital \\ntwins make more sense when you walk through \\nthem. Training is more impactful when you can \\nlive the experience rather than watch a video. \\nWhile these were often standalone pilots, a \\ncareful consideration of the unique advantages \\nof spatial computing can help shape and \\nguide enterprise strategy. The market is still \\nmaturing, but it is quickly becoming clear that spatial apps thrive when applied in three \\nways: conveying large volumes of complex \\ninformation; giving users agency over their \\nexperience; and, perhaps counterintuitively, \\nallowing us to augment physical spaces.  \\nWhen it comes to conveying complex \\ninformation, the advantage of the spatial \\nmedium over the alternatives is probably \\nclearest. Since a space can let users move and \\nact naturally, information can be conveyed in \\nmore dynamic, immersive ways. We’ve already \\nseen it in action. Some of the earliest examples \\nof successful spatial apps were industrial \\ndigital twins, virtual training scenarios, or  \\nreal-time remote assistance – all use cases \\nwhere lots of information may need to be \\nshared, and where conventional methods \\ncan cause information overload, leading to \\nconfused or ill-trained employees. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  56\\nThe space we need\\nSurgical training, for instance, is an extremely \\ninformation-heavy task. Research from the \\nUniversity of California Los Angeles found that \\nmedical students performed two and a half \\ntimes better on surgical skills assessments \\nwhen they learned new procedures in virtual \\nreality versus the standard guides.49 And \\nimportantly, including more spatial information \\ncan improve results even further. Haptic \\nfeedback – the sensation of touch – helped \\nsurgical trainees achieve proficiency in cortical \\nbone drilling roughly seven times more often \\nthan those operating without it.50\\nGoing forward, this ability to convey \\ninformation in a realistic setting, and through \\ndifferent senses, could reinvent many tasks. \\nLike virtual training, surgeons operating \\nremotely need to simuntaneously digest \\ninformation and perform delicate procedures. \\nTeleoperating through a spatial environment, \\nhowever, could help by displaying information \\nmore clearly, conveying it in a realistic \\nenvironment when and where it’s needed. \\nAlready, a surgeon in China has successfully \\nremoved a patient’s gallbladder from 4,600 KM \\naway, operating a surgical robot in the distant \\noperating room.51 \\n \\nAirports are also complex, information-\\ndense operations. And Vancouver Airport is \\nworking to optimize its efficiency with a spatial \\napplication that unifies real-time and historic \\nflight data, passenger counts, and security wait \\ntimes.52 The interactive app is layered with live \\nvideo and can be viewed in 2D, 3D, or from a \\nbird’s-eye view. Employees can collaboratively \\nsimulate passenger flows, test new facility layouts, and run virtual trainings – reducing \\ncostly errors and improving resource allocation.  \\n \\nThe second advantage spatial has over older \\nmediums is the ability to give users agency \\nto shape their in-app experiences. Because \\nspatial computing lets us build digital \\nexperiences that embody a physical sense of \\nspace, we can design experiences that give \\nusers more flexibility to move and explore. \\nAnd for some applications, putting users in \\nthe driver’s seat will make experiences more \\npersonal, organic, and useful. \\nParis’s Centre Pompidou capitalized on this \\nability to personalize a museum experience \\nthrough a collaboration with Snapchat and \\nartist Christian Marclay.53 Marclay overlayed \\nCentre Pompidou’s façade with a colorful \\ndigital instrument that users could play in \\nmany ways through Snapchat AR. Visitors \\ncould also record and share how they “played” \\nthe museum. Rather than an experience \\nexclusively shaped by the museum’s curators, \\nvisitors were able to infuse their own creativity \\nand discovery into the space. \\nAnd Fiat, the automotive brand, is showing \\nhow user self-driven discovery can help with \\nsales conversion.54 Typically, when people are \\nbuying a car they are able to test drive one \\nmodel, but then have to look at a different \\nmodel or a bunch of advertising pamphlets \\nto see the customization options. They don’t \\nnecessarily get to drive the exact car they \\nwant to buy. Fiat built the Fiat Metaverse \\nStore to challenge this paradigm. Within the \\nvirtual store, users are able to customize their Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  57\\nThe space we need\\ncar model with all the various body types, \\ncolors, interiors, and infotainment options.55 \\nUsers can then take the car out on a virtual \\ntest drive to experience what it would be like \\nand see the features up close. Throughout \\nthe experience they are accompanied by \\n“Product Genius” – a connection to a live \\nexpert who can answer any questions.\\nAnd finally, greater user agency can even \\nbe brought to shaping the experience itself. \\nResearchers from UC Berkeley recognized \\nthat as people adopt spatial computing \\nthere will be a range of asynchronous \\nsituations as people attempt to collaborate \\nand interact across devices. For instance, \\nan instructor may be on a laptop, while a \\ntrainee will be wearing a VR headset. They \\nrecognized that trying to force content \\ninto one mode or the other will result in a \\nworse experience for all involved, so they \\nproposed a model for “Interactive Mixed-\\nDimensional Media” – a way to seamlessly \\nshift information streams between 2D \\nand 3D. It allows users a greater degree \\nof control over their own experience, \\nallowing them to decide how information \\nis presented to them across spatial and \\ndimensional levels of detail.56 \\n Lastly, spatial applications bring advantages to \\nphysical spaces; they can augment, enhance, \\nand extend physical places without materially \\nchanging them. Imagine a future office where \\nphysical monitors, projectors, and displays \\nare replaced by spatial computers and apps. \\nPeople will have the flexibility to design simpler \\nspaces, lowering overhead costs, and to \\nchange their surroundings more easily.  \\n \\nGap and Mattel leveraged this when launching \\ntheir Barbie clothing line.57 Using Google’s \\nGeospatial Creator, they transformed New \\nYork City’s Times Square with lifelike 3D Barbie \\ndolls, massive digital billboards, and floating \\nneon-pink signs. This launch event showed \\nthe spatial medium as a captivating, scalable \\nalternative to building expensive  \\nnew infrastructure.  \\n \\nSpatial applications bring advantages to physical spaces;  \\nthey can augment, enhance, and extend physical places  \\nwithout materially changing them. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  58\\nThe space we need\\nAs the working world goes spatial, \\nbusinesses will also need to think about \\nsecurity. There will be more devices than \\never – employees will use spatial devices \\nfor work, customers will use them to access \\nexperiences. And with this ever expanding \\ndevice ecosystem there will be more entry \\npoints for attackers too. So how do you \\nput borders on the borderless? Businesses’ \\nspatial strategies will need to be designed \\nwith zero trust principles.\\nAdditionally, businesses should recognize \\nthat spatial is unfamiliar territory, so both \\nvendors and users should expect to have \\nblind spots. One line of defense won’t be \\nenough, but Defense in Depth strategies \\nthat leverage multiple layers of security (like \\nadministrative, technical, and physical) can \\nbe deployed to defend this new frontier. \\nWhat’s more, taking such a multidimensional \\napproach to security can help deliver the \\nconfidence needed for widespread adoption.  Security \\nimplicationsAccenture, Google, and Telstra, Australia’s \\nleading telecommunications firm, also \\nharnessed spatial computing in a pilot app \\nto improve the stadium experience – without \\nsignificant physical changes.58 In the app, \\nfans could find their seats with immersive \\nwayfinding and enjoy augmented  \\nreality experiences.  \\n \\nThe technology also has the potential to \\nenable multi-user AR games and enhanced \\nplayer analytics and performance statistics.  \\nBy tapping into spatial technology, future \\nevent spaces could be designed in a simpler \\nmanner, requiring less hardware and  \\nfewer displays.  \\n \\nThis evolution to our spaces will let enterprises \\nrethink operations and workflows entirely, \\npromising benefits like efficiency, safety,  \\nand more.  In one cross-industry collaboration at \\nCHRISTUS MUGUERZA Hospital Conchita in \\nMexico, a renowned orthopedic surgeon was \\nequipped with a Microsoft HoloLens during \\nsurgery.59 The technology transformed the \\nsurgical environment, allowing the doctor to \\naccess patient records, X-rays, scans, and \\n3D models during a procedure. Importantly, \\ndigitally accessing this information mid-surgery \\nlet the operating team preserve a sterile field \\nfor the patient. When the team needed to refer \\nto patient information, they could do so and \\noverlay it while still in the operating room – \\nwithout stepping out, removing scrubs, finding \\nthe record in question, recalling what they \\nneeded to know, changing back into scrubs, \\nand re-sterilizing upon return – so there was no \\nneed to delay.\\nAs companies begin experimenting with \\nspatial, it is paramount that they appreciate \\nwhat sets it apart from desktop and mobile.\\nUnderstanding these differences is the key to \\ndesigning applications that capture and unlock \\nthis medium’s promise and value.  Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  59\\nThe space we need\\nSpatial computing is about to hit its \\nstride, and the race is on for leaders to \\nget ahead. To position themselves at \\nthe top of the next era of technology \\ninnovation, enterprise leaders will need \\nto rethink their position on spatial and \\nrecognize the effect recent technology \\nadvances are about to have.  \\n \\nNew computing mediums are few \\nand far between, and they can have \\nimmeasurable impact on businesses and \\npeople for decades. Are you ready to \\nimmerse yourself in the moment?  Conclusion:  \\nThe space  \\nwe needOur bodies \\nelectronic\\nA new human interfaceTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  61\\nOur bodies electronic\\n1924\\n2012\\n20262009\\n20231947\\n2013\\n20271952\\n2015\\n20291964\\n2016\\n20321998\\n20211994\\n20212007 1990\\n2017\\n2035The first \\nelectroencephalo-\\ngram (EEG)  \\nrecording is made.1\\nSamsung \\nannounces voice \\nand gesture \\ncontrol for TVs.10 \\nA racing simulator \\nwill feature cars that \\nare controlled by \\nbrain activity and \\neye movement.Fitbit releases  \\nthe Fitbit Tracker.9 \\nUsing AI and a \\nBCI, researchers \\npartially return a \\nstroke survivor’s \\nspeech.17Pilots’ eye \\nmovements are \\nmonitored to \\nimprove instrument \\ndesign.2\\nThe White House \\nlaunches the \\nBRAIN initiative.11\\nA major retailer \\nwill launch a BCI \\npilot program for \\nemployee training \\nand skill retention. Bell Labs develops \\nthe initial speech \\nrecognition \\nsystem.3\\nOura introduces  \\nits first smart ring.12\\nA major bloc \\nof nations will \\npass legislation \\nprotecting citizens’ \\nneurorights.Early research on \\nfacial recognition \\nbegins.4\\nNeuralink  \\nis founded. 13\\nA large insurance \\nprovider will offer in-\\nhome gait analysis to \\ndetect early signs of \\nParkinson’s and ALS.The first \\ninvasive BCI is \\nimplanted in a \\nhuman.7\\nChile passes \\nthe world’s first \\nneurorights \\nlegislation.16Netscape invents \\nweb browser \\ncookies.6\\nA study finds \\nthat Kaiser \\nPermanente’s \\nCOVID-19 Home \\nMonitoring is safe \\nand effective.15Volvo introduces \\na distracted \\ndriver alert \\nsystem.8fMRI technology  \\nis developed.5\\nApple launches \\nFace ID.14\\nA consumer \\nneurotech device will \\ntranscribe dreams \\ninto visualizations \\nand text.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  62\\nOur bodies electronic\\nThe big picture\\nAsk your smart home to turn on the lights, and there’s a \\ndecent chance it’ll play “Lights,” Journey’s 1978 single. \\nOr it might turn on the lights in the wrong room. Smart \\nhomes can’t read our minds, after all, so the onus to be \\nunderstood is on us.   \\nDid we enunciate? Did we specify which lights?Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  63\\nOur bodies electronic\\nMisunderstanding people is a limiting factor \\nfor a lot of technologies we use today. Just \\nthink about robots and drones that we can \\nonly control if we translate what we want into \\ncommands they recognize. Or how digital \\nproducts can’t be as successful if we don’t \\nunderstand how they make customers feel. \\nEven in VR, people get dizzy when there’s a \\ndisconnect between what they expect to see \\nand what the device shows them. We try to \\nclose the gap – learning new gestures, running \\nfocus group and A/B tests, taking motion \\nsickness pills, and training employees on new \\ntech. Large companies in 2022 spent about \\n$1,689 per employee  for overall training.18 But \\nthe fact is, when tech struggles to connect with \\nus, it’s often because people – what they want, \\nexpect, or intend – are a black box. \\n \\nNow innovators are trying to change that. \\nAcross industries, they’re building technologies \\nand systems that can understand people \\nin new and deeper ways. They’re creating a \\n“human interface” – and the ripple effect will go \\nfar beyond improving smart homes.  \\n Look at how neurotech is beginning to \\nconnect with people’s minds. Recently, two \\nseparate studies from researchers at the \\nUniversity of California San Francisco and \\nStanford University demonstrated using neural \\nprostheses – like brain-computer interfaces \\n(BCI) – to decode speech from neural data.19,20 \\nThis could help patients with verbal disabilities \\n“talk” by translating attempted speech into \\ntext or generated voices. Interest in neurotech \\nhas reached enterprises, too. Researchers in \\nMeta’s AI lab experimented with decoding \\nspeech from brain activity in 2022, using non-\\ninvasive brain recordings and an AI model to \\ndecode sentences and stories that patients \\nheard out loud.21  \\n \\nOr look at technologies that read body \\nmovement, like eye and hand tracking. In 2023, \\nApple’s Vision Pro introduced visionOS, which \\nlets users navigate and click with just their gaze \\nand a simple gesture, bypassing the need for \\na handheld controller.22 The highly precise eye \\ntracking acts as a targeting system; users can \\npinch their index finger and thumb together \\nto click on what they’re looking at. Even more \\nimpressive eye tracking could also be coming. \\nApple has filed a patent that describes using pupil dilation to predict if a user intends to do \\nsomething – like select a button – even before \\nthey do it.23  \\n \\nInnovations like these are shifting the rules and \\nthe limits that have guided human-machine \\ninteraction for decades. So often today, we \\nbend over backwards, adapting and changing \\nwhat we do to make technologies work. But \\nthe “human interface” will turn that on its head; \\nwhen technologies can better understand \\nus – our behavior and our intentions – they will \\nmore effectively adapt to us.  \\n \\nJust think about how many business challenges \\nhinge on exactly that. Customer service, \\nproducts, and workplace experience, for a \\nstart. Across the business, we use a wide array \\nof technologies to help us understand and \\nadapt to people. And we’re so used to today’s \\nlimitations, to the way these technologies \\nnever fully “get” us, we might not even realize \\nhow much that gap constrains what we do. \\nNow, advancing the “human interface” – from \\neye-tracking to posture recognition, computer \\nvision to machine learning, brain sensing to \\nreading muscle signals – is lifting that limit. \\n So often we bend over  \\nbackwards, adapting  \\nand changing to make  \\ntechnologies work -  \\nthe human interface  \\nwill turn that  \\non its head. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  64\\nOur bodies electronic\\nThis collision between technology and \\nour bodies may evoke futuristic, even far-\\nfetched images. But a careful evaluation \\nof the technologies already at hand shows \\nus boundless opportunities for enterprise \\ninnovation. We could improve employee \\ntraining, leading to safer workplaces with less \\nmiscommunication and fewer accidents. We \\ncould build digital products that, in better \\nunderstanding and engaging people, reach a \\nwider customer base. Imagine the productivity \\ngains when we don’t need to contort ourselves \\naround technology, but when it can work \\naround us instead. It’s already starting with \\nheadsets that observe us to deliver smoother \\nexperiences. Soon we could see robots \\npaying attention to us to contextualize our \\ninstructions, or digital experiences that track \\nour engagement, picking up on the subtlest \\ncues and reactions. These technologies –  \\nthe ones that “get” us better – will transform \\nwhat we do.\\nThe “human interface” will be a major leap \\nforward in technology capability, with \\nwidespread impact on businesses.But to succeed, enterprises will also need \\nto address growing issues around trust and \\ntechnology misuse. Companies and individuals \\nalike may balk at the idea of letting technology \\nread and understand us in these new and more \\nintimate ways. Biometric privacy standards \\nwill need to be updated. And new neuroethics \\nsafeguards will need to be defined – including \\nhow to appropriately handle brain and other \\nbiometric data that can be used to infer \\npeople’s intentions and cognitive states. Until \\nformal regulations catch up, it’s on enterprises’ \\nshoulders to earn people’s trust.  \\n \\nThe ”human interface” is a tricky field to dive \\ninto today. There’s hesitation around the \\ntech, and the full implications are still unclear. \\nEven so, the tech is further along than many \\nrealize, and there’s no doubt businesses need \\nto get started. When technology can better \\nunderstand people, every human-centric \\narea of business – everywhere people and \\ntechnology interact and everything dependent \\non understanding people’s behaviors and \\nintentions – will be disrupted. From the smallest \\ntasks to the largest challenges, the “human \\ninterface” will raise the bar. You don’t want to \\nbe left behind.\\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  65\\nOur bodies electronic\\nThe technology:  \\nA more human-centric  \\nview of people  \\nAttempting to understand people – as \\nindividuals, target groups, or populations –  \\nis a centuries’ old business challenge. And in \\nrecent decades, using digital technology to  \\ndo this has been the ultimate differentiator.  \\n \\nDigital platforms and devices have let \\nbusinesses track and quantify people’s \\nbehaviors with enormously valuable impact. \\nNow, the “human interface” is changing the \\ngame again, making it possible to understand \\npeople in deeper, more human-centric ways. \\n \\nHow digital technology  \\n“understands” people \\nConsider the leading strategies companies \\nuse to understand human behavior today. They generally fall into two categories: data \\ncollection on the web and physical sensors  \\nout in the world. \\n \\nOn the web, businesses’ ability to understand \\nbehavior is a major factor shaping digital \\nexperiences. Platforms rely on user data \\nto personalize experiences and improve \\nthe product. According to Statista, the \\nglobal revenue of customer experience \\npersonalization and optimization software is \\nprojected to reach $11.6 billion by 2026.24 Not \\nonly that but many companies today spend \\nmore than half of their marketing budgets on \\npersonalization.25 Newsfeed rankings, content \\nrecommendations, ad targeting, and more all \\nstem from this.  Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  66\\nOur bodies electronic\\nOut in the physical world, a lot of recent \\ntechnology innovation has also centered \\naround understanding what people do.  \\nDevices have long collected data on people \\nphysically – letting businesses build products \\nand services around health and location. In-\\nstore sensors to analyze foot traffic, or facial \\nrecognition with sentiment analysis, are tools \\nto better understand people to steer them \\ntoward various outcomes. And voice and \\ngesture recognition are ways for people to \\ninteract with technology more naturally.  \\n \\nThese are important technologies. But they’re \\nbased on tracking and observing patterns that \\nstill lack specificity. People may read or watch \\nfamiliar content, but they may actually want \\nsomething new. They may try to command \\nrobots or devices with unclear gestures \\nor the wrong phrases. We’re very good at \\nrecognizing what people do, but we don’t \\nalways understand intent. Many frustrations \\nstem from this: technology misinterpreting \\nNearly one in three consumers agree they are often \\nfrustrated that technology fails to understand them \\nand their intentions accurately.  our instructions, or platforms trapping us in \\nrecommendation doom loops. It’s no surprise – \\npeople also can struggle to express themselves \\nin a world with both technology limits and \\nhuman constraints.  \\n \\nThat’s what the “human interface” promises  \\nto change. As algorithms get ever more precise, \\nmore comprehensive sensor arrays are built, \\nand new technologies emerge that can make \\nthe leap to intent, we are starting to understand \\npeople in a more human-centric way.\\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  67\\nOur bodies electronic\\nHow the “human interface” \\nmeasures intent \\nThe “human interface” isn’t any one technology. \\nRather, it encompasses a suite of technologies \\nthat are deepening how innovators see and \\nmake sense of people. Identifying intent – \\npeople’s state of mind or what they really want \\nor mean – is one of the most important factors \\nhere. And across industries, leaders are finding \\nways to predict or observe it. \\n \\nSome are using wearable devices to track \\nbiosignals that can help predict what people \\nwant or understand their cognitive state. \\nFor instance, researchers at the University \\nof Washington Seattle explored ways gaze-centered eye tracking in VR can grant users \\nmore control over a prosthesis.26 Here, \\nfollowing a person’s eye movements could help \\npredict intent, like if they wanted to move in a \\ncertain direction or pick up a particular object. \\nAs another example, Immersion Neuroscience \\nis a company using smartwatches and fitness \\nsensors to measure subtle changes in people’s \\nheart rate to predict their cognitive state.27 \\n \\nOthers are building more detailed ways to \\nunderstand people’s intent in relation to \\ntheir environments. Researchers at Tongji \\nUniversity’s School of Automotive Studies, for \\ninstance, wanted to find out how to reduce \\nhuman-vehicle collisions.28 Where most crash \\nprevention efforts focus simply on detecting pedestrians, these researchers conducted a \\nstudy that went further. They captured details \\nincluding the distance between the vehicle \\nand the pedestrian, the speed of the vehicle, \\nand the pedestrian’s physical posture – like \\nthe angle between their thighs and calves or \\nbetween their calves and the ground. Grasping \\na person’s posture while walking on a street \\ncan be a clue to discern where they are likely \\nto move next, potentially making the roads \\nsafer for everyone.  \\n \\nWaymo, the self-driving car subsidiary of \\nAlphabet, is doing something similar. In \\naddition to watching everything happening \\naround the car – like traffic signs and signals, \\npedestrians, construction, cyclists, and other cars – the Waymo Driver autonomous system \\nalso makes predictions about other road \\nusers’ intent.29 Understanding that pedestrians, \\ncyclists, and others all move differently, it \\npredicts the many possible paths each might \\ntake, all in real-time. \\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  68\\nOur bodies electronic\\nAnother approach to human intent is through \\nAI. Consider human-robot collaborations. \\nPeople’s state of mind, like if they’re feeling \\nambitious or tired, can impact how they \\napproach a task. But while humans tend to be \\ngood at understanding these states of mind, \\nrobots aren’t. So, researchers at the University \\nof Southern California tried to teach robots to \\nidentify these states to help them better assist \\npeople.30 Typically, training a robot to fit an \\nindividual’s work style takes a lot of time. But \\nthe researchers proposed a transfer learning \\nsystem, where the robot observes someone \\nperforming a small, canonical task, then \\ncreates a preference model that it continuously \\nupdates as the robot and person interact.31   \\n \\nSimilarly, a 2023 research paper from \\nAccenture and Cornell University describes a \\nway human-robot collaborations can benefit \\nwhen robots can identify when they’ve made \\nan error based on implicit reactions from \\nhumans they interact with – much like how \\npeople use social cues to recognize their \\nown mistakes.32 The authors built a dataset \\nof bystander responses to human and robot \\nerrors and used it as input to a deep-learning \\nmodel to predict failures. By creating these \\nsystems that are sensitive to human social \\nsignals, they are efficiently using the expertise \\nof human perception and action as a marker for \\nmitigating robotic errors.  \\n \\nLastly, perhaps one of the most exciting \\n\"human interface\" technologies is neurotech: \\nneuro-sensing and BCI.  \\n \\n \\n Many new neurotech companies have \\nappeared in the last decade, and the field  \\nholds clear potential to read and identify \\nhuman intent. \\nLook at how prosthetic limbs could act  \\nand feel more natural, as companies start \\nto use neural prostheses to strengthen the \\nconnection between a person’s intent and  \\ntheir robotic limbs.\\nBlackrock Neurotech and Phantom Neuro, for \\ninstance, have partnered to build advanced \\nprosthetic limbs and exoskeletons.33 By \\nimplanting a prosthetic at the amputated site, \\nthey can detect neural signals through muscle \\nactivity, and use that to control the robotic arm. \\nThe companies hope to build devices that can \\nmove in response to a person’s intentions much \\nlike an intact limb would.  \\n \\nThese are just a few of the “human interface” \\ntechnologies starting to emerge. In coming \\nyears, we expect to see a wide range of devices \\nand systems that can better understand human \\nintent, ranging all the way from fully external, \\nto skin-touching, to invasive implants in our \\nbodies. While we focus primarily on external \\nand skin-touching technologies here, it’s worth \\nnoting that invasive devices are also advancing. \\nAcross the board, what’s clear is that we’re \\nalready starting to understand human intent  \\nto a previously impossible degree, and we’re \\nonly getting better.\\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  69\\nOur bodies electronic\\nProgress towards the “human interface” \\nand a new generation of business built on \\nhuman-centric tech is happening fast – and \\nneurotech is a great example of this. Many may \\nthink neural-sensing and BCI are years from \\ncommercial use, but recent advances tell a \\ndifferent story. In fact, there has been an uptick \\nin consumer-oriented development in recent \\nyears, a strong signal that the “human interface” \\nis heading for the mainstream. \\nFirst off, skeptics tend to believe that \\nneurotech will stay limited to the healthcare \\nindustry. Many high-profile neurotech devices, \\nafter all, are centered around highly invasive \\nimplants as a course of medical treatment. In \\n2022, Galvani Bioelectronics, a joint venture \\nfrom GlaxoSmithKline and Google’s Verily, \\nimplanted its first neurostimulator in a patient \\nwith rheumatoid arthritis.34 And in early trials, \\nSynchron’s partially-invasive brain implant \\nhopes to restore communication and other \\nfunctions to severely paralyzed individuals.35 The invasiveness is justified by the medical \\nneed, making it unlikely anyone today would \\nopt in to using this technology without a \\nnecessitating condition. But there are a lot of \\napproaches to building these devices. They can \\nbe invasive, partially invasive, or fully external. \\nThey can be massive systems in medical \\noffices, or small and portable. With the range of \\ntechnology options available, the right solution \\ncan be found to match the need and comfort \\nlevel of the intended user – and commercial \\ninitiatives and use cases grow by the day.  \\n \\nTwo key advances are driving this. The first is \\ndecoding brain signals. For decades, it’s been \\npossible to sense brain signals, yet the leap \\nto commercial products is a giant one.36 It’s \\nvery difficult to identify common signals and \\npatterns across different people’s brains. But \\nadvances in AI pattern detection, as well as \\ngreater availability of brain data, is making a  \\nbig difference.  \\n \\nTake this example. In a 2023 paper in the \\nNeural Networks  journal, researchers used a \\ndeep learning model based on a transformer \\narchitecture to make it easier to recognize \\nbrain signals across multiple subjects, without There has been an uptick in consumer-oriented development in  \\nrecent years, a strong signal that the “human interface” is heading  \\nfor the mainstream. needing to recalibrate for each person’s \\nbrain.37 They described how conventional \\nsystems for classifying SSVEP (steady-state \\nvisual evoked potential) – a type of brain signal – \\nneed to be specifically trained for every \\nindividual. And though previous studies have \\nused deep learning to improve inter-subject \\nSSVEP recognition, this paper demonstrated \\nthat a transformer architecture could achieve \\neven better results in classification accuracy, \\npotentially alleviating the need for individual \\nBCI calibration.  \\n \\nFueling this research is an industry-wide shift \\ntoward open-source data. Transformer models \\n(and machine learning models in general) \\nrequire huge amounts of training data, and \\nbrain signal datasets are still relatively rare. \\nBut more institutions are starting to publish \\ntheir brain data, so a wider community of \\nresearchers can build on each other’s work.38  \\nAnd at Accenture Labs, researchers tested \\nhow AI can efficiently generate synthetic brain \\nsignals to train detection models, without \\nneeding to rely on people’s original brain \\nsignals.39 This capability demonstrates the \\npotential to develop novel AI health solutions \\nthat make use of synthetic proxies, rather than \\ndepending on sensitive patient bio-signals.  \\n \\nThe second area to watch is neuro-hardware – \\nspecifically, the quality of external devices. \\nThough invasive BCIs typically have higher \\nresolution, external devices are expected to \\nhave wider market appeal.  Neurotech highlights  \\nthe pace of “human interface” \\nadvances Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  70\\nOur bodies electronic\\nHistorically, EEG (electroencephalogram) and \\nfMRI (functional magnetic resonance imaging) \\nhave been two of the most widely used external \\nbrain sensing techniques.40 EEG measures the \\nbrain’s electric activity and has better temporal \\nresolution (capturing neural events as soon as \\nthey happen), and fMRI measures the brain’s \\nblood flow and has higher spatial resolution \\n(pinpointing where the neural activity took \\nplace).41 However, until recently, capturing \\neither type of brain signal required a lab setting. \\nNow, that\\'s starting to change.\\nEEG devices were once very sensitive to \\nenvironmental noise and muscle movement, \\nrequiring users to be very still. But new devices, \\nlike Wearable Sensing’s DSI-24 headset that \\nuses a dry electrode EEG system, are more \\nresilient to motion and noise.42 And while fMRI \\nis likely to remain in medical settings, a newer \\ntechnology called fNIRS (functional near-\\ninfrared spectroscopy) is making it possible to \\nmeasure blood flow in the brain without people \\nneeding to be in a tube in a lab.43 Kernel’s \\nFlow2 neuroimaging headset, for instance, \\ncombines fNIRS technology with EEG to gain \\na comprehensive view of brain activity in a \\nportable headset.44\\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  71\\nOur bodies electronic\\nPortability is critical to commercialization, and \\nsome companies are already building products \\nthat depend on it. EMOTIV, a bioinformatics \\nstartup, and X-trodes, which builds wireless \\nwearable tech, are collaborating on a wearable \\nat-home solution for brain and physiological \\nmeasurement.45 They’re building sticker-\\nelectrodes that conform to people’s skin and \\nenable use cases where larger EEG devices are \\nnot ideal – like sleep studies. And Apple has \\nsubmitted a patent application for measuring \\nbiosignals and electrical activity from a user’s \\nbrain through an AirPods Sensor System.46 \\n \\nLastly, also related to portability, being able to \\nquickly translate brain signals into action is very \\nimportant. For certain use cases, sending raw \\nbrain data to the cloud could be unacceptably \\nslow, creating usability frustrations and \\npotentially preventing people from moving \\nor communicating in a timely manner. But \\nadvances are happening here, too. In 2022, \\nVC funding drove a boom in edge AI chip \\nstartups.47 Established chip makers like Nvidia \\nand Qualcomm continue to work on building \\nsmaller, more powerful edge chips.48 And some \\nTechnology is quickly changing what it means  \\nto understand people, and for enterprises that is  \\ngoing to change a lot. are working to increase the computational \\nefficiency of processing brain signals. In a \\n2022 paper, researchers from the University \\nof California Merced and Ericsson proposed \\nan efficient method for classifying EEG brain \\nresponses to song recordings, demonstrating \\nthat EEG data can be processed effectively \\nthrough standard computer vision methods.49 \\nIn fact, their classification model was small \\nenough to fit on a floppy disc.  \\n \\nConnecting to people’s minds like this would \\nhave seemed impossible just a few years ago – \\nand that’s why neurotech is such a powerful \\nindicator of the “human interface.” So many \\ndifferent technologies are starting to observe \\nand understand people in deeper, more \\nhuman-centric ways. Neurotech is one, while \\nothers are observing and learning people’s \\nmoods and habits over time or grasping a \\nperson’s environmental context to predict \\naction and intention better. Across them all, \\ntechnology is quickly changing what it means \\nto understand people. And for enterprises, that \\nis going to change a lot.\\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  72\\nOur bodies electronic\\nThe implications:  \\nGetting started –  \\nthe right way\\nIt’s time for enterprises to ask what the “human \\ninterface” means for them, and right now, it’s \\nan open-ended question.  \\n \\nThese innovations and advances are closing \\nthe gap between people and technology, \\nmaking it possible for technology to \\nunderstand and adapt to people on a \\nwhole new level. They could drive huge \\ntransformations across the business,  \\nanywhere people and technology interact.  \\n \\nThe breadth of opportunity is exciting. \\nAlready, leaders are diving in – and in very \\ndifferent ways. Some are exploring how these \\ntechnologies can amplify the workforce. \\nAccenture, for instance, is working with \\nneurotech startup Mendi, to study how \\napplications of neurofeedback can improve \\nlearning and training programs.50 Still others are thinking about consumer products. In the \\nvideo game industry, leaders are exploring \\neverything from how eye tracking can control \\nor influence VR experiences to how neurotech \\ncan help them better understand how players \\nreact to games.51,52 \\n \\nBut widely distributed opportunities can also \\nmake it harder to see the big picture. As more \\nenterprises start to build “human interface” \\nstrategies, they should begin by scoping out \\nthe different business areas and challenges  \\nthat can be transformed.  \\n Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  73\\nOur bodies electronic\\n94% of executives believe human interface  \\ntechnologies will let us better understand  \\nbehaviors and intentions, transforming  \\nhuman-machine interaction. The scope of the  \\n“human interface” \\nFirst, consider how “human interface” \\ntechnologies are raising the bar when it \\ncomes to anticipating people’s actions. This \\nis something enterprises have tried to do for \\nmany years – but the “human interface” lets us \\ncombine the sophisticated behavior analysis \\nwe’ve been building with a novel ability to \\naccount for human complexity.  \\n \\nSome of the most promising use cases are in \\nareas where people and machines operate \\nin shared spaces. For instance, enterprises \\ncould create safer and more productive \\nmanufacturing systems if robots could \\nanticipate what people were about to do. Or \\ncars could monitor not just pedestrians but \\nalso drivers, nudging them to improve road \\nsafety, and potentially impacting the auto \\nindustry, supply chains, and more.  \\n Other industries see the promise, too. Cornell \\nUniversity researchers are experimenting \\nwith algorithms that can predict athletes’ \\nmoves during a sports game with about 80% \\naccuracy.53 Their algorithms use computer \\nvision to interpret visual information about the \\nplayer in real time, like their position on the \\ncourt and their body posture, then combine \\nthat information with contextual data like the \\nteam’s strategy or the player’s role. \\n \\nThe Cornell team envisions these algorithms \\nbeing used to help teams better prepare for \\ncompetitions. And for enterprises, tools like \\nthis could change a lot more than sports. So \\nmuch of what we do is influenced by our \\nsurroundings, so technologies that can capture \\nwhat’s going on and how it influences us hold \\na lot of promise. Training employees better, \\nconsumers getting more personalized and \\nengaging digital experiences – these examples \\nare only the beginning. Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  74\\nOur bodies electronic\\nAnother area that can be transformed is direct \\nhuman-machine collaboration – how we use \\nand control technology. As an example, think \\nabout how neurotech is letting us tap into our \\nminds and connect with technology in new, \\npotentially more natural ways.  \\n \\nResearchers at the University of Technology \\nSydney, for instance, have developed a \\nBCI headset that uses a biosensor to pick \\nup brainwaves and translate them into \\ncommands.54 In a test with the Australian \\nArmy, soldiers were able to use the device to \\ncontrol a four-legged robot dog with just their \\nminds – with up to 94% accuracy. And Snap \\nhas acquired NextMind, the maker of a mind-\\ncontrolled headband that lets people interact \\nwith and command digital objects with their \\nbrain signals.55 \\n \\nGoing forward, this could go beyond hands-\\nfree control to bring new capabilities or \\nimproved performance to certain technologies. \\nWiMi Hologram Cloud, a Chinese AR and \\nholographic services provider, is working to \\nbuild a hybrid BCI system – one that would \\ncombine and analyze multiple types of \\ncomplementary brain signals – for controlling \\nhumanoid robots.56 The company believes \\nthat using a person’s neural signals to deliver \\ncommands could let us control humanoid \\nrobots with greater precision.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  75\\nOur bodies electronic\\nAnd InnerEye, a neurotech startup, is \\ntransforming human-machine collaboration in \\nyet another way. People’s brains can process \\nvisual images very quickly but are slowed \\ndown by the cognitive and motor processes \\nneeded to decide and execute a response. \\nInnerEye demonstrated that a BCI headset can \\ntake advantage of our brain’s rapid response \\nto patterns to enhance productivity.57 The \\ncompany had a test subject watch airport \\nsecurity X-ray scans rapidly appear and \\ndisappear on a computer screen – at about \\nthree images per second. This pace is normally \\nway too fast for a person to properly look for \\nthings like hidden firearms. But after the stream \\nof scans ended, almost all the images flagged \\nfor firearms were indeed correct. \\n \\nLastly, the “human interface” could drive the \\ninvention of new products and services.  Brain-sensing, for instance, could help people \\n“get” themselves better. L\\'Oréal is working \\nwith EMOTIV to help people better understand \\ntheir fragrance preferences.58 The companies \\nare using EMOTIV’s EEG-based headset to \\nmeasure customers’ neuro responses to \\nvarious L\\'Oréal scent families. The headset’s \\nML algorithm then interprets the EEG data \\nto show customers how the different scents \\nmake them feel. \\n \\nStill others are thinking about the “human \\ninterface” as a safety measure. Meili \\nTechnologies is a startup working to improve \\nvehicle safety. It uses deep learning, visual \\ninputs, and in-cabin sensors to detect if a \\ndriver has been incapacitated by a heart attack, \\nseizure, stroke, or other emergency.59,60  And \\nSmartCap is a fatigue awareness product \\nbuilt into a hard hat.61 It measures alertness and fatigue by analyzing EEG brain signals \\nand provides early warning alerts. SmartCap \\nTechnologies first developed the product for \\nthe Australian mining industry to prevent safety \\nhazards. The company was later purchased by \\nWenco International Mining Systems, a division \\nof Hitachi Construction Machinery, which is \\nexpanding it into North America.  \\n \\nAll these business areas and challenges \\ndemonstrate different uses for the “human \\ninterface.” But underlying each of them is  \\nthat age-old business challenge: whether  \\nwe want to make sales, build customer  \\nloyalty, or give our employees the best \\nworkspaces and tools, success starts with  \\nhow well we understand people. \\nNow, the “human interface” is letting us meet \\nthat challenge like never before. \\nTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  76\\nOur bodies electronic\\nIn addition to promoting formal laws, \\nthey believe it is essential for companies, \\nentrepreneurs, scientists, and more to \\ncollaborate and set a standard for self-\\ngovernance and accountability, and to \\n“proactively manage the societal impact of their \\ninnovations.”65 Whether or not you choose to \\nwork with a foundation like this, the point about \\nself-governance and accountability stands. \\nAny business exploring this area today is a first \\nmover, and there’s opportunity to set the tone \\nfor the future.  \\n \\nAs enterprises get started with the wider suite \\nof “human interface” technologies, they can \\nlook to existing biometric laws and to the \\nmedical industry for guidance. Many of the \\nrisks around this space are the same. Any \\ncollection of biometric data, for instance, \\ncomes with privacy concerns and questions \\nabout the right to refuse the technology. \\nAnd with any biometric data, employees and \\ncustomers will want transparency around what \\nis being collected, what it’s used for, and if it’s \\nsaved anywhere. This extends to inferences \\nmade from that data, too. Care and due \\ndiligence here may make or break a business’s \\nsuccessful adoption of this trend.As enterprises explore the possibilities and \\nbuild their strategies, however, they will also \\nneed to address a growing challenge. The \\n“human interface” will never live up to its \\npotential – in products or in the enterprise –  \\nif people don’t trust it.  \\n \\nBusinesses need to start asking what risks \\ncome with these technologies, and what new \\npolicies and safeguards need to be put in \\nplace. Rather than wait for regulations to ramp \\nup, responsible enterprises need to begin \\nnow. In particular, the field of neuroethics is \\nalready getting attention. Questions around the \\nphysical safety of neurotech devices are being \\nraised, as well as the psychological risks around \\nhuman autonomy and societal factors like brain \\ndata privacy, informed consent, and fairness.62  \\n \\nIn 2021, Chile was the first country to pass a \\nconstitutional amendment to extend human \\nrights to include neurorights.63 And groups \\nlike the Neurorights Foundation are working \\nto further the development of neuroethics \\nguidelines and laws around the world. The \\nFoundation’s goal is to prevent the abuse or \\nmisuse of neurotechnology, which it defines as \\nany technology that records or interferes with \\nbrain activity.64 \\n Business competition is \\nchanging – and trust is more \\nimportant than ever \\nIf tin foil hats don’t prevent mind reading, \\nwhat will? More than any other trend this year, \\nsecurity will make or break enterprise and \\nconsumer adoption of the “human interface.\"  \\n \\nAcceptance of more perceptive and  \\nconnected tools hinges on humans’ ability to \\nbe the primary gatekeepers of what information \\ngets shared, at a minimum. This practice \\nneeds be integrated into the design of the next \\ngeneration of human-computer interface tools, \\nletting people either opt into sharing data or \\ntelemetry relevant to the task at hand, or opt out \\nof sharing extraneous or sensitive information.  \\nThink of it like mobile device management for \\nhumans: we already know how to control what \\nmobile device data stays local or gets sent \\nto the cloud, but the stakes are higher and \\nmore complex for sharing humans’ biological, \\nbehavioral, and sensory data. While rule-based \\napproaches can lay the foundation for data \\nsharing systems, humans will need more flexible \\nand interpretable safeguards to maintain control \\nof their own data. The data will also need to \\nbe interpreted for them, so that there is no \\nambiguity about what access they are granting.  Security \\nimplicationsTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  77 Our bodies electronic\\nConclusion: \\nOur bodies \\nelectronicThe human interface is a new \\napproach to one of the oldest business \\nchallenges – it’s giving companies a \\nglimpse into people as humans.  That’s \\na big responsibility and an even bigger \\nopportunity. People will have questions, \\nand concerns about privacy will be \\nthe first and most important hurdle \\nenterprises face. But the chance to \\nunderstand people in this deeper, more \\nhuman-centric way, is worth it.  \\n \\nFrom the biggest tasks to the smallest, \\nfrom reimagining products, to \\nanticipating when we will cross that \\nbusy street, to knowing if we’re too tired \\nto complete a task safely, the “human \\ninterface” could change everything.Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  78\\nThe Technology Vision trends represent some \\nof the most impactful, exciting advancements \\nin technology innovation. However, these \\nare just a few of the trends making up a \\nmuch broader technology revolution that is \\ntouching every dimension of businesses.\\nTwo years ago we asked businesses to \\n“Meet us in the Metaverse,” and last year we \\ndemonstrated how our digital and physical \\nlives are converging with “When Atoms Meet \\nBits” – messages that are still as topical and \\nimpactful as ever. While some trends may garner more excitement or progress more \\nthan others year over year (just look at the \\nbreakneck pace of AI innovation), innovation \\nis still happening across all these areas – and \\nit remains critical for enterprises to consider \\nthe entire scope of change taking place when \\nplanning their long-term strategy.  \\nNew this year we present the ongoing story: \\nmajor themes that have been raised in the \\nTechnology Vision and are underpinning \\nenterprise strategy, the market, and the \\nfuture of technology.Epilogue\\nThe ongoing story:\\nTrend evolutionTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  79\\n2024  \\nTechnology  \\ntrendsA match made  \\nin AIMeet my agent The space  \\nwe needOur bodies  \\nelectronic\\nDigital identity Your data, my \\ndata, our dataGeneralizing AI\\nWebMe Programmable  \\nworldThe unreal Computing  \\nthe impossibleOur forever  \\nfrontier\\nStack  \\nstrategicallyMirrored world I, technologist Anywhere,  \\neverywhereFrom me \\nto weScience tech  \\nThe convergence of science and technology \\ncontinues to influence innovation at large. \\nTechnologies such as AI are accelerating \\nscientific advancements, which are \\nproliferating into industry faster than ever.  \\nNew domains like energy, materials, space, \\nand biology will increasingly take a primary \\nrole in the innovation strategy of the world’s \\nmost disruptive companies. This feedback loop \\nbetween science and technology is expanding \\nthe horizon of what we can compute, creating \\ntools that will allow us to solve bigger \\nproblems, and fundamentally transforming \\nindustries and marketplaces. Digital ownership   \\nThe emergence of digital ownership \\ndriven by technologies like blockchain and \\ndigital ledgers continues to completely \\nupend long held conventions around data, \\nidentity, customer relationships, and online \\necosystems. Distributed computing lets \\nus create unique identities for an array \\nof people and things, allowing for once-\\nimpossible ownership across digital domains. \\nBut ownership itself is not the point – it is \\nwhat this can support. Digital ownership \\ncan excitingly spur new forms of customer \\nengagement, of raising capital, and of \\ninteroperability between digital environments.\\nSustainability \\nFrom regulatory requirements to customer \\npressure to the desire to be more efficient, \\nsustainability remains top of mind among \\nexecutives across industries. And technology \\ninnovation continues to play a vital part in \\ncreating truly circular economies. Emerging \\ntechnologies at enterprises can build cleaner \\nenergy systems, which can offset or diminish \\nnegative environmental impacts. While the \\nshort-term costs of sustainability efforts may \\nconcern some executives, enterprises must \\nnot lose sight of the long-term gains – and \\nhow leveraging technologies can help. The unreal  \\nWhile generative AI has seized the attention of \\nboardrooms around the world, conversations \\non deepfakes, doctored images, and falsified \\nvideos have inevitably followed. Enterprises \\nare in the middle of debates over what is \\nreal, what is not, and whether people really \\ncare. The \"unreal,\" however, can be incredibly \\nadvantageous to enterprises under the right \\ncircumstances. Synthetic data can help us \\nidentify and prepare for edge events. Talking \\nto a “fake” sales assistant could be a better, \\nmore judgement-free customer interaction. Yet \\nto navigate these possibilities, enterprises will \\nstill need to monitor their \"unreal\" solutions’ \\nimpact on people, all while bolstering security \\nand risk practices.2023 \\nTechnology  \\ntrends\\n2022  \\nTechnology  \\ntrends\\n2021 \\nTechnology  \\ntrendsTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  80\\nTechnology Vision 2024  \\nEditorial & Research Team \\n \\nAri Bernstein  \\nJulian Dreiman  \\nMaria Fabbroni  \\nNaomi Nishihara  \\nKrista Schnell  \\nDavid Strachan-Olson \\n Paul Daugherty  \\nChief Technology and Innovation Officer \\nLinkedIn\\nAdam Burden \\nGlobal Innovation Lead  \\nLinkedIn\\nMichael Biltz \\nManaging Director – Accenture Technology Vision \\nLinkedInAccenture  \\nResearch  \\n \\nPrashant P. Shukla, PhD  \\nRenee Byrnes  \\nGerry Farkova  \\nDonovan Griggs  \\nAbira Sathiyanathan  \\nVincenzo Palermo  \\nTal Roded  \\nMariusz Bidelski  \\nLohith Kumar  \\nKrish Jhaveri  \\nShruti Shalini  \\nRaghav Narsalay  \\nSandra Najem  \\nAnna Marszalik  \\nLinda Ringnalda  \\nCarrie Kleiner\\nAuthorsTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  81\\nInput from the Technology Vision \\nExternal Advisory Board, a group of \\nmore than two dozen experienced \\nindividuals from the public and private \\nsectors, academia, venture capital, and \\nentrepreneurial companies. In addition, \\nthe Technology Vision team conducts \\ninterviews with technology luminaries \\nand industry experts, as well as many \\nAccenture business leaders from across \\nthe organization. \\nPrimary research, including a global \\nsurvey of 20,027 consumers to capture \\ninsights into their use of, familiarity \\nwith, and perceptions about technology \\nin their daily lives. In addition, \\nAccenture conducted a survey of 3,450 \\nC-level executives across 21 industries \\nto understand their perspectives and \\norganizational priorities regarding \\nemerging technologies. The surveys \\nwere fielded from October to \\nNovember 2023 across 20 countries. Research and data science to \\nanalyze technology developments \\nand advancements; and generative \\nAI-led interviews of 50 developers, \\nindustrial workers, and advanced \\nusers of spatial computing. \\nAs a shortlist of themes emerges \\nfrom the research process, the \\nTechnology Vision team works to \\nvalidate and refine the set of trends. \\nThe themes are weighed for their \\nrelevance to real-world business \\nchallenges. The Technology Vision \\nteam seeks ideas that transcend the \\nwell-known drivers of technological \\nchange, concentrating instead on \\nthe themes that will soon start to \\nappear on the C-level agendas of \\nmost enterprises.Accenture Labs \\nand Accenture \\nResearch \\ncollaborate on the \\nannual research \\nprocess, which \\nthis year included: \\nFor more than 20 years, Accenture \\nhas developed the Technology Vision \\nreport as a systematic review across \\nthe enterprise landscape to identify \\nemerging technology trends that will \\nhave the greatest impact on companies, \\ngovernment agencies, and other \\norganizations in the coming years.  \\nThis year the trends look five to ten  \\nyears into the future, while remaining \\nrelevant across industries and \\nactionable for businesses today.About the  \\nTechnology  \\nVision Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  82\\n$50 billion or more\\n$30 to $49.9 billion\\n$10 to $29.9 billion\\n$5 to $9.9 billion \\n$1 to $4.9 billion\\n$500 to $999 million1% \\n1% \\n23% \\n31% \\n43% \\n1% Chief Analytics Officer\\nChief Customer Officer\\nChief Data Officer\\nChief Executive Officer\\nChief Experience Officer\\nChief Financial Officer \\nChief HR Officer\\nChief Information Officer\\nChief Innovation Officer\\nChief Marketing Officer\\nChief Operating Officer\\nChief Production Officer\\nChief Sales Officer\\nChief Strategy Officer \\nChief Supply Chain & Operations Officer\\nChief Technology Officer\\nR&D Lead2%\\n2%\\n2%\\n32%\\n2%\\n11%\\n9%\\n5%\\n5%\\n6%\\n4%\\n2%\\n1%\\n8%\\n3%\\n7%\\n1%Aerospace & Defense \\nAirline, Travel & Transport\\nAutomotive\\nBanking\\nBiopharmaceuticals\\nCapital Markets \\nChemicals \\nCommunications, Media & Entertainment \\nConsumer Goods \\nEnergy \\nHealth \\nHigh Technology \\nIndustrial Goods & Equipment \\nInsurance\\nMedTech\\nNatural Resources\\nPrivate Equity \\nPublic Service\\nRetail\\nSoftware & Platforms\\nUtilities Australia\\nBrazil\\nCanada\\nChina\\nFrance\\nGermany\\nIndia\\nIreland\\nItaly\\nJapan\\nNetherlands\\nSaudi Arabia\\nSingapore \\nSouth Africa\\nSpain\\nSweden\\nSwitzerland\\nUnited Arab Emirates\\nUnited Kingdom\\nUnited States5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%4%\\n3%\\n4%\\n8%\\n4%\\n4%\\n5%\\n3%\\n3%\\n4%\\n3%\\n2%\\n3%\\n3%\\n3%\\n3%\\n3%\\n1%\\n4%\\n32%3% \\n5% \\n5% \\n7% \\n3% \\n3% \\n4% \\n8% \\n5% \\n5% \\n6% \\n4% \\n8% \\n8% \\n3% \\n4% \\n1% \\n3% \\n5% \\n5% \\n5% Consumer  \\nsurvey  \\n(N=20,027) CountriesBusiness  \\nsurvey industriesBusiness  \\nsurvey rolesBusiness  \\nsurvey revenues (USD)Business  \\nsurvey  \\n(N=3,450)\\n       \\nSurvey demographicsTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  83\\nIntroduction: Human by design\\n1. Choi, C. (2009, November 11). Human Evolution: The Origin of \\nTool Use. Live Science: https://www.livescience.com/7968-hu -\\nman-evolution-origin-tool.html\\n2. Edwards, B. (2023, May 23). Adobe Photoshop’s new “Generative \\nFill” AI tool lets you manipulate photos with text. Ars Technica: \\nhttps://arstechnica.com/information-technology/2023/05/ado -\\nbe-photoshops-new-generative-fill-ai-tool-lets-you-manipulate-\\nphotos-with-text/\\n3. Capoot, A. (2023, October 9). Google announces new gener -\\native AI search capabilities for doctors. CNBC: https://www.\\ncnbc.com/2023/10/09/google-announces-new-genera -\\ntive-ai-search-capabilities-for-doctors-.html4. Gordon, R. (2023, July 12). Generative AI imagines new protein \\nstructures. MIT News: https://news.mit.edu/2023/genera -\\ntive-ai-imagines-new-protein-structures-0712\\n5. Kalliamvakou, E. (2022, September 7). Research: quantifying \\nGitHub Copilot’s impact on developer productivity and happi -\\nness. GitHub: https://github.blog/2022-09-07-research-quantify -\\ning-github-copilots-impact-on-developer-productivity-and-hap -\\npiness/\\n6. Finnegan, M. (2023, May 24). Microsoft advances mixed-reality \\nplans with Teams avatars, Mesh update. Computerworld: https://\\nwww.computerworld.com/article/3697316/microsoft-advanc -\\nes-mixed-reality-plans-with-teams-avatars-mesh-update.html7. Boston Dynamics Atlas. (2023): https://bostondynamics.com/\\natlas/\\n8. Diaz, J. (2023, May 2). Boston Dynamics robodog just got a \\nChatGPT brain. May it have mercy upon our souls. Fast Company: \\nhttps://www.fastcompany.com/90889271/boston-dynamics-\\nspot-chatgpt-brains\\n9. Metz, C. (2023, May 3). ‘The Godfather of A.I.’ Leaves Google \\nand Warns of Danger Ahead. The New York Times: https://www.\\nnytimes.com/2023/05/01/technology/ai-google-chatbot-engi -\\nneer-quits-hinton.html10. Anderson, M. (2023, April 7). ‘AI Pause’ Open Letter Stokes Fear \\nand Controversy. IEEE Spectrum: https://spectrum.ieee.org/ai-\\npause-letter-stokes-fear\\n11. Hanlon, A. (2022, June 1). Metaverse – together alone? LSE \\nBlogs:  https://blogs.lse.ac.uk/businessreview/2022/06/01/\\nmetaverse-together-alone/\\n12. Airhart, M. (2023, May 1). Brain Activity Decoder Can Reveal Sto -\\nries in People’s Minds. The University of Texas at Austin College of \\nNatural Sciences: https://cns.utexas.edu/news/podcast/brain-ac -\\ntivity-decoder-can-reveal-stories-peoples-mindsReferencesTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  84\\n1. Swoger, J. (2012, July 27). The (mostly true) origins of the scientific \\njournal. Scientific American: https://blogs.scientificamerican.\\ncom/information-culture/the-mostly-true-origins-of-the-scientif -\\nic-journal/  \\n2. Levy, M., Hardy Wise Kent, C., et al. (n.d.). Encyclopædia Britanni -\\nca. Brittanica.com: https://www.britannica.com/topic/Encyclo -\\npaedia-Britannica-English-language-reference-work  \\n3. The Editors of Encyclopaedia Brittanica. (n.d.). Dewey Decimal \\nClassification. Brittanica.com: https://www.britannica.com/sci -\\nence/Dewey-Decimal-Classification \\n4. Xu, S. (n.d.). Fact check with the “Ask NYPL” librarian hotline at \\nthe New York Public Library in NYC. Untapped New York: https://\\nuntappedcities.com/2016/10/24/fact-check-with-the-ask-nypl-li -\\nbrarian-hotline-at-the-new-york-public-library-in-nyc/  \\n5. White, M. (2022). A history of enterprise search 1938-2022. Uni -\\nversity of Sheffield: https://sheffield.pressbooks.pub/eshistory1/\\nchapter/1960-1969-the-pioneers/ \\n6. Borgman, C. (1996, July 1). Why Are Online Catalogs Still Hard to \\nUse? Journal of the American Society for Information Science: \\nhttps://escholarship.org/content/qt3mz7h8hr/qt3mz7h8hr.\\npdf?t=p7cad8\\n7. Nguyen, J. (2020, September 10). Archie, the very first search \\nengine, was released 30 years ago today. Mashable: https://mash -\\nable.com/article/first-search-engine-archie\\n8. Williams, N. (2023, August 10). The Real Reason Ask Jeeves Failed \\nSpectacularly. History-Computer: https://history-computer.com/\\nthe-real-reason-ask-jeeves-failed-spectacularly/ \\n9. Broderick, R. (2023, August 28). The end of the Googleverse. The \\nVerge: https://www.theverge.com/23846048/google-search-me -\\nmes-images-pagerank-altavista-seo-keywords  10. The Editors of Encyclopaedia Brittanica. (n.d.). Wikipedia. Brittani -\\nca.com: https://www.britannica.com/topic/Wikipedia  \\n11. Hanlon, J. (2013, September 16). Five years ago, Stack Overflow \\nlaunched. Then, a miracle occurred. Stack Overflow Blog: https://\\nstackoverflow.blog/2013/09/16/five-years-ago-stack-overflow-\\nlaunched-then-a-miracle-occurred  \\n12. Lutkevich, B., Scardina, J., et al. (n.d.). Microsoft SharePoint. \\nTechTarget: https://www.techtarget.com/searchcontentmanage -\\nment/definition/Microsoft-SharePoint-2016  \\n13. Knowledge Graph. (2022, December 2). Scholarly Community \\nEncyclopedia: https://encyclopedia.pub/entry/37713\\n14. Liu, W., Zhou, P., et al. (2019, September 17). K-BERT: Enabling \\nLanguage Representation with Knowledge Graph. arXiv: https://\\narxiv.org/abs/1909.07606  \\n15. Stringers, A., and Wiggers, K. (2023, September 28). ChatGPT: \\nEverything you need to know about the AI-powered chatbot. \\nTechCrunch: https://techcrunch.com/2023/08/10/chatgpt-every -\\nthing-you-need-to-know-about-the-open-ai-powered-chatbot/  \\n16. Warren, T. (2023, August 7). Microsoft’s AI-powered Bing chat is \\ncoming to mobile browsers. The Verge: https://www.theverge.\\ncom/2023/8/7/23822773/microsoft-bing-ai-chat-mobile-brows -\\ners-google-chrome-safari\\n17. Carr, N. (2008, July/August). Is Google Making Us Stupid? The At -\\nlantic: https://www.theatlantic.com/magazine/archive/2008/07/\\nis-google-making-us-stupid/306868/  \\n18. Organic Search Improves Ability to Map to Consumer Intent: Or -\\nganic Channel Share Expands to 53.3% of Traffic. (2019). Bright -\\nEdge Research: https://videos.brightedge.com/research-report/\\nBrightEdge_ChannelReport2019_FINAL.pdf  19. Metz, C., and Grant, N. (2022, December 21). A New Chat Bot Is \\na ‘Code Red’ for Google’s Search Business. The New York Times: \\nhttps://www.nytimes.com/2022/12/21/technology/ai-chatgpt-\\ngoogle-search.html  \\n20. Hu, K. (2023, February 2). ChatGPT sets record for fastest-grow -\\ning user base – analyst note. Reuters: https://www.reuters.com/\\ntechnology/chatgpt-sets-record-fastest-growing-user-base-ana -\\nlyst-note-2023-02-01/  \\n21. Mehdi, Y. (2023, February 7). Reinventing search with a new \\nAI-powered Microsoft Bing and Edge, your copilot for the \\nweb. Official Microsoft Blog: https://blogs.microsoft.com/\\nblog/2023/02/07/reinventing-search-with-a-new-ai-powered-mi -\\ncrosoft-bing-and-edge-your-copilot-for-the-web/  \\n22. Salesforce Announces Einstein GPT, the World’s First Generative \\nAI for CRM. (2023, March 7). Salesforce News & Insights: https://\\nwww.salesforce.com/news/press-releases/2023/03/07/ein -\\nstein-generative-ai/  \\n23. Landi, H. (2023, June 27). Epic, Nuance bring ambient listening, \\nGPT-4 tools to the exam room to help save doctors time. Fierce \\nHealthcare: https://www.fiercehealthcare.com/health-tech/epic-\\nnuance-build-out-more-gpt4-tools-ehrs-help-save-doctors-time  \\n24. Gartner Press Release. Gartner Survey Reveals 47% of Digital \\nWorkers Struggle to Find the Information Needed to Effectively \\nPerform Their Jobs. (2023, May 10). Gartner: https://www.gartner.\\ncom/en/newsroom/press-releases/2023-05-10-gartner-survey-\\nreveals-47-percent-of-digital-workers-struggle-to-find-the-infor -\\nmation-needed-to-effectively-perform-their-jobs  GARTNER is a \\nregistered trademark and service mark of Gartner, Inc. and/or its \\naffiliates in the U.S. and internationally and is used herein with \\npermission. All rights reserved.\\n25. Hilger, J. (2022, February 17). The Top 5 KM Technologies. Enter -\\nprise Knowledge: https://enterprise-knowledge.com/the-top-5-\\nkm-technologies/  \\n26. Hilger, J. Top 5 KM technologies. 27. CXOtoday News Desk. (2022, August 29). Real-Time Graph Analy -\\nsis of Documents Saves Company Over 4 Million Employee Hours. \\nCXO Today: https://www.cxotoday.com/case-studies/real-time-\\ngraph-analysis-of-documents-saves-company-over-4-million-em -\\nployee-hours/  \\n28. Trajanoska, M., Stojanov, R., et al. (2023, May 8). Enhancing \\nKnowledge Graph Construction Using Large Language Models. \\narXiv: https://arxiv.org/abs/2305.04676  \\n29. Rao, D. (2023, April 15). How to use large language models and \\nknowledge graphs to manage enterprise data. VentureBeat: \\nhttps://venturebeat.com/ai/how-to-use-large-language-models-\\nand-knowledge-graphs-to-manage-enterprise-data/  \\n30. Introducing BloombergGPT, Bloomberg’s 50-billion parameter \\nlarge language model, purpose-built from scratch for finance. \\n(2023, March 30). Bloomberg: https://www.bloomberg.com/\\ncompany/press/bloomberggpt-50-billion-parameter-llm-tuned-\\nfinance/  \\n31. Shah, A. (2023, April 6). Bloomberg Uses 1.3 Million Hours of GPU \\nTime for Homegrown Large-Language Model. HPC Wire: https://\\nwww.hpcwire.com/2023/04/06/bloomberg-uses-1-3-million-\\nhours-of-gpu-time-for-homegrown-large-language-model/  \\n32. Davis, W. (2023, August 22). OpenAI opens GPT-3.5 Tur -\\nbo up for custom tuning. The Verge: https://www.theverge.\\ncom/2023/8/22/23842042/openai-gpt-3-5-turbo-fine-tuning-en -\\nterprise-business-custom-chatbot-ai-artificial-intelligence  \\n33. Sivasubramanian, S. (2023, April 13). Announcing New Tools for \\nBuilding with Generative AI on AWS. AWS Blog Home: https://aws.\\namazon.com/blogs/machine-learning/announcing-new-tools-for-\\nbuilding-with-generative-ai-on-aws/  \\n34. Hawk, J. (2023, May 23). Build next-generation, AI-powered appli -\\ncations on Microsoft Azure. Azure Blog: https://azure.microsoft.\\ncom/en-us/blog/build-next-generation-ai-powered-applica -\\ntions-on-microsoft-azure/  A match made in AITechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  85\\n35. Customize and deploy generative models. (n.d.). Google Cloud: \\nhttps://cloud.google.com/generative-ai-studio  \\n36. Wang, Z. (2023, May 17). The Underdog Revolution: How Smaller \\nLanguage Models Can Outperform LLMs. Deepgram: https://\\ndeepgram.com/learn/the-underdog-revolution-how-smaller-lan -\\nguage-models-outperform-llms  \\n37. Berger, E. (2023, June 9). Grounding LLMs. Microsoft Blog: \\nhttps://techcommunity.microsoft.com/t5/fasttrack-for-azure/\\ngrounding-llms/ba-p/3843857 \\n38. How to Request Technical Support for Einstein GPT Products. \\n(2023, July 19). Salesforce Product Support: https://help.sales -\\nforce.com/s/articleView?id=000395674&type=1  \\n39. Mata v. Avianca, Inc. (2023, June 22). Casetext: https://casetext.\\ncom/case/mata-v-avianca-inc-2  \\n40. Claburn, T. (2023, June 22). Lawyers who cited fake cases halluci -\\nnated by ChatGPT must pay. The Register: https://www.theregis -\\nter.com/2023/06/22/lawyers_fake_cases/   \\n41. Moran, L. (2023, May 30). Lawyer cites fake cases generated \\nby ChatGPT in legal brief. Legal Dive: https://www.legaldive.\\ncom/news/chatgpt-fake-legal-cases-generative-ai-hallucina -\\ntions/651557/  \\n42. Baxter, K., and Schlesinger, Y. (2023, June 6). Managing the \\nRisks of Generative AI. Harvard Business Review: https://hbr.\\norg/2023/06/managing-the-risks-of-generative-ai  \\n43. Miller, R. (2023, May 11). Writer introduces product that could \\nhelp reduce hallucinated content in its LLMs. TechCrunch: \\nhttps://techcrunch.com/2023/05/11/writer-introduces-prod -\\nuct-that-could-help-reduce-hallucinated-content-in-its-llms/  44. Morgan Stanley wealth management deploys GPT-4 to organize \\nits vast knowledge base. (2023, March 14). OpenAI: https://ope -\\nnai.com/customer-stories/morgan-stanley  \\n45. Wheeler, R., and Stone, J. (2023, May). Using Large Language \\nModels and Generative AI to Build a Digital Owner’s Manual. \\nToyota Connected: https://www.toyotaconnected.com/insights/\\nusing-large-language-models-and-generative-ai-to-build-a-digi -\\ntal-owners-manual  \\n46. Brynjolfsson, E., Li, D., et al. (2023, April). Generative AI At Work. \\nNational Bureau of Economic Research: https://www.nber.org/\\nsystem/files/working_papers/w31161/w31161.pdf \\n47. Malik, A. (2023, March 9). Discord updates its bot with ChatGPT-\\nlike features, rolls out AI-generated conversation summaries \\nand more. TechCrunch: https://techcrunch.com/2023/03/09/\\ndiscord-updates-its-bot-with-chatgpt-like-features-rolls-out-ai-\\ngenerated-conversation-summaries-and-more/  \\n48. Early Insights on My AI. (2023, June 11). Snapchat Newsroom: \\nhttps://newsroom.snap.com/early-insights-on-my-ai  \\n49. Bing Chat. (n.d.). Microsoft Edge: https://www.microsoft.com/\\nen-us/edge/features/bing-chat?form=MT00D8  \\n50. Reid, E. (2023, May 10). Supercharging Search with generative \\nAI. The Keyword: https://blog.google/products/search/genera -\\ntive-ai-search/  \\n51. Lee, M. (2023, June 27). Looking for help buying a car? Ask \\nChatGPT. How new plugins can save you time (and money). USA \\nToday: https://www.usatoday.com/story/money/personalfi -\\nnance/2023/06/27/chatgpt-car-buying-purchasing-research-de -\\ntails/70358652007/  \\n52. Ahmad, F. (2023, May 17). Bard Plugins: How to Add New Features \\nand Functionality. Tech Craving: https://www.techcarving.com/\\nbard-plugins-how-to-add-new-features-and-functionality/  1. Josić, K. (2012, January 11). The Mechanical Turk. The Engines of \\nour Ingenuity: https://engines.egr.uh.edu/episode/2765  \\n2. Almon Brown Strowger. (n.d.). National Inventors Hall of Fame: \\nhttps://www.invent.org/inductees/almon-brown-strowger\\n3. Marsh, A. (2022, August 30). In 1961, the First Robot Arm Punched \\nIn. IEEE Spectrum: https://spectrum.ieee.org/unimation-robot\\n4. Anker, A. (1995, October 1). BargainFinder. Wired: https://www.\\nwired.com/1995/10/bargainfinder/ \\n5. Meyer, R. (2015, June 23). Even Early Focus Groups Hated \\nClippy. The Atlantic: https://www.theatlantic.com/technology/\\narchive/2015/06/clippy-the-microsoft-office-assistant-is-the-pa -\\ntriarchys-fault/396653/ \\n6. Bonabeau, E. (2002, May 14). Agent-based modeling: Methods \\nand techniques for simulating human systems. Proceedings of \\nthe National Academy of Sciences: https://www.pnas.org/doi/\\npdf/10.1073/pnas.082080899 \\n7. About Us. (n.d.). eSnipe: https://www.esnipe.com/about \\n8. Berry, B., Kiel, L., et al. (2002, May 14). Adaptive agents, intelli -\\ngence, and emergent human organization: Capturing complexity \\nthrough agent-based modeling. Proceedings of the National \\nAcademy of Sciences: https://www.pnas.org/doi/epdf/10.1073/\\npnas.092078899 \\n9. Siri. (n.d.). SRI International: https://www.sri.com/hoi/siri/  \\n10. Charles Schwab Launches Schwab Intelligent Portfolios. (2015, \\nMarch 9). BusinessWire: https://www.businesswire.com/\\nnews/home/20150309005290/en/Charles-Schwab-Launch -\\nes-Schwab-Intelligent-Portfolios11. Shirado, H., and Christakis, N. (2018, April 23). Locally Noisy \\nAutonomous Agents Improve Global Human Coordination in Net -\\nwork Experiments. Nature: https://www.ncbi.nlm.nih.gov/pmc/\\narticles/PMC5912653/ \\n12. The AlphaStar Team. (2019, October 30). AlphaStar: Grandmaster \\nlevel in StarCraft II using multi-agent reinforcement learning. \\nDeepMind Research: https://www.deepmind.com/blog/alphastar-\\ngrandmaster-level-in-starcraft-ii-using-multi-agent-reinforce -\\nment-learning\\n13. Wessling, B. (2022, June 22). A decade after acquiring Kiva, \\nAmazon unveils its first AMR. The Robot Report: https://www.\\ntherobotreport.com/a-decade-after-acquiring-kiva-amazon-un-\\nveils-its-first-amr/ \\n14. Fried, I. (2023, September 25). We tested ChatGPT’s new sup -\\nport for images and voice search. Axios: https://www.axios.\\ncom/2023/09/25/testing-chatgpt-support-images-voice-search\\n15. Sullivan, M. (2023, April 13). Auto-GPT and BabyAGI: How ‘au -\\ntonomous agents’ are bringing generative AI to the masses. Fast \\nCompany: https://www.fastcompany.com/90880294/auto-gpt-\\nand-babyagi-how-autonomous-agents-are-bringing-generative-\\nai-to-the-masses  \\n16. Adobe Firefly: Your imagination’s new best friend. (n.d.). Adobe.\\ncom: https://www.adobe.com/sensei/generative-ai/firefly.html \\n17. Greenfield, D. (2019, August 1). Volkswagen Applies Generative \\nDesign. Automation World: https://www.automationworld.com/\\nproducts/software/blog/13320039/volkswagen-applies-genera -\\ntive-design \\n18. Deslandes, N. (2023, July 4). Siemens and Intrinsic to make AI-\\nbased robotics accessible to SMEs. Tech Informed: https://techin -\\nformed.com/siemens-and-intrinsic-to-make-ai-based-robotics-\\naccessible-to-smes/  \\n19. Micropsi Industries’ AI Software MIRAI Now Compatible with \\nFANUC Robots. (2022, November 30). Automation.com: https://\\nwww.automation.com/en-us/articles/november-2022/microp -\\nsi-industries-ai-software-mirai-fanuc-robots  Meet my agentTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  86\\n20. Dencheva, V. (2023, June 7). Generative AI in marketing – statis -\\ntics & facts. Statista: https://www.statista.com/topics/10994/\\ngenerative-ai-in-marketing/#topicOverview  \\n21. Bastian, M. (2023, April 30). How GPT-4 and AutoGPT could save \\nyou money. The Decoder: https://the-decoder.com/how-gpt-4-\\nand-auto-gpt-could-save-you-money/ \\n22. Daugherty, P., Ghosh, B., et al. (2023). A new era of generative AI \\nfor everyone. Accenture: https://www.accenture.com/content/\\ndam/accenture/final/accenture-com/document/Accenture-A-\\nNew-Era-of-Generative-AI-for-Everyone.pdf \\n23. ChatGPT plugins. (2023, March 23). OpenAI: https://openai.com/\\nblog/chatgpt-plugins \\n24. Peterson, J. (2023, July 31). A Beginner’s Guide to Using ChatGPT \\nPlugins. Lifehacker: https://lifehacker.com/a-beginners-guide-to-\\nusing-chatgpt-plugins-1850578719  \\n25. Wei, J., and Zhou, D. (2022, May 11). Language Models Perform \\nReasoning via Chain of Thought. Google Research: https://\\nai.googleblog.com/2022/05/language-models-perform-reason -\\ning-via.html \\n26. Wei, J., Wang, X., et al. (2022, Janurary 28). Chain-of-Thought \\nPrompting Elicits Reasoning in Large Language Models. arXiv: \\nhttps://arxiv.org/abs/2201.11903 \\n27. Shinn, N., Cassano, F., et al. (2023, March 20). Reflexion: Lan -\\nguage Agents with Verbal Reinforcement Learning. arXiv: https://\\narxiv.org/abs/2303.11366\\n28. CyberNews. (2023, June). What is AutoGPT? Explained in 2 min -\\nutes. YouTube:  https://www.youtube.com/watch?v=-N5Rhx-1K7w  \\n29. Parthasarathy, S. (2023, May 10). Meet BabyAGI – The Autono -\\nmous AI Agent to Streamline Your Tasks. TowardsAI: https://pub.\\ntowardsai.net/meet-babyagi-the-autonomous-ai-agent-to-stream -\\nline-your-tasks-f44cb9de5503  30. Suki. (2023, June 2). VOYAGER: Revolutionizing Lifelong Learning \\nin Minecraft with LLM Technology. Medium: https://medium.\\ncom/@SukiLLM/voyager-revolutionizing-lifelong-learning-in-mi -\\nnecraft-with-llm-technology-ec99a1bf867b \\n31. Wang, G., Xie, Y., et al. (2023, May 25). Voyager: An Open-Ended \\nEmbodied Agent with Large Language Models. arXiv: https://arxiv.\\norg/abs/2305.16291\\n32. Tool up! DeepMind, Princeton & Stanford’s LATM Enables LLMs to \\nMake Their Own Tools. (2023, May 31). Synced: https://syncedre -\\nview.com/2023/05/31/tool-up-deepmind-princeton-stanfords-\\nlatm-enables-llms-to-make-their-own-tools/ \\n33. Pejcha, C. (2023, April 13). Google put 25 AI agents together in a \\nSims-inspired virtual town, and told them to go about their lives. \\nDocument Journal: https://www.documentjournal.com/2023/04/\\ngoogle-ai-study-creates-chatgpt-generative-agents-that-simu -\\nlate-artificial-human-society/  \\n34. Fu, Y., Peng, H., et al. (2023, May 17). Improving Language Model \\nNegotiation with Self-Play and In-Context Learning from AI Feed -\\nback. arXiv: https://arxiv.org/abs/2305.10142\\n35. Boiko, D., MacKnight, R., et al. (2023, April 11). Emergent autono -\\nmous scientific research capabilities of large language models. \\narXiv: https://arxiv.org/abs/2304.05332 \\n36. Driess, D., and Florence, P. (2023, March 10). PaLM-E: An em -\\nbodied multimodal language model. Google Research: https://\\nai.googleblog.com/2023/03/palm-e-embodied-multimodal-lan -\\nguage.html \\n37. Xing, P. (2023, August 5). MetaGPT: a Multi-Agent Framework to \\nAutomate Your Software Company. DataDrivenInvestor: https://\\nmedium.datadriveninvestor.com/metagpt-a-multi-agent-frame -\\nwork-to-automate-your-software-company-4b6ae747cc36  \\n38. Machines make up 43% of digital identities on enterprise net -\\nworks. (n.d.). Security Magazine: https://www.securitymagazine.\\ncom/articles/98401-machines-make-up-43-of-digital-identi -\\nties-on-enterprise-networks 39. Davenport, T., and Alavi, M. (2023, July 6). How to Train Gener -\\native AI Using Your Company’s Data. Harvard Business Review: \\nhttps://hbr.org/2023/07/how-to-train-generative-ai-using-your-\\ncompanys-data\\n40. Davenport, T. (2023, June 26). Fast, Cheap, And In Control: Gen -\\nerative AI In Morningstar’s Mo. Forbes: https://www.forbes.com/\\nsites/tomdavenport/2023/06/26/fast-cheap-and-in-control-gen -\\nerative-ai-in-morningstars-mo/?sh=651146f716ae \\n41. Davenport, T. (2023, March 20). How Morgan Stanley Is Training \\nGPT To Help Financial Advisors. Forbes: https://www.forbes.com/\\nsites/tomdavenport/2023/03/20/how-morgan-stanley-is-training-\\ngpt-to-help-financial-advisors/?sh=6184710c3fc3  \\n42. Ren, A., Dixit, A., et al. (2023, September 4). Robots That Ask For \\nHelp: Uncertainty Alignment for Large Language Model Planners. \\narXiv: https://arxiv.org/pdf/2307.01928.pdf\\nThe space we need\\n1. Morton Heilig: Inventor VR. (n.d). USC School of Cinematic Arts: \\nhttps://www.uschefnerarchive.com/morton-heilig-inventor-vr/\\n2. Laidlaw, Z. (2020, November 15). What are haptics, and why are \\nthey such an important part of your smartphone? Android Police: \\nhttps://www.androidpolice.com/2020/11/15/what-are-haptics-\\nand-why-are-they-such-an-important-part-of-your-smartphone-\\nsponsored/  \\n3. PARC History. (n.d.). PARC: https://www.parc.com/about-parc/\\nparc-history/ \\n4. Rosenberg, L. (2022, April 7). How A Parachute Accident Helped \\nJump-Start Augmented Reality. IEEE Spectrum: https://spectrum.\\nieee.org/history-of-augmented-reality\\n5. Physical, Elecrical, Digital. (n.d.). NYU: https://kimon.hosting.nyu.\\nedu/physical-electrical-digital/items/show/1518  6. Greenwold, S. (1995, June). Spatial Computing. MIT: https://acg.\\nmedia.mit.edu/people/simong/thesis/SpatialComputing.pdf  \\n7. Dredge, S. (2019, September 28). All you need to know about Ro -\\nblox. The Guardian: https://www.theguardian.com/games/2019/\\nsep/28/roblox-guide-children-gaming-platform-developer-mine -\\ncraft-fortnite\\n8. Javornik, A. (2016, October 4). The Mainstreaming of Augmented \\nReality: A Brief History. Harvard Business Review: https://hbr.\\norg/2016/10/the-mainstreaming-of-augmented-reality-a-brief-his -\\ntory\\n9. Warren, T. (2017, October 25). Microsoft kills off Kinect, \\nstops manufacturing it. The Verge: https://www.theverge.\\ncom/2017/10/25/16542870/microsoft-kinect-dead-stop-manufac -\\nturing\\n10. Kumparak, G. (2014, March 26). A Brief History of Oculus. Tech -\\nCrunch: https://techcrunch.com/2014/03/26/a-brief-history-of-\\noculus/\\n11. Leswing, K. (2023, March 15). Google ends enterprise sales \\nof Google Glass, its augmented reality smartglasses. CNBC: \\nhttps://www.cnbc.com/2023/03/15/google-discontinues-goo -\\ngle-glass-enterprise-end-to-early-ar-project.html \\n12. Clement, J. (2023, August 7). Global Pokémon GO app down -\\nloads as of Q2 2023. Statista: https://www.statista.com/statis -\\ntics/641690/pokemon-go-number-of-downloads-worldwide/\\n13. Robertson, A. (2017, June 5). Apple is launching an iOS \\n‘ARKit’ for augmented reality apps. The Verge: https://www.\\ntheverge.com/2017/6/5/15732832/apple-augmented-reali -\\nty-arkit-ar-sdk-wwdc-2017\\n14. Christian, J. (2018, September 27). Make Music A Full Body Experi -\\nence With A “Vibro-Tactile” Suit. Futurism: https://futurism.com/\\nthe-byte/vibro-tactile-suit-musicTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  87\\n15. Newton, C. (2019, April 4). Snapchat’s new Landmarkers will make \\nthe Eiffel Tower puke rainbows. The Verge: https://www.theverge.\\ncom/2019/4/4/18294062/snapchat-landmarkers-ar-lenses-fil -\\nters-eiffel-tower-rainbows\\n16. Takahashi, D. (2020, May 14). Nvidia launches early access for Om -\\nniverse computer graphics and simulation platform. VentureBeat: \\nhttps://venturebeat.com/business/nvidia-launches-early-ac -\\ncess-for-omniverse-computer-graphics-and-simulation-platform/\\n17. Roach, J. (2021, November 2). Mesh for Microsoft Teams aims to \\nmake collaboration in the ‘metaverse’ personal and fun. Micro -\\nsoft: https://news.microsoft.com/source/features/innovation/\\nmesh-for-microsoft-teams \\n18. Introducing Apple Vision Pro: Apple’s first spatial comput -\\ner. (2023, June 5). Apple: https://www.apple.com/news -\\nroom/2023/06/introducing-apple-vision-pro\\n19. Apple\\'s long history of lousy first reviews. (2015). The Week: \\nhttps://theweek.com/articles/496303/apples-long-history-lousy-\\nfirst-reviews\\n20. Slattery, B. (2010, January 28). Apple iPad Reviews: The Critics \\nWeigh In. PC World: https://www.pcworld.com/article/516566/\\napple_ipad_reviews-2.html \\n21. Pogue, D. (2010). Looking at the iPad From Two Angles. The New \\nYork Times: https://www.nytimes.com/2010/04/01/technology/\\npersonaltech/01pogue.html \\n22. Young, M. (2023, May 26). Metaverse ‘Virtual Land Barons’ Down \\nBad in 2023 as Prices Decline Further. Be(in)Crypto: https://bein -\\ncrypto.com/metaverse-virtual-land-barons-down-2023-prices-\\ndecline/ \\n23. Hatmaker, T. (2023, February 3). Meta’s Reality Labs lost $13.7 \\nbillion on VR and AR last year. TechCrunch: https://techcrunch.\\ncom/2023/02/03/metas-reality-labs-lost-13-7-billion-on-vr-and-\\nar-last-year/ 24. Joseph, S. (2023, May 1). Meta Introduces Codex Avatars, Life-\\nLike Avatars that can be Created Using a Smartphone. The Tech \\nOutlook: https://www.thetechoutlook.com/news/innovation/\\nmeta-introduces-codex-avatars-life-like-avatars-that-can-be-cre -\\nated-using-a-smartphone/ \\n25. Peters, J. (2022, December 1). Epic’s free app that turns real-life \\nitems into 3D models is available now on iOS. The Verge: https://\\nwww.theverge.com/2022/12/1/23488421/epic-games-reality -\\nscan-ios-app-scan-objects-3d-models  \\n26. Reid, E. (2020, February 5).  A look back at 15 years of mapping \\nthe world. Google:  https://blog.google/products/maps/look-\\nback-15-years-mapping-world/ \\n27. Introduction to USD. (n.d.). OpenUSD: https://openusd.org/re -\\nlease/intro.html \\n28. Universal Scene Description. (n.d.). Nvidia: https://www.nvidia.\\ncom/en-us/omniverse/usd/ \\n29. Pixar, Adobe, Apple, Autodesk, and NVIDIA form Alliance for \\nOpenUSD to drive open standards for 3D content. (2023, \\nAugust 1). Apple: https://www.apple.com/newsroom/2023/08/\\npixar-adobe-apple-autodesk-and-nvidia-form-alliance-for-\\nopenusd/?1690894762 \\n30. Loder, C. (2023, June 22). Getting started with Apple Vision Pro \\ndeveloper software. Apple Insider: https://appleinsider.com/\\narticles/23/06/22/getting-started-with-apple-vision-pro-develop -\\ner-software \\n31. OpenXR. (n.d.). Khronos Group: https://www.khronos.org/openxr/ \\n32. Takahashi, D. (2023, July 3). OMA3 offers way for users to travel \\nbetween blockchain gaming worlds in the metaverse. Venture -\\nBeat: https://venturebeat.com/games/oma3-offers-way-for-users-\\nto-travel-between-blockchain-gaming-worlds-in-the-metaverse/  33. Deltombe, A. (2020, October 20). How Much Does it Cost to \\nCreate 3D Models? Sketchfab: https://sketchfab.com/blogs/enter -\\nprise/create-3d-models-cost \\n34. Salian, I. (2023, June 1). Digital Renaissance: NVIDIA Neuralangelo \\nResearch Reconstructs 3D Scenes. Nvidia: https://blogs.nvidia.\\ncom/blog/2023/06/01/neuralangelo-ai-research-3d-reconstruc -\\ntion/\\n35. Intel Labs Introduces AI Diffusion Model, Generates 360-Degree \\nImages from Text Prompts. (2023, June 21). Intel: https://www.\\nintel.com/content/www/us/en/newsroom/news/intel-introduc -\\nes-3d-generative-ai-model.html\\n36. Qualcomm Accelerates an Open, XR Developer Ecosystem with \\nSnapdragon Spaces Milestones. (2023, May 31). Qualcomm: \\nhttps://www.qualcomm.com/news/releases/2023/05/qual -\\ncomm-accelerates-an-open--xr-developer-ecosystem-with-snap -\\ndr \\n37. Kemelmacher-Shlizerman, I. (2023, June 14). How AI makes virtual \\ntry-on more realistic. Google: https://blog.google/products/shop -\\nping/virtual-try-on-google-generative-ai/ \\n38. Silva, S. (2023, May 10). Create world-scale augmented re -\\nality experiences in minutes with Google’s Geospatial Cre -\\nator. Google: https://developers.googleblog.com/2023/05/\\ncreate-world-scale-augmented-reality-experiences-in-min -\\nutes-with-google-geospatial-creator.html \\n39. Designing the future: How we prototype in AR and VR. (2023, \\nJanuary 19). Meta: https://design.facebook.com/stories/design -\\ning-the-future-how-we-prototype-in-ar-and-vr/ \\n40. Inworld AI: 99% of gamers are excited by the potential of smart \\nNPCs powered by advanced artificial intelligence. (2023, February \\n15). PR Newswire: https://www.prnewswire.com/news-releases/\\ninworld-ai-99-of-gamers-are-excited-by-the-potential-of-smart-\\nnpcs-powered-by-advanced-artificial-intelligence-301747635.\\nhtml\\n41. Gibbs, K. (2023, July 18). Feature Release Week 1: Our Improved \\nCharacter Brain. Inworld: https://www.inworld.ai/blog/im -\\nproved-character-brain 42. Contextual Mesh. (n.d.). Inworld: https://www.inworld.ai/#contex -\\ntual-mesh \\n43. Ackerman, E. (2023, April 22). Haptic System Creates Fin -\\nger-Touch Sensations Hardware-Free. IEEE Spectrum: https://\\nspectrum.ieee.org/finger-haptics \\n44. Kersley, A. (2023, April 11). Smell Your Way Out of the Uncanny \\nValley. Wired: https://www.wired.com/story/scentient-smell-vir -\\ntual-reality/ \\n45. McKeague, M., Cohrs, J., et al. (2023, March 16). Adapting Spatial \\nAudio for Browser-Based 3D Storytelling. New York Times: https://\\nrd.nytimes.com/projects/adapting-spatial-audio-for-brows -\\ner-based-3d-storytelling\\n46. Schwartz, M., Khurana, M., et al. (2023, March 3). The Quest \\nto Restore Notre Dame’s Glorious Sound. The New York Times: \\nhttps://www.nytimes.com/interactive/2023/03/03/magazine/\\nnotre-dame-cathedral-acoustics-sound.html\\n47. Lesiv, A. (2023, June 28). The Dawn of Spatial Computing. Every: \\nhttps://every.to/p/the-dawn-of-spatial-computing\\n48. Peters, J. (2023, February 21). Meta is improving Quest hand \\ntracking so you can touch buttons and type on virtual keyboards. \\nThe Verge: https://www.theverge.com/2023/2/21/23609001/me -\\nta-quest-pro-hand-tracking-direct-touch-v50  \\n49. Blumstein, G., Zukotynski, B., et al. (2020, February 5). Random -\\nized trial of a virtual reality tool to teach surgical technique for tib -\\nial shaft fracture intramedullary nailing. National Library of Medi -\\ncine:  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7351249/  \\n50. Validation study: Haptic feedback is proven to improve surgical \\neducation (2022, October). Fundamental Surgery: https://funda -\\nmentalsurgery.com/validation-study-haptic-feedback/ \\n51. Zhenhuan, M. (2023, February 12). Gallbladder surgery done at \\nlong distance with 5G. China Daily: https://www.chinadaily.com.\\ncn/a/202302/18/WS63f0223ea31057c47ebaf7e9.html Technology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  88\\n52. Unity, Vancouver Airport Authority Partner to Leverage Digital \\nTwins. (2023, June 19). XR Today: https://www.xrtoday.com/\\nvirtual-reality/unity-vancouver-airport-authority-partner-to-lever -\\nage-digital-twins/ \\n53. “Playing Pompidou”: Christian Marclay and Snap’s AR Studio \\ntransform The Centre Pompidou into a musical instrument. (2022, \\nNovember 16). Snap: https://newsroom.snap.com/playing-pom -\\npidou \\n54. Purdy, M. (2023, April 3). Building a Great Customer Experi -\\nence in the Metaverse. Harvard Business Review: https://hbr.\\norg/2023/04/building-a-great-customer-experience-in-the-\\nmetaverse \\n55. FIAT Metaverse Store, the world\\'s first metaverse-powered show -\\nroom, a revolution in customer experience. (2022, December 1). \\nStellantis: https://www.media.stellantis.com/uk-en/fiat/press/\\nfiat-metaverse-store-the-world-s-first-metaverse-powered-show -\\nroom-a-revolution-in-customer-experience \\n56. Kumaravel, B. T. and Hartmann, B. (2022, May 12). Interactive \\nMixed-Dimensional Media for Cross-Dimensional Collaboration in \\nMixed Reality Environments. Frontiers: https://www.frontiersin.\\norg/articles/10.3389/frvir.2022.766336/full \\n57. Gap x Mattel: Barbie\\'s Times Square Takeover powered by Goo -\\ngle\\'s Geospatial Creator. (n.d.). Unit9: https://www.unit9.com/\\nproject/gap-x-mattel-barbies-times-square-takeower-powered-\\nby-googles-geospatial-creator \\n58. Telstra, Google, and Accenture develop 5G powered augmented \\nreality wayfinding experience at Marvel Stadium. (2022, May 11). \\nAccenture: https://newsroom.accenture.com/news/telstra-goo -\\ngle-and-accenture-develop-5g-powered-augmented-reality-way -\\nfinding-experience-at-marvel-stadium.htm \\n59. Augmented for surgical success—a reality now. (n.d.). Accenture: \\nhttps://www.accenture.com/fi-en/case-studies/technology/mic -\\nrosoft-hololens-surgery 1. Kawala-Sterniuk, A., and Browarska, N., et al. (2021, January 3). \\nSummary of over Fifty Years with Brain-Computer Interfaces—A \\nReview. National Library of Medicine: https://www.ncbi.nlm.nih.\\ngov/pmc/articles/PMC7824107/\\n2. Di Stasi, L., and Diaz-Piedra, C. (2021, January 14). Re-examining \\nthe Pioneering Studies on Eye Movements in Aviation: Connecting \\nthe Past to the Present. ResearchGate: https://www.researchgate.\\nnet/publication/346675919_Re-examining_the_Pioneering_Stud -\\nies_on_Eye_Movements_in_Aviation_Connecting_the_Past_to_the_\\nPresent \\n3. Spicer, D. (2021, June 9). Audrey, Alexa, Hal, and More. Computer \\nHistory Museum: https://computerhistory.org/blog/audrey-alexa-\\nhal-and-more/ \\n4. A brief history of Facial Recognition. (2022, May 12). NEC: \\nhttps://www.nec.co.nz/market-leadership/publications-me -\\ndia/a-brief-history-of-facial-recognition/\\n5. Glover, G. (2011, April). Overview of Functional Magnetic Reso -\\nnance Imaging. National Library of Medicine: https://www.ncbi.\\nnlm.nih.gov/pmc/articles/PMC3073717/\\n6. Singleton, S. (2000, July 11). How Cookie-Gate Crumbles. Cato \\nInstitute: https://www.cato.org/commentary/how-cookie-gate-\\ncrumbles\\n7. Kawala-Sterniuk, A., and Browarska, N., et al. Summary of over \\nFifty Years with Brain-Computer Interfaces.\\n8. Volvo Cars introduces new systems for alerting tired and distract -\\ned drivers. (2007, August 31). Volvo Cars: https://www.media.\\nvolvocars.com/us/en-us/media/pressreleases/12130\\n9. Ewalt, D. (2010, June 11). Getting Fitbit. Forbes: https://www.\\nforbes.com/2010/06/11/fitbit-tracker-pedometer-lifestyle-heat -\\nlh-lifetracking.html 10. Katzmaier, D. (2012, April 9). Samsung Smart Interaction: Hands-\\non with voice and gesture control. CNET: https://www.cnet.com/\\ntech/home-entertainment/samsung-smart-interaction-hands-on-\\nwith-voice-and-gesture-control/\\n11. DARPA and the Brain Initiative. (n.d.). DARPA: https://www.darpa.\\nmil/program/our-research/darpa-and-the-brain-initiative\\n12. Wong, J. (2023, June 15). Oura. Contrary Research: https://re -\\nsearch.contrary.com/reports/oura#\\n13. Revell, T. (2023, May 26). Elon Musk’s brain implant firm Neuralink \\ngets approval for human trial. New Scientist: https://www.newsci -\\nentist.com/article/2375886-elon-musks-brain-implant-firm-neura -\\nlink-gets-approval-for-human-trial/\\n14. Brandom, R. (2017, September 12). iPhone X will unlock with facial \\nrecognition instead of the home button. The Verge: https://www.\\ntheverge.com/2017/9/12/16270352/apple-iphone-x-home-button-\\nremoved-unlock-touch-id\\n15. Huynh, D., Millan, A., et al. (2021, July 28). Description and Early \\nResults of the Kaiser Permanent Southern California COVID-19 \\nHome Monitoring Program. National Library of Medicine: https://\\nwww.ncbi.nlm.nih.gov/pmc/articles/PMC8784054/\\n16. Neurorights in Chile. (n.d.). The Neurorights Foundation: https://\\nneurorightsfoundation.org/chile\\n17. Belluck, P. (2023, August 23). A Stroke Stole Her Ability to \\nSpeak at 30. A.I. Is Helping to Restore It Years Later. The New \\nYork Times: https://www.nytimes.com/2023/08/23/health/\\nai-stroke-speech-neuroscience.html\\n18. Freifeld, L. (2022, November 16). 2022 Training Industry Report. \\nTraining: https://trainingmag.com/2022-training-industry-report/  \\n19. Metzger, S., Littlejohn, K., et al. (2023, August 23). A high-perfor -\\nmance neuroprosthesis for speech decoding and avatar control. \\nNature: https://www.nature.com/articles/s41586-023-06443-420. Willett, F., Kunz, E., et al. (2023, August 23). A high-performance \\nspeech neuroprosthesis. Nature: https://www.nature.com/arti -\\ncles/s41586-023-06377-x\\n21. King, J. (2022, August 31). Using AI to decode speech from brain \\nactivity. Meta: https://ai.meta.com/blog/ai-speech-brain-activity/\\n22. Heaney, D. (2023, June 9). Here’s How You Control Apple Vision \\nPro With Eye Tracking And Hand Gestures. UploadVR: https://\\nwww.uploadvr.com/apple-vision-pro-gesture-controls/\\n23. Crispin, S., Yildiz, I., et al. (2021, November 11). Biofeedback Meth -\\nod of Modulating Digital Content to Invoke Greater Pupil Radius \\nResponse. Google Patents: https://patents.google.com/patent/\\nUS20210349536A1\\n24. Dencheva, V. (2023, January 6). Customer experience personal -\\nization and optimization software and services revenue worldwide \\nfrom 2020 to 2026. Statista: https://www.statista.com/statis -\\ntics/1333448/cx-personalization-optimization-revenue-world -\\nwide/\\n25. Navarro, J. (2023, January 6). Marketing personalization world -\\nwide – statistics & facts. Statista: https://www.statista.com/top -\\nics/4481/personalized-marketing/#topicOverview  \\n26. Karrenbach, M., Boe, D., et al. (2022, January). Improving Auto -\\nmatic Control of Upper-Limb Prosthesis Wrists Using Gaze-Cen -\\ntered Eye Tracking and Deep Learning. ResearchGate: https://\\nwww.researchgate.net/publication/358291275_Improving_Au -\\ntomatic_Control_of_Upper-Limb_Prosthesis_Wrists_Using_\\nGaze-Centered_Eye_Tracking_and_Deep_Learning\\n27. The Science of Immersion. (n.d.): https://www.getimmersion.\\ncom/v4/why-it-works\\n28. Ma, J., and Rong, W. (2022, July 31). Pedestrian Crossing Intention \\nPrediction Method Based on Multi-Feature Fusion. MDPI: https://\\nwww.mdpi.com/2032-6653/13/8/158Our bodies electronicTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  89\\n29. Waymo Driver. (n.d.): https://waymo.com/waymo-driver/\\n30. Dawson, C. (2023, April 5). Robots predict human intention for \\nfaster builds. ScienceDaily: https://www.sciencedaily.com/releas -\\nes/2023/04/230405090242.htm\\n31. Nemlekar, H., Dhanaraj, N., et al. Transfer Learning of Human Pref -\\nerences for Proactive Robot Assistance in Assembly Tasks. ACM \\nDigital Library: https://dl.acm.org/doi/10.1145/3568162.3576965\\n32. Bremers, A., Pabst, A., et al. (2023, January 27). Using Social Cues \\nto Recognize Task Failures for HRI: A Review of Current Research \\nand Future Directions. arXiv.org: https://arxiv.org/abs/2301.11972\\n33. Blackrock Neurotech and Phantom Neuro Partner to Enable \\nProsthetics and Exoskeletons that Mimic Human Movement in Re -\\nal-Time. (2022, April 26). PR Newswire: https://www.prnewswire.\\ncom/news-releases/blackrock-neurotech-and-phantom-neuro-\\npartner-to-enable-prosthetics-and-exoskeletons-that-mimic-hu -\\nman-movement-in-real-time-301532495.html\\n34. Bardin, E. (2022, February 7). Galvani Bioelectronics implants first \\nneurostimulator for splenic nerve stimulation in rheumatoid arthri -\\ntis patient. MedCity News: https://medcitynews.com/2022/02/\\ngalvani-bioelectronics-implants-first-neurostimulator-for-splen -\\nic-nerve-stimulation-in-rheumatoid-arthritis-patient/  \\n35. Capoot, A. (2023, February 18). Brain implant startup backed \\nby Bezos and Gates is testing mind-controlled computing on \\nhumans. CNBC: https://www.cnbc.com/2023/02/18/synchron-\\nbacked-by-bezos-and-gates-tests-brain-computer-interface.html  \\n36. Stripe, S. (2021, August 14). Brain-computer interfaces are making \\nbig progress this year. VentureBeat: https://venturebeat.com/\\nbusiness/brain-computer-interfaces-are-making-big-progress-\\nthis-year/\\n37. Chen, J., Zhang, Y., et al. (2023, July). A transformer-based \\ndeep neural network model for SSVEP classification. Science -\\nDirect: https://www.sciencedirect.com/science/article/abs/pii/\\nS089360802300231938. Agarwal, M. (2023, May 31). EEG-Datasets. GitHub: https://github.\\ncom/meagmohit/EEG-Datasets\\n39. Accenture. (2023). Internal Interview.\\n40. Gonfalonieri, A. (2018, November 25). A Beginner’s Guide to \\nBrain-Computer Interface and Convolutional Neural Networks. \\nTowards Data Science: https://towardsdatascience.com/a-begin -\\nners-guide-to-brain-computer-interface-and-convolutional-neu -\\nral-networks-9f35bd4af948\\n41. Warbrick, T. (2022, March 15). Simultaneous EEG-fMRI: What \\nHave We Learned and What Does the Future Hold? National \\nLibrary of Medicine: https://www.ncbi.nlm.nih.gov/pmc/articles/\\nPMC8952790/\\n42. Wearable Sensing DSI-24. (n.d.): https://wearablesensing.com/\\ndsi-24/\\n43. Karim, H., Schmidt, B., et al. (2012, March). Functional Near-infra -\\nred Spectroscopy (fNIRS) of Brain Function During Active Balanc -\\ning Using a Video Game System. National Library of Medicine: \\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3294084/\\n44. Kernel Introduces Flow2: Revolutionary Advanced Neuroim -\\naging Technology Enabling Precision Neuromedicine. (2023, \\nJuly 31). Business Wire: https://www.businesswire.com/news/\\nhome/20230731933400/en/Kernel-Introduces-Flow2-Revolu -\\ntionary-Advanced-Neuroimaging-Technology-Enabling-Preci -\\nsion-Neuromedicine\\n45. EMOTIV and X-trodes collaborate on brain and physiological \\nmeasurement. (2022, November 22). Med-Tech News: https://\\nwww.med-technews.com/news/ai-and-vr-in-healthcare/emo -\\ntiv-and-x-trodes-collaborate-on-brain-and-physiological-m/\\n46. Purcher, J. (2023, July 20). Apple Invents a next-generation Air -\\nPods Sensor System that could measure Biosignals and Electrical \\nActivity of a user’s Brain. Patently Apple: https://www.patentlyap -\\nple.com/2023/07/apple-invents-a-next-generation-airpods-sen -\\nsor-system-that-could-measure-biosignals-and-electrical-activi -\\nty-of-a-users-brain.html47. Ray, T. (2022, February 11). The AI edge chip market is on fire, kin -\\ndled by ‘staggering’ VC funding. ZDNET: https://www.zdnet.com/\\narticle/the-ai-edge-inference-chip-market-is-raging/\\n48. Clark, A. (2023, June 7). Qualcomm, Mobileye, Other AI Chip \\nStocks That Can Thrive. It’s Not All About the Cloud. Barron’s: \\nhttps://www.barrons.com/articles/qualcomm-mobileye-nvidia-ai-\\nchip-stocks-497a0ab7\\n49. Ramirez-Aristizabal, A., Ebrahimpour, M., et al. (2022, January \\n31). Image-Based EEG Classification of Brain Responses to Song \\nRecordings. arXiv.org: https://arxiv.org/pdf/2202.03265.pdf\\n50. Accenture. (2023). Internal Interview.\\n51. Knight, S. (2023, February 20). Switchback VR uses eye tracking \\nto make the game scarier every time you blink. TechSpot: https://\\nwww.techspot.com/news/97667-switchback-vr-uses-eye-track -\\ning-make-game-scarier.html\\n52. Porter, J. (2021, January 25). Gabe Newell has big plans for \\nbrain-computer interfaces in gaming. The Verge: https://\\nwww.theverge.com/2021/1/25/22248202/gabe-new -\\nell-valve-brain-computer-interface-bci-meat-peripherals\\n53. Kacapyr, S. (2022, October 5). Algorithms predict sports \\nteams’ moves with 80% accuracy. Cornell Chronicle: \\nhttps://news.cornell.edu/stories/2022/10/algorithms-pre -\\ndict-sports-teams-moves-80-accuracy\\n54. McClure, P. (2023, March 21). New graphene sensors make for \\nbetter brain-machine interface. New Atlas: https://newatlas.com/\\ntechnology/graphene-sensor-interface-thought-controlled-robot/ \\n55. Heater, B. (2022, March 23). Snap buys mind-controlled head -\\nband maker, NextMind. TechCrunch: https://techcrunch.\\ncom/2022/03/23/snap-buys-mind-controlled-headband-maker-\\nnextmind/56. WiMi to Design A Hybrid BCI-Based Humanoid Control System. \\n(2023, June 23). PR Newswire: https://www.prnewswire.com/\\nnews-releases/wimi-to-design-a-hybrid-bci-based-humanoid-\\ncontrol-system-301860836.html\\n57. Radic, N. (2022, November 19). Are You Ready for Workplace \\nBrain Scanning? IEEE Spectrum: https://spectrum.ieee.org/neuro -\\ntech-workplace-innereye-emotiv\\n58. Loréal, in partnership with global neurotech leader, EMOTIV, \\nlaunches new deivce to help consumers personalize their fra -\\ngrance choices. (2022, March 21). Emotiv: https://www.emotiv.\\ncom/news/loreal-in-partnership-with-emotiv-neurotech-leader/\\n59. Meili Technologies. (n.d.). LinkedIn: https://www.linkedin.com/\\ncompany/meilitech/about/\\n60. Meili Technologies. (n.d.). AutoSens: https://auto-sens.com/or -\\nganisation/meili-technologies/\\n61. Doyle, M. (2021, June 15). One Smart Hard Hat: How This Device \\nKeeps Your Workers Awake. Equipment World: https://www.\\nequipmentworld.com/technology/article/15066019/how-smart -\\ncap-keeps-workers-awake \\n62. Shipman, M. (2023, April 28). The Future Is Now: Wrestling with \\nEthics, Policy and Brain-Computer Interfaces. NC State News: \\nhttps://news.ncsu.edu/2023/04/ethics-brain-computer-interfac -\\nes/\\n63. Strickland, E. (2021, December 18). Worldwide Campaign for Neu -\\nrorights Notches Its First Win. IEEE Spectrum: https://spectrum.\\nieee.org/neurotech-neurorights\\n64. The Neurorights Foundation. (n.d.): https://neurorightsfounda -\\ntion.org/\\n65. Policy. (n.d.). The Neurorights Foundation: https://neurorights -\\nfoundation.org/policypageTechnology Vision  2024  |  Human by design  \\n#TechVision2024Technology Vision  2 024  |  Human by design  90\\nAbout Accenture  \\nAccenture is a leading global professional \\nservices company that helps the world’s leading businesses, governments and other organizations build their digital core, optimize their operations, accelerate revenue growth and enhance citizen services – creating tangible value at scale. We are a talent- and innovation-led company with approximately 743,000 people serving clients in more than 120 countries. Technology is at the core of change today, and we are one of the world’s leaders in helping drive that change, with strong ecosystem relationships. We combine our strength in technology and leadership in cloud, data and AI with unmatched industry experience, functional expertise and global delivery capability. We are uniquely able to deliver tangible outcomes because of our broad range of services, solutions and assets across Strategy & Consulting, Technology, Operations, Industry X and Song. These capabilities, together with our culture of shared success and commitment to creating 360° value, enable us to help our clients reinvent and build trusted, lasting relationships. We measure our success by the 360° value we create for our clients, each other, our shareholders, partners and communities. Visit us at\\n \\nwww.accenture.com . About Accenture Labs \\nAccenture Labs incubates and prototypes new concepts through applied R&D projects that are expected to have a significant impact on business and society. Our dedicated team of technologists and researchers work with leaders across the company and external partners to imagine and invent the future. Accenture Labs is located in six key research hubs around the world: San Francisco, CA; Washington, D.C.; Dublin, Ireland; Sophia Antipolis, France; Herzliya, Israel; and Bangalore, India. For more information,  visit Accenture Labs on www.accenture.com .\\nAccenture Research  \\nAccenture Research creates thought leadership about the most pressing business issues organizations face. Combining innovative research techniques, such as data-science-led analysis, with a deep understanding of industry and technology, our team of 300 researchers in 20 countries publish hundreds of reports, articles and points of view every year. Our thought-provoking research developed with world leading organizations helps our clients embrace change, create value and deliver on the power of technology and human ingenuity.  For more information, visit Accenture Research on www.accenture.com .\\nDisclaimer: The material in this document reflects \\ninformation available at the point in time at which this document was prepared as indicated by the date provided on the front page, however the global situation is rapidly evolving and the position may change. This content is provided for general information purposes only, does not take into account the reader’s specific circumstances, and is not intended to be used in place of consultation with our professional advisors. Accenture disclaims, to the fullest extent permitted by applicable law, any and all liability for the accuracy and completeness of the information in this document and for any acts or omissions made based on such information.  Accenture does not provide legal, regulatory, audit,  or tax advice. Readers are responsible for obtaining  such advice from their own legal counsel or other licensed professionals.  \\nThis document refers to marks owned by third parties.  \\nAll such third-party marks are the property of their respective owners. No sponsorship, endorsement or approval of this content by the owners of such marks is intended, expressed or implied.  \\nCopyright © 2024 Accenture. All rights reserved. Accenture \\nand its logo are registered trademarks of Accenture.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b46bf154-90e5-407b-ab45-5bf816f5468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\",\n",
    "    chunk_size = 256,\n",
    "    length_function = len    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efc0b328-c7b8-4353-837c-b1138791e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b8ec905-1914-49db-91d8-8e66b9947f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4011"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890cef0-9f51-44ad-977c-88be5614a781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7768344f-7316-4739-a686-c0ad867612bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ac6eb9ed-b70d-4e03-92ea-d3e86150eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d3d9ea9-d5f2-4f37-ba62-3ec07ee7c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(text_splitter.create_documents(texts), embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4137d98-e897-43de-8767-c81926ec989f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x26d466234c0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f4cbffa1-278a-4071-8ad0-82335e5be382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "454ed646-7560-456a-b00f-197d26230989",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(Ollama(model=\"llama2\"), chain_type = \"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "81c58464-26ff-48ee-9341-0cc1d0011849",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the key areas of research?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "266e8bba-b5b9-44e2-98a1-1b8bf327f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e0f38ff5-3560-4192-b9fe-c891f26fd34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='research techniques, such as data-science-led analysis, with a deep understanding of industry and technology, our team of 300 researchers in 20 countries publish hundreds of reports, articles and points of view every year. Our thought-provoking research d'),\n",
       " Document(page_content='e, India. For more information,  visit Accenture Labs on www.accenture.com .\\nAccenture Research  \\nAccenture Research creates thought leadership about the most pressing business issues organizations face. Combining innovative research techniques, such as da'),\n",
       " Document(page_content='ntire tracts of scientific research by looking \\nfor information on the web, consulting scientific documents, and using scientific \\nequipment in a cloud lab.35 Google’s PaLM-E \\ncan take a command in natural language, \\nbreak it down into a series of subtasks'),\n",
       " Document(page_content='www.accenture.com .\\nAccenture Research  \\nAccenture Research creates thought leadership about the most pressing business issues organizations face. Combining innovative research techniques, such as data-science-led analysis, with a deep understanding of in')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c3578ebb-f077-423b-83c3-87155806c701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='research techniques, such as data-science-led analysis, with a deep understanding of industry and technology, our team of 300 researchers in 20 countries publish hundreds of reports, articles and points of view every year. Our thought-provoking research d'),\n",
       "  Document(page_content='e, India. For more information,  visit Accenture Labs on www.accenture.com .\\nAccenture Research  \\nAccenture Research creates thought leadership about the most pressing business issues organizations face. Combining innovative research techniques, such as da'),\n",
       "  Document(page_content='ntire tracts of scientific research by looking \\nfor information on the web, consulting scientific documents, and using scientific \\nequipment in a cloud lab.35 Google’s PaLM-E \\ncan take a command in natural language, \\nbreak it down into a series of subtasks'),\n",
       "  Document(page_content='www.accenture.com .\\nAccenture Research  \\nAccenture Research creates thought leadership about the most pressing business issues organizations face. Combining innovative research techniques, such as data-science-led analysis, with a deep understanding of in')],\n",
       " 'question': 'what are the key areas of research?',\n",
       " 'output_text': \"Based on the provided context, the key areas of research conducted by Accenture Research can be inferred to be:\\n\\n1. Data-science-led analysis: The use of innovative research techniques that involve analyzing large datasets and using data science tools to gain insights into business issues.\\n2. Industry and technology: A deep understanding of the latest industry trends and technological advancements, which are used to inform research reports, articles, and points of view.\\n3. Global coverage: Accenture Research has a team of 300 researchers in 20 countries, indicating that their research covers a wide range of geographic locations and cultural contexts.\\n4. Thought leadership: The creation of thought-provoking research that explores the most pressing business issues organizations face, with the aim of providing valuable insights and recommendations to help them navigate these challenges.\\n5. Cloud lab: The use of a cloud-based laboratory for conducting scientific research, which enables the team to access information on the web, consult scientific documents, and use scientific equipment remotely.\\n6. Natural language processing: The ability to take commands in natural language and break them down into subtasks, using tools like Google's PaLM-E, highlights the focus on developing intelligent systems that can understand and respond to human language.\"}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input_documents\": docs, \"question\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1e8c7-1e3e-4af5-b5dc-b63f9cacc596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4474519a-72db-4c72-909a-0b1c0439c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(folder_path = \"\", index_name = \"Acc_TechVision_FiassIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c73259d3-f1b4-49bc-be20-50e1a497fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_test = FAISS.load_local(folder_path = \"\", index_name = \"Acc_TechVision_FiassIndex\",embeddings = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "deba48df-b44e-4e70-bd34-f1bd756833d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x26d4476f910>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a94eb19c-7bd9-4232-88ec-4619772feb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='research techniques, such as data-science-led analysis, with a deep understanding of industry and technology, our team of 300 researchers in 20 countries publish hundreds of reports, articles and points of view every year. Our thought-provoking research d'),\n",
       " Document(page_content='e, India. For more information,  visit Accenture Labs on www.accenture.com .\\nAccenture Research  \\nAccenture Research creates thought leadership about the most pressing business issues organizations face. Combining innovative research techniques, such as da'),\n",
       " Document(page_content='ntire tracts of scientific research by looking \\nfor information on the web, consulting scientific documents, and using scientific \\nequipment in a cloud lab.35 Google’s PaLM-E \\ncan take a command in natural language, \\nbreak it down into a series of subtasks'),\n",
       " Document(page_content='www.accenture.com .\\nAccenture Research  \\nAccenture Research creates thought leadership about the most pressing business issues organizations face. Combining innovative research techniques, such as data-science-led analysis, with a deep understanding of in')]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_test.similarity_search(\"what are the key areas of research?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2198d97-c115-49e6-a97c-1ccfd344715d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7e1f4-6a70-4cc7-9687-66de6acd00ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9090445-27ed-4162-87dd-d922ed301a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52fb3a1-66d0-4448-bea1-05672653ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "vectorstore = FAISS.load_local(folder_path = \"\", index_name = \"Acc_TechVision_FiassIndex\",embeddings = embeddings)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1454ee60-5f9e-4477-a565-578ffd8eec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InvokeQnAmodel(query,history):\n",
    "    docs = vectorstore.similarity_search(query)\n",
    "    return chain.invoke({\"input_documents\": docs, \"question\": query})['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211a98bd-1aab-468d-80d3-c52e93de52a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = gr.ChatInterface(fn=InvokeQnAmodel,title=\"Accenture-Tech-Vision-2024 helper Bot\",retry_btn=None,undo_btn=None,clear_btn=None)\n",
    "t.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84bd830d-8ab5-4609-afb0-3ca560c57859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run DocumentChatBot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ab3fa-f74a-4def-bb8c-2a9f52cf04cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
